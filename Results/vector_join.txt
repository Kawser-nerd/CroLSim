#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
import math
import shutil
import numpy as np
from numpy import genfromtxt

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-f", "--folder", help="txt file of folder names", required=True)
  parser.add_argument("-i", "--imgfolder", help="imgfolder name", required=True)
  parser.add_argument("-o", "--outdir", help="new folder destination", required=True)
  args = parser.parse_args()
  return args


def main():
  
  args = options()
  path=os.getcwd()
  #folders=open(args.folder, 'r')
  #for l in folders:
  #  fname=l.replace("\n", "")
  #  source=str(path)+"/"+str(args.imgfolder)+"/"+str(fname)
  #  destination=str(path)+"/"+str(args.outdir)
  #  shutil.move(source, destination)
  
  snapshots=str(path)+"/"+str(args.imgfolder)+"/SnapshotInfo.csv"
  #snapshot_data = genfromtxt(snapshots, delimiter=',')
  regex="^.*B*.*$"
  select=np.fromregex(snapshots,regex)
  print snapshot_data
  
if __name__ == '__main__':
  main()
# Color palette returns an array of colors (rainbow)

from random import randrange


def color_palette(num):
    """color_palette: Returns a list of colors length num

    Inputs:
    num        = number of colors to return. If num = 1 a random color is returned,
                 otherwise, evenly spaced colors are returned.

    Returns:
    colors     = a list of color tuples (RGB values)

    :param num: int
    :return colors: list
    """
    # Rainbow color scheme (red->red)
    rainbow = (
        (0, 0, 255), (0, 6, 255), (0, 12, 255), (0, 18, 255), (0, 24, 255), (0, 30, 255), (0, 36, 255), (0, 42, 255),
        (0, 48, 255), (0, 54, 255), (0, 60, 255), (0, 66, 255), (0, 72, 255), (0, 78, 255), (0, 84, 255), (0, 90, 255),
        (0, 96, 255), (0, 102, 255), (0, 108, 255), (0, 114, 255), (0, 120, 255), (0, 126, 255), (0, 131, 255),
        (0, 137, 255), (0, 143, 255), (0, 149, 255), (0, 155, 255), (0, 161, 255), (0, 167, 255), (0, 173, 255),
        (0, 179, 255), (0, 185, 255), (0, 191, 255), (0, 197, 255), (0, 203, 255), (0, 209, 255), (0, 215, 255),
        (0, 221, 255), (0, 227, 255), (0, 233, 255), (0, 239, 255), (0, 245, 255), (0, 251, 255), (0, 255, 253),
        (0, 255, 247), (0, 255, 241), (0, 255, 235), (0, 255, 229), (0, 255, 223), (0, 255, 217), (0, 255, 211),
        (0, 255, 205), (0, 255, 199), (0, 255, 193), (0, 255, 187), (0, 255, 181), (0, 255, 175), (0, 255, 169),
        (0, 255, 163), (0, 255, 157), (0, 255, 151), (0, 255, 145), (0, 255, 139), (0, 255, 133), (0, 255, 128),
        (0, 255, 122), (0, 255, 116), (0, 255, 110), (0, 255, 104), (0, 255, 98), (0, 255, 92), (0, 255, 86),
        (0, 255, 80),
        (0, 255, 74), (0, 255, 68), (0, 255, 62), (0, 255, 56), (0, 255, 50), (0, 255, 44), (0, 255, 38), (0, 255, 32),
        (0, 255, 26), (0, 255, 20), (0, 255, 14), (0, 255, 8), (0, 255, 2), (4, 255, 0), (10, 255, 0), (16, 255, 0),
        (22, 255, 0), (28, 255, 0), (34, 255, 0), (40, 255, 0), (46, 255, 0), (52, 255, 0), (58, 255, 0), (64, 255, 0),
        (70, 255, 0), (76, 255, 0), (82, 255, 0), (88, 255, 0), (94, 255, 0), (100, 255, 0), (106, 255, 0),
        (112, 255, 0),
        (118, 255, 0), (124, 255, 0), (129, 255, 0), (135, 255, 0), (141, 255, 0), (147, 255, 0), (153, 255, 0),
        (159, 255, 0), (165, 255, 0), (171, 255, 0), (177, 255, 0), (183, 255, 0), (189, 255, 0), (195, 255, 0),
        (201, 255, 0), (207, 255, 0), (213, 255, 0), (219, 255, 0), (225, 255, 0), (231, 255, 0), (237, 255, 0),
        (243, 255, 0), (249, 255, 0), (255, 255, 0), (255, 249, 0), (255, 243, 0), (255, 237, 0), (255, 231, 0),
        (255, 225, 0), (255, 219, 0), (255, 213, 0), (255, 207, 0), (255, 201, 0), (255, 195, 0), (255, 189, 0),
        (255, 183, 0), (255, 177, 0), (255, 171, 0), (255, 165, 0), (255, 159, 0), (255, 153, 0), (255, 147, 0),
        (255, 141, 0), (255, 135, 0), (255, 129, 0), (255, 124, 0), (255, 118, 0), (255, 112, 0), (255, 106, 0),
        (255, 100, 0), (255, 94, 0), (255, 88, 0), (255, 82, 0), (255, 76, 0), (255, 70, 0), (255, 64, 0), (255, 58, 0),
        (255, 52, 0), (255, 46, 0), (255, 40, 0), (255, 34, 0), (255, 28, 0), (255, 22, 0), (255, 16, 0), (255, 10, 0),
        (255, 4, 0), (255, 0, 2), (255, 0, 8), (255, 0, 14), (255, 0, 20), (255, 0, 26), (255, 0, 32), (255, 0, 38),
        (255, 0, 44), (255, 0, 50), (255, 0, 56), (255, 0, 62), (255, 0, 68), (255, 0, 74), (255, 0, 80), (255, 0, 86),
        (255, 0, 92), (255, 0, 98), (255, 0, 104), (255, 0, 110), (255, 0, 116), (255, 0, 122), (255, 0, 128),
        (255, 0, 133), (255, 0, 139), (255, 0, 145), (255, 0, 151), (255, 0, 157), (255, 0, 163), (255, 0, 169),
        (255, 0, 175), (255, 0, 181), (255, 0, 187), (255, 0, 193), (255, 0, 199), (255, 0, 205), (255, 0, 211),
        (255, 0, 217), (255, 0, 223), (255, 0, 229), (255, 0, 235), (255, 0, 241), (255, 0, 247), (255, 0, 253),
        (251, 0, 255), (245, 0, 255), (239, 0, 255), (233, 0, 255), (227, 0, 255), (221, 0, 255), (215, 0, 255),
        (209, 0, 255), (203, 0, 255), (197, 0, 255), (191, 0, 255), (185, 0, 255), (179, 0, 255), (173, 0, 255),
        (167, 0, 255), (161, 0, 255), (155, 0, 255), (149, 0, 255), (143, 0, 255), (137, 0, 255), (131, 0, 255),
        (126, 0, 255), (120, 0, 255), (114, 0, 255), (108, 0, 255), (102, 0, 255), (96, 0, 255), (90, 0, 255),
        (84, 0, 255),
        (78, 0, 255), (72, 0, 255), (66, 0, 255), (60, 0, 255), (54, 0, 255), (48, 0, 255), (42, 0, 255), (36, 0, 255),
        (30, 0, 255), (24, 0, 255), (18, 0, 255), (12, 0, 255), (6, 0, 255))

    if num == 1:
        color = rainbow[randrange(0, 255)]
        return [color]
    else:
        dist = int(len(rainbow) / num)
        colors = []
        index = 0
        for i in range(1, num + 1):
            colors.append(rainbow[index])
            index += dist
        return colors
# Function to scan for pseudolandmarks along the y-axis

import cv2
import numpy as np
from . import plot_image


def y_axis_pseudolandmarks(obj, mask, img, device, debug=None):
    """Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    device   = a counter variable
    debug    = True/False. If True, print image

    Returns:
    device   = pipeline step counter
    top      =
    bottom   =
    center_v =

    :param obj: list
    :param mask: ndarray
    :param img: ndarray
    :param device: int
    :param debug: str
    :return device: int
    :return left:
    :return right:
    :return center_h:
    """
    # Lets get some landmarks scanning along the y-axis
    device += 1
    if not np.any(obj):
        return device, ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA')
    x, y, width, height = cv2.boundingRect(obj)
    extent = height
    # If height is greater than 21 pixels make 20 increments (5% intervals)
    if extent >= 21:
        inc = extent / 21
        # Define variable for max points and min points
        pts_max = []
        pts_min = []
        # Get max and min points for each of the intervals
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + (inc * i)
            else:
                pt_max = y + (inc * (i - 1))
                pt_min = y + (inc * i)
            # Put these in an array
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        # Combine max and min into a set of tuples
        point_range = list(zip(pts_max, pts_min))
        # define some list variables to fill
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        # For each of the 20 intervals
        for pt in point_range:
            # Get the lower and upper bounds
            # (lower and higher in terms of value; low point is actually towards top of photo, higher is lower of photo)
            low_point, high_point = pt
            # Get all rows within these two points
            rows = []
            lps = []
            rps = []
            # Get a continuous list of the values between the top and the bottom of the interval save as vals
            vals = list(range(low_point, high_point))
            # For each row... get all coordinates from object contour that match row
            for v in vals:
                # Value is all entries that match the row
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    # Could potentially be more than two points in all contour in each pixel row
                    # Grab largest x coordinate (column)
                    largest = value[:, 0, 0].max()
                    # Grab smallest x coordinate (column)
                    smallest = value[:, 0, 0].min()
                    # Take the difference between the two (this is how far across the object is on this plane)
                    row_width = largest - smallest
                    # Append this value to a list
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            # For each of the points find the median and average width
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            # Make a copy of the mask; we want to get landmark points from this
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            # Centroid (center of mass x, center of mass y)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    smx, smy = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    smx, smy = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        # Get the indicie of the largest median/average x-axis value (if there is a tie it takes largest index)
        # indice_median = row_median.index(max(row_median))
        # indice_ave = row_ave.index(max(row_ave))
        # median_value = row_median[indice_median]
        # ave_value = row_ave[indice_ave]
        # max_value = max_width[indice_ave]
        left = zip(left_points, y_vals)
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        if debug == 'plot':
            img2 = np.copy(img)
            for i in left:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 0), -1)
            for i in right:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 255), -1)
            for i in center_h:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (0, 79, 255), -1)
            # print_image(img2, (str(device) + '_y_axis_pseudolandmarks.png'))
            plot_image(img2)
        return device, left, right, center_h
    
    if extent < 21:
        # If the length of the object is less than 20 pixels just make the object a 20 pixel rectangle
        x, y, width, height = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        # Centroid (center of mass x, center of mass y)
        cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
        c_points = [cmx] * 20
        center_h = list(zip(c_points, y_coords))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        if debug == 'plot':
            img2 = np.copy(img)
            for i in left:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 0), -1)
            for i in right:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 255), -1)
            for i in center_h:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (0, 79, 255), -1)
            # print_image(img2, (str(device) + '_y_axis_pseudolandmarks.png'))
            plot_image(img2)
        return device, left, right, center_h
# Invert gray image

import cv2
from . import print_image
from . import plot_image


def invert(img, device, debug=None):
    """Inverts grayscale images.

    Inputs:
    img     = image object, grayscale
    device  = device number. Used to count steps in the pipeline
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device  = device number
    img_inv = inverted image

    :param img: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return img_inv: numpy array
    """

    device += 1
    img_inv = cv2.bitwise_not(img)
    if debug == 'print':
        print_image(img_inv, (str(device) + '_invert.png'))
    elif debug == 'plot':
        plot_image(img_inv, cmap='gray')
    return device, img_inv
# Scharr filtering

import cv2
from . import print_image
from . import plot_image


def scharr_filter(img, dX, dY, scale, device, debug=None):
    """This is a filtering method used to identify and highlight gradient edges/features using the 1st derivative.
       Typically used to identify gradients along the x-axis (dx = 1, dy = 0) and y-axis (dx = 0, dy = 1) independently.
       Performance is quite similar to Sobel filter. Used to detect edges / changes in pixel intensity. ddepth = -1
       specifies that the dimensions of output image will be the same as the input image.

    Inputs:
    # img    = image
    # dx     = derivative of x to analyze (1-3)
    # dy     = derivative of x to analyze (1-3)
    # scale  = scaling factor applied (multiplied) to computed Scharr values (scale = 1 is unscaled)
    # device = device number. Used to count steps in the pipeline
    # debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    sr_img   = Scharr filtered image

    :param img: numpy array
    :param dX: int
    :param dY: int
    :param scale: int
    :param device: int
    :param debug: str
    :return device: int
    :return sr_img: numpy array
    """

    sr_img = cv2.Scharr(src=img, ddepth=-1, dx=dX, dy=dY, scale=scale)
    device += 1
    if debug == 'print':
        print_image(sr_img,
                    str(device) + '_sr_img_dx' + str(dX) + '_dy' + str(dY) + '_scale' + str(scale) + '.png')
    elif debug == 'plot':
        plot_image(sr_img, cmap='gray')
    return device, sr_img
#!/usr/bin/env python
from __future__ import print_function
import os
import sys
import multiprocessing as mp
import argparse
import time
import datetime
from dateutil.parser import parse as dt_parser
import sqlite3
import re
from subprocess import call
import mimetypes


# Parse command-line arguments
###########################################
def options():
    """Parse command line options.
    
    Args:
    
    Returns:
        argparse object.
    Raises:
        IOError: if dir does not exist.
        IOError: if pipeline does not exist.
        IOError: if the metadata file SnapshotInfo.csv does not exist in dir when flat is False.
        ValueError: if adaptor is not phenofront or dbimportexport.
        ValueError: if a metadata field is not supported.
    """
    # Job start time
    start_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    print("Starting run " + start_time + '\n', file=sys.stderr)

    # These are metadata types that PlantCV deals with.
    # Values are default values in the event the metadata is missing
    valid_meta = {
        # Camera settings
        'camera': 'none',
        'imgtype': 'none',
        'zoom': 'none',
        'exposure': 'none',
        'gain': 'none',
        'frame': 'none',
        'lifter': 'none',
        # Date-Time
        'timestamp': 'none',
        # Sample attributes
        'id': 'none',
        'plantbarcode': 'none',
        'treatment': 'none',
        'cartag': 'none',
        # Experiment attributes
        'measurementlabel': 'none',
        # Other
        'other': 'none'
    }
    parser = argparse.ArgumentParser(description='Parallel imaging processing with PlantCV.',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("-d", "--dir", help='Input directory containing images or snapshots.', required=True)
    parser.add_argument("-a", "--adaptor",
                        help='Image metadata reader adaptor. PhenoFront metadata is stored in a CSV file and the '
                             'image file name. For the filename option, all metadata is stored in the image file '
                             'name. Current adaptors: phenofront, image', default="phenofront")
    parser.add_argument("-p", "--pipeline", help='Pipeline script file.', required=True)
    parser.add_argument("-s", "--db", help='SQLite database file name.', required=True)
    parser.add_argument("-i", "--outdir", help='Output directory for images. Not required by all pipelines.',
                        default=".")
    parser.add_argument("-T", "--cpu", help='Number of CPU to use.', default=1, type=int)
    parser.add_argument("-c", "--create",
                        help='will overwrite an existing database'
                             'Warning: activating this option will delete an existing database!',
                        default=False, action="store_true")
    parser.add_argument("-D", "--dates",
                        help='Date range. Format: YYYY-MM-DD-hh-mm-ss_YYYY-MM-DD-hh-mm-ss. If the second date '
                             'is excluded then the current date is assumed.',
                        required=False)
    parser.add_argument("-t", "--type", help='Image format type (extension).', default="png")
    parser.add_argument("-l", "--delimiter", help='Image file name metadata delimiter character.', default='_')
    parser.add_argument("-f", "--meta",
                        help='Image file name metadata format. List valid metadata fields separated by the '
                             'delimiter (-l/--delimiter). Valid metadata fields are: ' +
                             ', '.join(map(str, list(valid_meta.keys()))), default='imgtype_camera_frame_zoom_id')
    parser.add_argument("-M", "--match",
                        help='Restrict analysis to images with metadata matching input criteria. Input a '
                             'metadata:value comma-separated list. This is an exact match search. '
                             'E.g. imgtype:VIS,camera:SV,zoom:z500',
                        required=False)
    parser.add_argument("-C", "--coprocess",
                        help='Coprocess the specified imgtype with the imgtype specified in --match '
                             '(e.g. coprocess NIR images with VIS).',
                        default=None)
    parser.add_argument("-w", "--writeimg", help='Include analysis images in output.', default=False,
                        action="store_true")
    parser.add_argument("-o", "--other_args", help='Other arguments to pass to the pipeline script.', required=False)
    args = parser.parse_args()

    if not os.path.exists(args.dir):
        raise IOError("Directory does not exist: {0}".format(args.dir))
    if not os.path.exists(args.pipeline):
        raise IOError("File does not exist: {0}".format(args.pipeline))
    if args.adaptor is 'phenofront':
        if not os.path.exists(args.dir + '/SnapshotInfo.csv'):
            raise IOError(
                'The snapshot metadata file SnapshotInfo.csv does not exist in {0}. '
                'Perhaps you meant to use a different adaptor?'.format(
                    args.dir))
    if not os.path.exists(args.outdir):
        raise IOError("Directory does not exist: {0}".format(args.outdir))

    args.jobdir = start_time
    try:
        os.makedirs(args.jobdir)
    except IOError as e:
        raise IOError("{0}: {1}".format(e.strerror, args.jobdir))

    if args.adaptor != 'phenofront' and args.adaptor != 'filename':
        raise ValueError("Adaptor must be either phenofront or filename")

    if args.dates:
        dates = args.dates.split('_')
        if len(dates) == 1:
            # End is current time
            dates.append(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))
        start = map(int, dates[0].split('-'))
        end = map(int, dates[1].split('-'))
        # Convert start and end dates to Unix time
        start_td = datetime.datetime(*start) - datetime.datetime(1970, 1, 1)
        end_td = datetime.datetime(*end) - datetime.datetime(1970, 1, 1)
        args.start_date = (start_td.days * 24 * 3600) + start_td.seconds
        args.end_date = (end_td.days * 24 * 3600) + end_td.seconds
    else:
        end = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        end_list = map(int, end.split('-'))
        end_td = datetime.datetime(*end_list) - datetime.datetime(1970, 1, 1)
        args.start_date = 1
        args.end_date = (end_td.days * 24 * 3600) + end_td.seconds

    args.valid_meta = valid_meta
    args.start_time = start_time

    # Image filename metadata structure
    fields = args.meta.split(args.delimiter)
    # Keep track of the number of metadata fields matching filenames should have
    args.meta_count = len(fields)
    structure = {}
    for i, field in enumerate(fields):
        structure[field] = i
    args.fields = structure

    # Are the user-defined metadata valid?
    for field in args.fields:
        if field not in args.valid_meta:
            raise ValueError("The field {0} is not a currently supported metadata type.".format(field))

    # Metadata restrictions
    args.imgtype = {}
    if args.match is not None:
        pairs = args.match.split(',')
        for pair in pairs:
            key, value = pair.split(':')
            args.imgtype[key] = value
    else:
        args.imgtype['None'] = 'None'

    if (args.coprocess is not None) and ('imgtype' not in args.imgtype):
        raise ValueError("When the coprocess imgtype is defined, imgtype must be included in match.")

    return args


###########################################

# Main
###########################################
def main():
    """Main program.
      
    Args:
    
    Returns:
    
    Raises:
  
    """

    # Get options
    args = options()

    # Variables
    ###########################################
    meta = {}

    # Get this script's path
    exedir = os.path.abspath(os.path.dirname(sys.argv[0]))
    # db_schema = exedir + '/../../include/results.sql'

    # Get command
    command = ' '.join(map(str, sys.argv))

    # Database upload file name prefix
    # Use user inputs to make filenames
    prefix = 'plantcv'
    if args.imgtype is not None:
        kv_list = []
        for key in args.imgtype:
            kv_list.append(key + str(args.imgtype[key]))
        prefix = prefix + '_' + '_'.join(map(str, kv_list))
    if args.dates:
        prefix = prefix + '_' + args.dates
    ###########################################

    # Open log files
    args.fail_log = file_writer(prefix + '_failed_images_' + args.start_time + '.log')
    args.error_log = file_writer(prefix + '_errors_' + args.start_time + '.log')

    # Open intermediate database files
    runinfo_file = file_writer(prefix + '_runinfo.tab')
    args.metadata_file = file_writer(prefix + '_metadata.tab')
    args.analysis_images_file = file_writer(prefix + '_analysis_images.tab')
    args.features_file = file_writer(prefix + '_features.tab')
    args.signal_file = file_writer(prefix + '_signal.tab')

    # Database setup
    ###########################################
    args = db_connect(args)
    ###########################################

    # Run info
    ###########################################
    # Next run ID
    args.run_id += 1

    runinfo_file.write("|".join(map(str, (args.run_id, args.start_time, command))) + '\n')
    ###########################################

    # Read image file names
    ###########################################
    if args.adaptor == 'filename':
        # Input directory contains images where the file name contains all metadata
        args, meta = filename_parser(args)
    elif args.adaptor == 'phenofront':
        # Input directory is in PhenoFront snapshot format with subdirectories for each snapshot.
        # Metadata is stored in a CSV file.
        args, meta = phenofront_parser(args)
    ###########################################

    # Process images
    ###########################################
    # Job builder start time
    job_builder_start_time = time.time()
    print("Building job list... ", file=sys.stderr)
    jobs = job_builder(args, meta)
    # Job builder clock time
    job_builder_clock_time = time.time() - job_builder_start_time
    print("took " + str(job_builder_clock_time) + '\n', file=sys.stderr)

    # Parallel image processing time
    multi_start_time = time.time()
    print("Processing images... ", file=sys.stderr)

    exe_multiproc(jobs, args.cpu)

    # Parallel clock time
    multi_clock_time = time.time() - multi_start_time
    print("took " + str(multi_clock_time) + '\n', file=sys.stderr)

    ###########################################

    # Compile image analysis results
    ###########################################
    # Process results start time
    process_results_start_time = time.time()
    print("Processing results... ", file=sys.stderr)
    process_results(args)
    # Process results clock time
    process_results_clock_time = time.time() - process_results_start_time
    print("took " + str(process_results_clock_time) + '\n', file=sys.stderr)
    ###########################################

    # Cleanup
    ###########################################
    runinfo_file.close()
    args.metadata_file.close()
    args.features_file.close()
    args.signal_file.close()
    args.analysis_images_file.close()
    args.fail_log.close()
    args.error_log.close()
    args.connect.close()
    ###########################################

    # Load database
    ###########################################
    call("sqlite3 " + args.db + " \".import " + runinfo_file.name + " runinfo\"", shell=True)
    call("sqlite3 " + args.db + " \".import " + args.metadata_file.name + " metadata\"", shell=True)
    call("sqlite3 " + args.db + " \".import " + args.features_file.name + " features\"", shell=True)
    call("sqlite3 " + args.db + " \".import " + args.analysis_images_file.name + " analysis_images\"", shell=True)
    call("sqlite3 " + args.db + " \".import " + args.signal_file.name + " signal\"", shell=True)
    ###########################################


###########################################

# Open a file for writing
###########################################
def file_writer(filename):
    """
    Open a file for writing.
  
    Args:
        filename: (string) the name of the path/file to open for writing.
    Returns:
        file object.
    Raises:
        IOError: If filename is not writeable.
    """
    try:
        fileobj = open(filename, 'w')
    except IOError as e:
        raise IOError("{0}: {1}".format(e.strerror, filename))
    return fileobj


###########################################

# Print a message and exit the program
###########################################
def exit_message(message):
    """
    Print error message and exit program.
  
    Args:
        message: (string) the error message to print.
    Returns:
    
    Raises:
  
    """
    sys.exit(message)


###########################################

# Dictionary factory for SQLite query results
###########################################
def dict_factory(cursor, row):
    """
    Replace the row_factory result constructor with a dictionary constructor.
  
    Args:
        cursor: (object) the sqlite3 database cursor object.
        row: (list) a result list.
    Returns:
        d: (dictionary) sqlite3 results dictionary.
    Raises:
  
    """
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


###########################################

# Connect to output database
###########################################
def db_connect(args):
    """
    Connect to the output database, initialize if requested.
    Currently supports SQLite3.
  
    Args:
        args: (object) argsparse object.
    Returns:
        args: (object) argparse object with the following added:
        args.sq: Database cursor.
        args.run_id: Last run ID.
        args.image_id: Last image ID.
    Raises:
        IOError: If schema does not exist or is not readable.
    """

    # Delete the existing database if create is true
    if args.create:
        if os.path.isfile(args.db):
            os.remove(args.db)

    # Run and image IDs
    args.run_id = 0
    args.image_id = 0

    # Connect to the database
    args.connect = sqlite3.connect(args.db)
    # Replace the row_factory result constructor with a dictionary constructor
    args.connect.row_factory = dict_factory
    # Change the text output format from unicode to UTF-8
    args.connect.text_factory = str
    # Database handler
    args.sq = args.connect.cursor()

    # If the runinfo and metadata tables exists,
    # then we have data in the database and can increment run_id and image_id
    runinfo = False
    metadata = False
    for row in args.sq.execute('SELECT name FROM sqlite_master WHERE type="table"'):
        if row["name"] == "runinfo":
            runinfo = True
        elif row["name"] == "metadata":
            metadata = True

    if runinfo is True and metadata is True:
        for row in args.sq.execute('SELECT MAX(run_id) AS max FROM runinfo'):
            if row['max'] is not None:
                args.run_id = row['max']

        # Get the last run ID
        for row in args.sq.execute('SELECT MAX(image_id) AS max FROM metadata'):
            if row['max'] is not None:
                args.image_id = row['max']

    return args


###########################################

# Reads images from a single directory in
# LemnaTec DBImportExport format
###########################################
def filename_parser(args):
    """
    Reads metadata from file names.
  
    Args:
        args: (object) argparse object.
    Returns:
        meta: image metadata object.
    Raises:
        IOError: if an image file does not exist.
    """
    # Metadata data structure
    meta = {}
    args.jobcount = 0

    # Compile regular expression to remove image file extensions
    pattern = '\.' + args.type + '$'
    ext = re.compile(pattern, re.IGNORECASE)

    # Walk through the input directory and find images that match input criteria
    for (dirpath, dirnames, filenames) in os.walk(args.dir):
        for filename in filenames:
            # Is filename and image?
            is_img = ext.search(filename)
            # If filename is an image, parse the metadata
            if is_img is not None:
                # Remove the file extension
                prefix = ext.sub('', filename)
                metadata = prefix.split(args.delimiter)

                # Image metadata
                img_meta = {'path': dirpath}
                img_pass = 1
                # For each of the type of metadata PlantCV keeps track of
                for field in args.valid_meta:
                    # If the same metadata is found in the image filename, store the value
                    if field in args.fields:
                        meta_value = metadata[args.fields[field]]
                        # If the metadata type has a user-provided restriction
                        if field in args.imgtype:
                            # If the input value does not match the image value, fail the image
                            if meta_value != args.imgtype[field]:
                                img_pass = 0
                        img_meta[field] = meta_value
                    # Or use the default value
                    else:
                        img_meta[field] = args.valid_meta[field]

                if args.dates and 'timestamp' in img_meta:
                    in_date_range = check_date_range(args, img_meta['timestamp'])
                    if in_date_range is False:
                        img_pass = 0

                # If the image meets the user's criteria, store the metadata
                if img_pass == 1:
                    meta[filename] = img_meta
                    args.jobcount += 1

    return args, meta


###########################################

# Reads images from a structured directory in
# PhenoFront format
###########################################
def phenofront_parser(args):
    """
    Reads metadata in PhenoFront format.
  
    Args:
        args: (object) argparse object.
    Returns:
        meta: image metadata object.
    Raises:
    
    """
    # Metadata data structure
    meta = {}
    args.jobcount = 0

    # Open the SnapshotInfo.csv file
    csvfile = open(args.dir + '/SnapshotInfo.csv', 'rU')

    # Read the first header line
    header = csvfile.readline()
    header = header.rstrip('\n')

    # Remove whitespace from the field names
    header = header.replace(" ", "")

    # Table column order
    cols = header.split(',')
    colnames = {}
    for i, col in enumerate(cols):
        colnames[col] = i

    # Read through the CSV file
    for row in csvfile:
        row = row.rstrip('\n')
        data = row.split(',')
        img_list = data[colnames['tiles']]
        if img_list[:-1] == ';':
            img_list = img_list[:-1]
        imgs = img_list.split(';')
        for img in imgs:
            if len(img) != 0:
                dirpath = args.dir + '/snapshot' + data[colnames['id']]
                filename = img + '.' + args.type
                if not os.path.exists(dirpath + '/' + filename):
                    args.error_log.write("Something is wrong, file {0}/{1} does not exist".format(dirpath, filename))
                    continue
                    # raise IOError("Something is wrong, file {0}/{1} does not exist".format(dirpath, filename))
                # Metadata from image file name
                metadata = img.split(args.delimiter)
                # Not all images in a directory may have the same metadata structure only keep those that do
                if len(metadata) == args.meta_count:
                    # Image metadata
                    img_meta = {'path': dirpath}
                    img_pass = 1
                    coimg_store = 0
                    # For each of the type of metadata PlantCV keeps track of
                    for field in args.valid_meta:
                        # If the same metadata is found in the image filename, store the value
                        if field in args.fields:
                            meta_value = metadata[args.fields[field]]
                            # If the metadata type has a user-provided restriction
                            if field in args.imgtype:
                                # If the input value does not match the image value, fail the image
                                if meta_value != args.imgtype[field]:
                                    img_pass = 0
                            img_meta[field] = meta_value
                        # If the same metadata is found in the CSV file, store the value
                        elif field in colnames:
                            meta_value = data[colnames[field]]
                            # If the metadata type has a user-provided restriction
                            if field in args.imgtype:
                                # If the input value does not match the image value, fail the image
                                if meta_value != args.imgtype[field]:
                                    img_pass = 0
                            img_meta[field] = meta_value
                        # Or use the default value
                        else:
                            img_meta[field] = args.valid_meta[field]

                    if args.dates:
                        in_date_range = check_date_range(args, img_meta['timestamp'])
                        if in_date_range is False:
                            img_pass = 0

                    if img_pass:
                        args.jobcount += 1

                    if args.coprocess is not None:
                        if img_meta['imgtype'] == args.coprocess:
                            coimg_store = 1

                    # If the image meets the user's criteria, store the metadata
                    if img_pass == 1:
                        # Link image to coprocessed image
                        coimg_pass = 0
                        if args.coprocess is not None:
                            for coimg in imgs:
                                if len(coimg) != 0:
                                    meta_parts = coimg.split(args.delimiter)
                                    coimgtype = meta_parts[args.fields['imgtype']]
                                    if coimgtype == args.coprocess:
                                        if 'camera' in args.fields:
                                            cocamera = meta_parts[args.fields['camera']]
                                            if 'frame' in args.fields:
                                                coframe = meta_parts[args.fields['frame']]
                                                if cocamera == img_meta['camera'] and coframe == img_meta['frame']:
                                                    img_meta['coimg'] = coimg + '.' + args.type
                                                    coimg_pass = 1
                                            else:
                                                if cocamera == img_meta['camera']:
                                                    img_meta['coimg'] = coimg + '.' + args.type
                                                    coimg_pass = 1
                                        else:
                                            img_meta['coimg'] = coimg + '.' + args.type
                                            coimg_pass = 1
                            if coimg_pass == 0:
                                args.error_log.write(
                                    "Could not find an image to coprocess with " + dirpath + '/' + filename + '\n')
                        meta[filename] = img_meta
                    elif coimg_store == 1:
                        meta[filename] = img_meta

    return args, meta

###########################################


# Process images using multiprocessing
###########################################
def process_images_multiproc(jobs):
    for job in jobs:
        os.system(job)


# Multiprocessing pool builder
###########################################
def exe_multiproc(jobs, cpus):
    try:
        p = mp.Pool(processes=cpus)
        p.map_async(process_images_multiproc, jobs).get(9999999)
    except KeyboardInterrupt:
        p.terminate()
        p.join()
        raise ValueError("Execution terminated by user\n")

###########################################


# Build job list
###########################################
def job_builder(args, meta):
    """
    Build a list of image processing jobs.
  
    Args:
        args: (object) argparse object.
        meta: metadata data structure.
    Returns:
    
    Raises:
    
    """
    # Overall job stack. List of list of jobs
    job_stack = []

    # Jobs/CPU (INT): divide the number of images by the number of requested CPU resources
    jobs_per_cpu = args.jobcount / args.cpu

    # Get the list of images
    # images = list(meta.keys())
    images = []
    for img in list(meta.keys()):
        # # If a date range was requested, check whether the image is within range
        # if args.dates:
        #     # Convert image datetime to unix time
        #     timestamp = dt_parser(meta[img]['timestamp'])
        #     time_delta = timestamp - datetime.datetime(1970, 1, 1)
        #     unix_time = (time_delta.days * 24 * 3600) + time_delta.seconds
        #     if unix_time < args.start_date or unix_time > args.end_date:
        #         continue
        if args.coprocess is not None:
            if meta[img]['imgtype'] != args.coprocess:
                images.append(img)
        else:
            images.append(img)

    print("Job list will include " + str(len(images)) + " images" + '\n', file=sys.stderr)

    # For each image
    for img in images:
        if (args.coprocess is not None) and ('coimg' in meta[img]):
            # Create an output file to store the co-image processing results and populate with metadata
            coimg = meta[meta[img]['coimg']]
            coout = file_writer("./{0}/{1}.txt".format(args.jobdir, meta[img]['coimg']))
            coout.write('\t'.join(map(str, ("META", "image", coimg['path'] + '/' + meta[img]['coimg']))) + '\n')
            # Valid metadata
            for m in list(args.valid_meta.keys()):
                coout.write('\t'.join(map(str, ("META", m, coimg[m]))) + '\n')

        # Create an output file to store the image processing results and populate with metadata
        outfile = file_writer("./{0}/{1}.txt".format(args.jobdir, img))
        outfile.write('\t'.join(map(str, ("META", "image", meta[img]['path'] + '/' + img))) + '\n')
        # Valid metadata
        for m in list(args.valid_meta.keys()):
            outfile.write('\t'.join(map(str, ("META", m, meta[img][m]))) + '\n')

        outfile.close()

    # Build the job stack
    # The first n - 1 CPUs will get INT jobs_per_cpu
    # The last CPU will get the remainder
    job = 0
    # For the first n - 1 CPU
    for c in range(1, args.cpu):
        # List of jobs for this CPU
        jobs = []

        # For each job/CPU
        for j in range(0, jobs_per_cpu):
            # Add job to list
            if args.coprocess is not None and ('coimg' in meta[images[job]]):
                job_str = "python {0} --image {1}/{2} --outdir {3} --result ./{4}/{5}.txt --coresult ./{6}/{7}.txt".format(
                    args.pipeline, meta[images[job]]['path'], images[job], args.outdir, args.jobdir, images[job],
                    args.jobdir, meta[images[job]]['coimg'])
                if args.writeimg:
                    job_str += ' --writeimg'
                if args.other_args:
                    job_str += ' ' + args.other_args
                jobs.append(job_str)
            else:
                job_str = "python {0} --image {1}/{2} --outdir {3} --result ./{4}/{5}.txt".format(args.pipeline,
                                                                                           meta[images[job]]['path'],
                                                                                           images[job], args.outdir,
                                                                                           args.jobdir, images[job])
                if args.writeimg:
                    job_str += ' --writeimg'
                if args.other_args:
                    job_str += ' ' + args.other_args
                jobs.append(job_str)

            # Increase the job counter by 1
            job += 1

        # Add the CPU job list to the job stack
        job_stack.append(jobs)

    # Add the remaining jobs to the last CPU
    jobs = []
    for j in range(job, len(images)):
        # Add job to list
        if args.coprocess is not None and ('coimg' in meta[images[j]]):
            job_str = "python {0} --image {1}/{2} --outdir {3} --result ./{4}/{5}.txt --coresult ./{6}/{7}.txt".format(
                args.pipeline, meta[images[j]]['path'], images[j], args.outdir, args.jobdir, images[j], args.jobdir,
                meta[images[j]]['coimg'])
            if args.writeimg:
                job_str += ' --writeimg'
            if args.other_args:
                job_str += ' ' + args.other_args
            jobs.append(job_str)
        else:
            job_str = "python {0} --image {1}/{2} --outdir {3} --result ./{4}/{5}.txt".format(args.pipeline,
                                                                                       meta[images[j]]['path'],
                                                                                       images[j], args.outdir,
                                                                                       args.jobdir, images[j])
            if args.writeimg:
                job_str += ' --writeimg'
            if args.other_args:
                job_str += ' ' + args.other_args
            jobs.append(job_str)
    # Add the CPU job list to the job stack
    job_stack.append(jobs)

    return job_stack


###########################################

# Process results. Parse individual image output files.
###########################################
def process_results(args):
    """
    Get results from individual files.
    Parse the results and recompile for SQLite.
  
    Args:
        args: (object) argparse object.
    Returns:
    
    Raises:
    
    """

    # Metadata table
    metadata_fields = ['image_id', 'run_id']
    metadata_fields.extend(args.valid_meta.keys())
    metadata_fields.append("image")
    # args.metadata_file.write('#' + '\t'.join(map(str, metadata_fields)) + '\n')

    # Feature data table
    feature_fields = ['area', 'hull-area', 'solidity', 'perimeter', 'width', 'height',
                      'longest_axis', 'center-of-mass-x', 'center-of-mass-y', 'hull_vertices',
                      'in_bounds', 'ellipse_center_x', 'ellipse_center_y', 'ellipse_major_axis',
                      'ellipse_minor_axis', 'ellipse_angle', 'ellipse_eccentricity']
    opt_feature_fields = ['y-position', 'height_above_bound', 'height_below_bound',
                          'above_bound_area', 'percent_above_bound_area', 'below_bound_area',
                          'percent_below_bound_area']
    marker_fields = ['marker_area','marker_major_axis_length','marker_minor_axis_length','marker_eccentricity']
    watershed_fields=['estimated_object_count']
    landmark_fields = ['tip_points', 'tip_points_r', 'centroid_r', 'baseline_r', 'tip_number', 'vert_ave_c',
                       'hori_ave_c', 'euc_ave_c', 'ang_ave_c', 'vert_ave_b', 'hori_ave_b', 'euc_ave_b', 'ang_ave_b',
                       'left_lmk', 'right_lmk', 'center_h_lmk', 'left_lmk_r', 'right_lmk_r', 'center_h_lmk_r',
                       'top_lmk', 'bottom_lmk', 'center_v_lmk', 'top_lmk_r', 'bottom_lmk_r', 'center_v_lmk_r']

    # args.features_file.write('#' + '\t'.join(map(str, feature_fields + opt_feature_fields)) + '\n')

    # Signal channel data table
    signal_fields = ['bin-number', 'channel_name', 'values', 'bin_values']

    # bin-number	blue	green	red	lightness	green-magenta	blue-yellow	hue	saturation	value

    # Initialize the database with the schema template if create is true
    args.sq.execute(
        'CREATE TABLE IF NOT EXISTS `runinfo` (`run_id` INTEGER PRIMARY KEY, `datetime` INTEGER NOT NULL, '
        '`command` TEXT NOT NULL);')
    args.sq.execute(
        'CREATE TABLE IF NOT EXISTS `metadata` (`image_id` INTEGER PRIMARY KEY, `run_id` INTEGER NOT NULL, `' +
        '` TEXT NOT NULL, `'.join(map(str, metadata_fields[2:])) + '` TEXT NOT NULL);')
    args.sq.execute(
        'CREATE TABLE IF NOT EXISTS `features` (`image_id` INTEGER PRIMARY KEY, `' + '` TEXT NOT NULL, `'.join(
            map(str, feature_fields + opt_feature_fields + marker_fields+ watershed_fields + landmark_fields)) + '` TEXT NOT NULL);')
    args.sq.execute(
        'CREATE TABLE IF NOT EXISTS `analysis_images` (`image_id` INTEGER NOT NULL, `type` TEXT NOT NULL, '
        '`image_path` TEXT NOT NULL);')
    args.sq.execute(
        'CREATE TABLE IF NOT EXISTS `signal` (`image_id` INTEGER NOT NULL, `' + '` TEXT NOT NULL, `'.join(
            map(str, signal_fields)) + '` TEXT NOT NULL);')

    # Walk through the image processing job directory and process data from each file
    for (dirpath, dirnames, filenames) in os.walk(args.jobdir):
        for filename in filenames:
            # Make sure file is a text file
            if 'text/plain' in mimetypes.guess_type(filename):
                meta = {}
                images = {}
                features = []
                feature_data = {}
                signal = []
                signal_data = {}
                boundary = []
                boundary_data = {}
                marker = []
                marker_data = {}
                watershed=[]
                watershed_data={}
                landmark = []
                landmark_data = {}
                # Open results file
                with open(dirpath + '/' + filename) as results:
                    # For each line in the file
                    for row in results:
                        # Remove the newline character
                        row = row.rstrip('\n')
                        # Split the line by tab characters
                        cols = row.split('\t')
                        # If the data is of class meta, store in the metadata dictionary
                        if cols[0] == 'META':
                            meta[cols[1]] = cols[2]
                        # If the data is of class image, store in the image dictionary
                        elif cols[0] == 'IMAGE':
                            images[cols[1]] = cols[2]
                        # If the data is of class shapes, store in the shapes dictionary
                        elif cols[0] == 'HEADER_SHAPES':
                            features = cols
                        elif cols[0] == 'SHAPES_DATA':
                            for i, datum in enumerate(cols):
                                if i > 0:
                                    feature_data[features[i]] = datum
                        # If the data is of class histogram/signal, store in the signal dictionary
                        elif cols[0] == 'HEADER_HISTOGRAM':
                            signal = cols
                        elif cols[0] == 'HISTOGRAM_DATA':
                            for i, datum in enumerate(cols):
                                if i > 0:
                                    signal_data[signal[i]] = datum
                        # If the data is of class boundary (horizontal rule), store in the boundary dictionary
                        elif 'HEADER_BOUNDARY' in cols[0]:
                            boundary = cols
                            # Temporary hack
                            boundary_data['y-position'] = cols[0].replace('HEADER_BOUNDARY', '')
                        elif cols[0] == 'BOUNDARY_DATA':
                            for i, datum in enumerate(cols):
                                if i > 0:
                                    boundary_data[boundary[i]] = datum
                        elif 'HEADER_MARKER' in cols[0]:
                            marker = cols
                            # Temporary hack
                            marker[1] = 'marker_area'
                        elif 'MARKER_DATA' in cols[0]:
                            for i, datum in enumerate(cols):
                                if i > 0:
                                    marker_data[marker[i]] = datum
                        elif 'HEADER_WATERSHED' in cols[0]:
                            watershed=cols
                            watershed[1]='estimated_object_count'
                        elif 'WATERSHED_DATA' in cols[0]:
                            for i, datum in enumerate(cols):
                                if i>0:
                                    watershed_data[watershed[i]]=datum
                        elif 'HEADER_LANDMARK' in cols[0]:
                            landmark = cols
                        elif 'LANDMARK_DATA' in cols[0]:
                            for i, datum in enumerate(cols):
                                if i > 0:
                                    landmark_data[landmark[i]] = datum

                # Check to see if the image failed, if not continue

                # Print the image metadata to the aggregate output file
                args.image_id += 1
                meta['image_id'] = args.image_id
                meta['run_id'] = args.run_id

                meta_table = []
                for field in metadata_fields:
                    meta_table.append(meta[field])

                if len(feature_data) != 0:
                    args.metadata_file.write('|'.join(map(str, meta_table)) + '\n')

                    # Print the image feature data to the aggregate output file
                    feature_data['image_id'] = args.image_id

                    # Boundary data is optional, if it's not there we need to add in placeholder data
                    if len(boundary_data) == 0:
                        for field in opt_feature_fields:
                            boundary_data[field] = 0
                    feature_data.update(boundary_data)

                    # Marker data is optional, if it's not there we need to add in placeholder data
                    if len(marker_data) == 0:
                        for field in marker_fields:
                            marker_data[field] = 0
                    feature_data.update(marker_data)

                    # Watershed data is optional, if it's not there we need to add in placeholder data
                    if len(watershed_data) == 0:
                        for field in watershed_fields:
                            watershed_data[field] = 0
                    feature_data.update(watershed_data)

                    # Landmark data is optional, if it's not there we need to add in placeholder data
                    if len(landmark_data) == 0:
                        for field in landmark_fields:
                            landmark_data[field] = 0
                    feature_data.update(landmark_data)

                    feature_table = [args.image_id]
                    for field in feature_fields + opt_feature_fields + marker_fields + watershed_fields + landmark_fields:
                        feature_table.append(feature_data[field])

                    args.features_file.write('|'.join(map(str, feature_table)) + '\n')

                    # Print the analysis image data to the aggregate output file
                    for img_type in images:
                        args.analysis_images_file.write(
                            '|'.join(map(str, (args.image_id, img_type, images[img_type]))) + '\n')

                    # Print the image signal data to the aggregate output file
                    for key in signal_data.keys():
                        if key != 'bin-number' and key != 'bin-values':
                            signal_data[key] = signal_data[key].replace('[', '')
                            signal_data[key] = signal_data[key].replace(']', '')
                            signal_table = [args.image_id, signal_data['bin-number'],key, signal_data[key],signal_data['bin-values']]
                            args.signal_file.write('|'.join(map(str, signal_table)) + '\n')
                else:
                    args.fail_log.write('|'.join(map(str, meta_table)) + '\n')

                    args.metadata_file.write('|'.join(map(str, meta_table)) + '\n')

                    feature_table = [args.image_id]

                    for field in feature_fields + opt_feature_fields + marker_fields + watershed_fields+ landmark_fields:
                        feature_table.append(0)

                    args.features_file.write('|'.join(map(str, feature_table)) + '\n')

###########################################


# Check to see if the image was taken between a specified date range
###########################################
def check_date_range(args, img_time):
    """Check image time versus included date range

    :param args: (object) argparse object.
    :param img_time: date-time string
    :return: boolean
    """
    # Convert image datetime to unix time
    timestamp = dt_parser(img_time)
    time_delta = timestamp - datetime.datetime(1970, 1, 1)
    unix_time = (time_delta.days * 24 * 3600) + time_delta.seconds
    # Does the image date-time fall outside or inside the included range
    if unix_time < args.start_date or unix_time > args.end_date:
        return False
    else:
        return True
###########################################

if __name__ == '__main__':
    main()
# Naive Bayes

import os
import cv2
import numpy as np
from scipy import stats


def naive_bayes(imgdir, maskdir, outfile, mkplots=False):
    """Naive Bayes training function

    Inputs:
    imgdir  = Path to a directory of original 8-bit RGB images. 
    maskdir = Path to a directory of binary mask images. Mask images must have the same name as their corresponding
              color images.
    outfile = Name of the output text file that will store the color channel probability density functions.
    mkplots = Make PDF plots (True or False).
    
    :param imgdir: str
    :param maskdir: str
    :param outfile: str
    :param mkplots: bool
    """
    # Initialize color channel ndarrays for plant (foreground) and background
    plant = {"hue": np.array([], dtype=np.uint8), "saturation": np.array([], dtype=np.uint8),
             "value": np.array([], dtype=np.uint8)}
    background = {"hue": np.array([], dtype=np.uint8), "saturation": np.array([], dtype=np.uint8),
                  "value": np.array([], dtype=np.uint8)}

    # Walk through the image directory
    print("Reading images...")
    for (dirpath, dirnames, filenames) in os.walk(imgdir):
        for filename in filenames:
            # Is this an image type we can work with?
            if filename[-3:] in ['png', 'jpg', 'jpeg']:
                # Does the mask exist?
                if os.path.exists(os.path.join(maskdir, filename)):
                    # Read the image as BGR
                    img = cv2.imread(os.path.join(dirpath, filename), 1)
                    # Read the mask as grayscale
                    mask = cv2.imread(os.path.join(maskdir, filename), 0)

                    # Convert the image to HSV and split into component channels
                    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
                    hue, saturation, value = cv2.split(hsv)

                    # Store channels in a dictionary
                    channels = {"hue": hue, "saturation": saturation, "value": value}

                    # Split channels into plant and non-plant signal
                    for channel in channels.keys():
                        fg, bg = _split_plant_background_signal(channels[channel], mask)

                        # Randomly sample from the plant class (sample 10% of the pixels)
                        fg = fg[np.random.random_integers(0, len(fg) - 1, len(fg) / 10)]
                        # Randomly sample from the background class the same n as the plant class
                        bg = bg[np.random.random_integers(0, len(bg) - 1, len(fg))]
                        plant[channel] = np.append(plant[channel], fg)
                        background[channel] = np.append(background[channel], bg)

    # Calculate a probability density function for each channel using a Gaussian kernel density estimator
    # Create an output file for the PDFs
    out = open(outfile, "w")
    out.write("class\tchannel\t" + "\t".join(map(str, range(0, 256))) + "\n")
    for channel in plant.keys():
        print("Calculating PDF for the " + channel + " channel...")
        plant_kde = stats.gaussian_kde(plant[channel])
        bg_kde = stats.gaussian_kde(background[channel])
        # Calculate p from the PDFs for each 8-bit intensity value and save to outfile
        plant_pdf = plant_kde(range(0, 256))
        out.write("plant\t" + channel + "\t" + "\t".join(map(str, plant_pdf)) + "\n")
        bg_pdf = bg_kde(range(0, 256))
        out.write("background\t" + channel + "\t" + "\t".join(map(str, bg_pdf)) + "\n")
        if mkplots:
            # If mkplots is True, make the PDF charts
            _plot_pdf(channel, os.path.dirname(outfile), plant=plant_pdf, background=bg_pdf)

    out.close()


def naive_bayes_multiclass(samples_file, outfile, mkplots=False):
    """Naive Bayes training function for two or more classes from sampled pixel RGB values
    
    Inputs:
    samples_file = Input text file containing sampled pixel RGB values for each training class. The file should be a
                   tab-delimited table with one training class per column. The required first row must contain header
                   labels for each class. The row values for each class must be comma-delimited RGB values. See the
                   file plantcv/tests/data/sampled_rgb_points.txt for an example.
    outfile      = Name of the output text file that will store the color channel probability density functions.
    mkplots      = Make PDF plots (True or False).
    
    :param samples_file: str
    :param outfile: str
    :param mkplots: bool
    """
    # Initialize a dictionary to store sampled RGB pixel values for each input class
    sample_points = {}
    # Open the sampled points text file
    f = open(samples_file, "r")
    # Read the first line and use the column headers as class labels
    header = f.readline()
    header = header.rstrip("\n")
    class_list = header.split("\t")
    # Initialize a dictionary for the red, green, and blue channels for each class
    for cls in class_list:
        sample_points[cls] = {"red": [], "green": [], "blue": []}
    # Loop over the rest of the data in the input file
    for row in f:
        # Remove newlines and quotes
        row = row.rstrip("\n")
        row = row.replace('"', '')
        # If this is not a blank line, parse the data
        if len(row) > 0:
            # Split the row into a list of points per class
            points = row.split("\t")
            # For each point per class
            for i, point in enumerate(points):
                # Split the point into red, green, and blue integer values
                red, green, blue = map(int, point.split(","))
                # Append each intensity value into the appropriate class list
                sample_points[class_list[i]]["red"].append(red)
                sample_points[class_list[i]]["green"].append(green)
                sample_points[class_list[i]]["blue"].append(blue)
    f.close()
    # Initialize a dictionary to store probability density functions per color channel in HSV colorspace
    pdfs = {"hue": {}, "saturation": {}, "value": {}}
    # For each class
    for cls in class_list:
        # Create a blue, green, red-formatted image ndarray with the class RGB values
        bgr_img = cv2.merge((np.asarray(sample_points[cls]["blue"], dtype=np.uint8),
                             np.asarray(sample_points[cls]["green"], dtype=np.uint8),
                             np.asarray(sample_points[cls]["red"], dtype=np.uint8)))
        # Convert the BGR ndarray to an HSV ndarray
        hsv_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2HSV)
        # Split the HSV ndarray into the component HSV channels
        hue, saturation, value = cv2.split(hsv_img)
        # Create an HSV channel dictionary that stores the channels as lists (horizontally stacked ndarrays)
        channels = {"hue": np.hstack(hue), "saturation": np.hstack(saturation), "value": np.hstack(value)}
        # For each channel
        for channel in channels.keys():
            # Create a kernel density estimator for the channel values (Guassian kernel)
            kde = stats.gaussian_kde(channels[channel])
            # Use the KDE to calculate a probability density function for the channel
            # Sample at each of the possible 8-bit values
            pdfs[channel][cls] = kde(range(0, 256))
    if mkplots:
        # If mkplots is True, generate a density curve plot per channel for each class
        for channel, cls in pdfs.items():
            _plot_pdf(channel, os.path.dirname(outfile), **cls)
    # Write the PDFs to a text file
    out = open(outfile, "w")
    # Write the column labels
    out.write("class\tchannel\t" + "\t".join(map(str, range(0, 256))) + "\n")
    # For each channel
    for channel, cls in pdfs.items():
        # For each class
        for class_name, pdf in cls.items():
            # Each row is the PDF for the given class and color channel
            out.write(class_name + "\t" + channel + "\t" + "\t".join(map(str, pdf)) + "\n")


def _split_plant_background_signal(channel, mask):
    """Split a single-channel image by foreground and background using a mask

    :param channel: ndarray
    :param mask: ndarray
    :return plant: ndarray
    :return background: ndarray
    """
    plant = channel[np.where(mask == 255)]
    background = channel[np.where(mask == 0)]

    return plant, background


def _plot_pdf(channel, outdir, **kwargs):
    """Plot the probability density function of one or more classes for the given channel

    :param channel: str
    :param outdir: str
    :param kwargs: dict
    """
    import matplotlib
    matplotlib.use("Agg")
    from matplotlib import pyplot as plt
    for class_name, pdf in kwargs.items():
        plt.plot(pdf, label=class_name)
    plt.legend(loc="best")
    plt.savefig(os.path.join(outdir, str(channel) + "_pdf.png"))
    plt.close()
# Plot histogram

import cv2
import numpy as np

def plot_hist(img, name=False):
    """Plot a histogram using the pyplot library.

    Inputs:
    img  = image to analyze
    name = name for plot output

    :param img: numpy array
    :param name: str
    :return: bins,hist
    """

    import matplotlib
    matplotlib.use('Agg')
    from matplotlib import pyplot as plt

    # get histogram
    if img.dtype == 'uint8':
        hist = cv2.calcHist([img], [0], None, [256], [0, 255])
        bins = range(0, 256, 1)

        if name != False:
            # open pyplot plotting window using hist data
            plt.plot(hist)
            # set range of x-axis
            xaxis = plt.xlim([0, 255])
            fig_name = name + '.png'
            # write the figure to current directory
            plt.savefig(fig_name)
            # close pyplot plotting window
            plt.clf()

    else:
        hist, bins = np.histogram(img, bins='auto')

        if name != False:
            # open pyplot plotting window using hist data
            plt.plot(bins[:-1], hist)
            plt.xticks(bins[:-1], rotation='vertical', fontsize=4)
            # set range of x-axis
            # xaxis = plt.xlim([0, bins.max()])
            fig_name = name + '.png'
            # write the figure to current directory
            plt.savefig(fig_name)
            # close pyplot plotting window
            plt.clf()

    return bins, hist
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-c", "--csv", help="PhenoFront CSV file.")
  parser.add_argument("-r", "--random", help="number of random images you would like", type=int, required=False)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=True)
  parser.add_argument("-s", "--camera", help= "vis_sv, vis_tv, nir_sv, nir_tv, flu_tv", required=True)
  parser.add_argument("-t", "--type", help="Image format type.", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Grab Random Image Ids From Database
def grab_random(database,random, camera, outdir, type):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
    
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  imageid_list=[]
  num=random
  print num
  list_random= db.execute('select * from snapshots where camera=? order by random() limit ?' , (camera,num,))
  for i, x in enumerate(list_random):
    imgid=x['image_id']
    imageid_list.append(imgid)
  
  print imageid_list
  
  for i,t in enumerate(imageid_list):
    get_image=db.execute('select * from snapshots where image_id=?',(t,))
    for a, data in enumerate(get_image):
      img_name = outdir + '/' + data['plant_id'] + '_' + data['camera'] + '_' + str(data['frame']) + '_z' + str(data['zoom']) + '_h' + str(data['lifter']) + '_' + str(t) + '.' + type
      copy(data['image_path'], img_name)
    print "copying"
    print img_name   
        

### Main pipeline
def main():
  # Get options
  args = options()
  

  grab_random(args.database, args.random, args.camera, args.outdir, args.type)


if __name__ == '__main__':
  main()
import os
import cv2
import numpy as np
from datetime import datetime
from . import print_image
from . import plot_image
from . import apply_mask


def cluster_contour_splitimg(device, img, grouped_contour_indexes, contours, outdir=None, file=None,
                             filenames=None, debug=None):

    """
    This function takes clustered contours and splits them into multiple images, also does a check to make sure that
    the number of inputted filenames matches the number of clustered contours.

    Inputs:
    device                  = Counter for image processing steps
    img                     = ideally a masked RGB image.
    grouped_contour_indexes = output of cluster_contours, indexes of clusters of contours
    contours                = contours to cluster, output of cluster_contours
    outdir                  = out directory for output images
    file                    = the name of the input image to use as a base name,
                              output of filename from read_image function
    filenames               = input txt file with list of filenames in order from top to bottom left to right
                              (likely list of genotypes)
    debug                   = print debugging images

    Returns:
    device                  = pipeline step counter
    output_path             = array of paths to output images

    :param device: int
    :param img: ndarray
    :param grouped_contour_indexes: list
    :param contours: list
    :param outdir: str
    :param file: str
    :param filenames: str
    :param debug: str
    :return device: int
    :return output_path: str
    """

    # get names to split also to check the target number of objects

    i = datetime.now()
    timenow = i.strftime('%m-%d-%Y_%H:%M:%S')

    if file == None:
        filebase = timenow
    else:
        filebase = file[:-4]

    if filenames == None:
        l = len(grouped_contour_indexes)
        namelist = []
        for x in range(0, l):
            namelist.append(x)
    else:
        with open(filenames, 'r') as n:
            namelist = n.read().splitlines()
        n.close()

    # make sure the number of objects matches the namelist, and if not, remove the smallest grouped countor
    # removing contours is not ideal but the lists don't match there is a warning to check output

    if len(namelist) == len(grouped_contour_indexes):
        corrected_contour_indexes = grouped_contour_indexes
    elif len(namelist) < len(grouped_contour_indexes):
        print("Warning number of names is less than number of grouped contours, attempting to fix, to double check "
              "output")
        diff = len(grouped_contour_indexes) - len(namelist)
        size = []
        for i, x in enumerate(grouped_contour_indexes):
            totallen = []
            for a in x:
                g = i
                la = len(contours[a])
                totallen.append(la)
            sumlen = np.sum(totallen)
            size.append((sumlen, g, i))

        dtype = [('len', int), ('group', list), ('index', int)]
        lencontour = np.array(size, dtype=dtype)
        lencontour = np.sort(lencontour, order='len')

        rm_contour = lencontour[diff:]
        rm_contour = np.sort(rm_contour, order='group')
        corrected_contour_indexes = []

        for x in rm_contour:
            index = x[2]
            corrected_contour_indexes.append(grouped_contour_indexes[index])

    elif len(namelist) > len(grouped_contour_indexes):
        print("Warning number of names is more than number of  grouped contours, double check output")
        diff = len(namelist) - len(grouped_contour_indexes)
        namelist = namelist[0:-diff]
        corrected_contour_indexes = grouped_contour_indexes

    # create filenames

    group_names = []
    for i, x in enumerate(namelist):
        plantname = str(filebase) + '_' + str(x) + '_p' + str(i) + '.jpg'
        group_names.append(plantname)

    # split image

    output_path = []

    for y, x in enumerate(corrected_contour_indexes):
        if outdir != None:
            savename = os.path.join(str(outdir), group_names[y])
        else:
            savename = os.path.join(".", group_names[y])
        iy, ix, iz = np.shape(img)
        mask = np.zeros((iy, ix, 3), dtype=np.uint8)
        masked_img = np.copy(img)
        for a in x:
            cv2.drawContours(mask, contours, a, (255, 255, 255), -1, lineType=8)

        mask_binary = mask[:, :, 0]

        if np.sum(mask_binary) == 0:
            pass
        else:
            retval, mask_binary = cv2.threshold(mask_binary, 254, 255, cv2.THRESH_BINARY)
            device, masked1 = apply_mask(masked_img, mask_binary, 'white', device, debug)
            if outdir != None:
                print_image(masked1, savename)
            output_path.append(savename)

            if debug == 'print':
                print_image(masked1, (str(device) + '_clusters.png'))
            elif debug == 'plot':
                if len(np.shape(masked1)) == 3:
                    plot_image(masked1)
                else:
                    plot_image(masked1, cmap='gray')
                    plot_image(masked1)

    return device, output_path
#!/usr/bin/env python

import argparse
import numpy as np
import sys, os
from os import listdir
import plantcv as pcv
import datetime

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get images from an SQLite database and some input information")
  parser.add_argument("-d", "--directory", help="path to directory of images to average.")
  parser.add_argument("-o", "--outdir", help="Output directory.", required=False)
  args = parser.parse_args()
  return args


### Functions

def average_all_img(directory,outdir):
    allfiles=os.listdir(directory)
    
    path=str(directory)
    
    allpaths=[]
    
    for files in allfiles:
        p=path+str(files)
        allpaths.append(p)
    
    img, path, filename = pcv.readimage(allpaths[0])
    n=len(allpaths)

    
    if len(np.shape(img))==3:
        ix,iy,iz=np.shape(img)
        arr=np.zeros((ix,iy,iz),np.float)
    else:
        ix,iy=np.shape(img)
        arr=np.zeros((ix,iy,iz),np.float)

    # Build up average pixel intensities, casting each image as an array of floats
    for i,paths in enumerate(allpaths):
        img,path,filename=pcv.readimage(allpaths[i])
        imarr=np.array(img,dtype=np.float)
        arr=arr+imarr/n

    #Round values in array and cast as 8-bit integer
    arr=np.array(np.round(arr),dtype=np.uint8)

    pcv.print_image(arr, (str(outdir)+"average_"+str(allfiles[0])))



### Main pipeline

def main():
    
  # Get options
  args = options()
  
  average_all_img(args.directory, args.outdir)

if __name__ == '__main__':
  main()# Binary image adaptive threshold

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def adaptive_threshold(img, maxValue, thres_type, object_type, device, debug=None):
    """Creates a binary image from a grayscaled image using adaptive thresholding

    Inputs:
    img         = img object, grayscale
    maxValue    = value to apply above threshold (usually 255 = white)
    thres_type  = type for thresholding (gaussian or mean)
    object_type = light or dark
                  - If object is light then standard thresholding is done
                  - If object is dark then inverse thresholding is done
    device      = device number. Used to count steps in the pipeline
    debug       = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device      = device number
    t_img       = image object as numpy array

    :param img: numpy array
    :param maxValue: int
    :param thres_type: str
    :param object_type: str
    :param device: int
    :param debug: str
    :return device: int
    :return t_img: numpy array
    """

    device += 1

    thres = 0
    # check to see which type of adaptive threshold to use
    if thres_type == 'mean':
        thres = cv2.ADAPTIVE_THRESH_MEAN_C
    elif thres_type == 'gaussian':
        thres = cv2.ADAPTIVE_THRESH_GAUSSIAN_C
    else:
        fatal_error('threshold type ' + str(thres_type) + ' is not "mean" or "gaussian"!')

    # check whether to invert the image or not and make an ending extension
    obj = 0
    ext = ''
    if object_type == 'light':
        ext = '.png'
        obj = cv2.THRESH_BINARY
    elif object_type == 'dark':
        ext = '_inv.png'
        obj = cv2.THRESH_BINARY_INV
    else:
        fatal_error('Object type ' + str(object_type) + ' is not "light" or "dark"!')

    # threshold the image based on the thres_type and object_type
    t_img = cv2.adaptiveThreshold(img, maxValue, thres, obj, 11, 2)

    # print out the image if the debug is true
    if debug == 'print':
        name = str(device) + '_adaptive_threshold_' + thres_type + ext
        print_image(t_img, name)
    elif debug == 'plot':
        plot_image(t_img, cmap='gray')

    return device, t_img
# Binary image auto threshold

import cv2
from . import print_image
from . import plot_image


def otsu_auto_threshold(img, maxValue, object_type, device, debug=None):
    """Creates a binary image from a grayscaled image using Otsu's thresholding.

    Inputs:
    img         = img object, grayscale
    maxValue    = value to apply above threshold (usually 255 = white)
    object_type = light or dark
                  - If object is light then standard thresholding is done
                  - If object is dark then inverse thresholding is done
    device      = device number. Used to count steps in the pipeline
    debug       = True/False. If True, print image

    Returns:
    device      = device number
    t_img       = the thresholded image


    :param img: numpy array
    :param maxValue: int
    :param object_type: str
    :param device: int
    :param debug: bool
    :return device: int
    :return t_img: numpy array
    """
    device += 1

    # check whether to inverse the image or not and make an ending extension
    obj = 0
    extension = ''
    if object_type == 'light':
        extension = '.png'
        obj = cv2.THRESH_BINARY + cv2.THRESH_OTSU
    elif object_type == 'dark':
        extension = '_inv.png'
        obj = cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU

    # threshold the image based on the object type using otsu's binarization
    t_val, t_img = cv2.threshold(img, 0, maxValue, obj)

    if debug == 'print':
        name = str(device) + '_otsu_auto_threshold_' + str(t_val) + str(extension)
        print_image(t_img, name)
    elif debug == 'plot':
        plot_image(t_img, cmap="gray")

    return device, t_img
# RGB -> Gray

import cv2
from . import print_image
from . import plot_image


def rgb2gray(img, device, debug=None):
    """Convert image from RGB colorspace to Gray.

    Inputs:
    img    = image object, RGB colorspace
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    gray   = grayscale image

    :param img: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return gray: numpy array
    """

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    device += 1
    if debug == 'print':
        print_image(gray, (str(device) + '_gray.png'))
    elif debug == 'plot':
        plot_image(gray, cmap='gray')
    return device, gray
#!/usr/bin/env python

import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--dir", help="directory of files to rename.")
  parser.add_argument("-c", "--camera", help="VIS_SV, VIS_TV.")
  parser.add_argument("-a", "--angle", help="0,90,180,270.")
  parser.add_argument("-z", "--zoom", help="zoom.")
  parser.add_argument("-e", "--date", help="startdate in epoch time", type=int)  
  parser.add_argument("-i", "--timeinterval", help="timeinterval in seconds.", type=int)
  parser.add_argument("-p", "--imagetype", help="jpg,png.")  
  args = parser.parse_args()
  return args


def read_dir(directory):
    for a,b,c in os.walk(directory):
        filenames=c
        path=a
    return filenames,path

def rename_move(filenames,path,camera,angle, zoom, date, timeinterval, imagetype):
    new_dir=str(path)+str(datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S"))
    os.mkdir(new_dir)
    
    for i,x in enumerate(filenames):
        timeinterval_x=int(date)+int(((i+1)*timeinterval))
        new_time=datetime.datetime.fromtimestamp(int(timeinterval_x)).strftime('%Y-%m-%d %H_%M_%S')
        new_x=x.replace("_",".",2)
        new_name=str(new_x)+'-'+str(new_time)+'-'+str(camera)+'_'+str(angle)+'_'+str(zoom)+'.'+str(imagetype)
        copy(str(path)+ str(x),str(new_dir)+'/'+str(new_name))
        
        
### Main pipeline
def main():
  # Get options
  args = options()
  
  filenames,path=read_dir(args.dir)
  rename_move(filenames,path, args.camera, args.angle, args.zoom, args.date, args.timeinterval, args.imagetype)
  
if __name__ == '__main__':
  main()
#!/usr/bin/env python



# White Balance correction function by Monica Tessman and Malia Gehan

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import apply_mask
from . import fatal_error


def white_balance(device, img, debug=None, roi=None):
    """Corrects the exposure of an image based on its histogram.

    Inputs:
    device  = pipeline step counter
    img     = An RGB image on which to perform the correction, correction is done on each channel and then reassembled,
              alternatively a single channel can be input but is not recommended.
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.
    roi     = A list of 4 points (x, y, width, height) that form the rectangular ROI of the white color standard.
              If a list of 4 points is not given, whole image will be used.

    Returns:
    device  = pipeline step counter
    img     = Image after exposure correction

    :param device: int
    :param img: ndarray
    :param debug: str
    :param roi: list
    """
    device += 1

    ori_img = np.copy(img)

    if roi is not None:
        roiint = all(isinstance(item, int) for item in roi)

        if len(roi) != 4 | roiint is False:
            fatal_error('If ROI is used ROI must have 4 elements as a list and all must be integers')
    else:
        pass

    if len(np.shape(img)) == 3:
        iy, ix, iz = np.shape(img)
        mask = np.zeros((iy, ix, 3), dtype=np.uint8)
        ori_img2 = np.copy(ori_img)

        if roi is None:
            x = 0
            y = 0
            w = ix
            h = iy

        else:
            x = roi[0]
            y = roi[1]
            w = roi[2]
            h = roi[3]

        cv2.rectangle(mask, (x, y), (x + w, y + h), (255, 255, 255), -1)
        cv2.rectangle(ori_img, (x, y), (x + w, y + h), (0, 255, 0), 3)

        mask_binary = mask[:, :, 0]
        retval, mask_binary = cv2.threshold(mask_binary, 254, 255, cv2.THRESH_BINARY)

        _, masked = apply_mask(ori_img2, mask_binary, 'black', 0, debug=None)

        channel1 = np.amax(masked[:, :, 0])
        channel2 = np.amax(masked[:, :, 1])
        channel3 = np.amax(masked[:, :, 2])

        alpha1 = 255 / float(channel1)

        alpha2 = 255 / float(channel2)

        alpha3 = 255 / float(channel3)

        # Converts values greater than hmax to 255 and scales all others by alpha

        correctedimg1 = np.asarray(np.where(img[:, :, 0] <= channel1, np.multiply(alpha1, img[:, :, 0]), 255), np.uint8)
        correctedimg2 = np.asarray(np.where(img[:, :, 1] <= channel2, np.multiply(alpha2, img[:, :, 1]), 255), np.uint8)
        correctedimg3 = np.asarray(np.where(img[:, :, 2] <= channel3, np.multiply(alpha3, img[:, :, 2]), 255), np.uint8)

        finalcorrected = np.dstack((correctedimg1, correctedimg2, correctedimg3))

        if debug == 'print':
            print_image(ori_img, (str(device) + '_whitebalance_roi.png'))
            print_image(finalcorrected, (str(device) + '_whitebalance.png'))
        elif debug == 'plot':
            if len(np.shape(ori_img)) == 3:
                plot_image(ori_img)
                plot_image(finalcorrected)
            else:
                plot_image(ori_img, cmap='gray')
                plot_image(finalcorrected, cmap='gray')

        return device, finalcorrected

    else:
        if img.dtype == 'uint8':

            if roi is not None:
                x = roi[0]
                y = roi[1]
                w = roi[2]
                h = roi[3]

                hist = cv2.calcHist(tuple(img[y:y + h, x:x + w]), [0], None, [256], [0, 256])
                cv2.rectangle(ori_img, (x, y), (x + w, y + h), (0, 255, 0), 3)

            else:
                hist = cv2.calcHist(tuple(img), [0], None, [256], [0, 256])  # Creates histogram of original image

            hmax = np.argmax(hist)
            alpha = 255 / float(hmax)
            finalcorrected = np.asarray(np.where(img <= hmax, np.multiply(alpha, img), 255), np.uint8)

        elif img.dtype == 'uint16':

            if roi is not None:
                x = roi[0]
                y = roi[1]
                w = roi[2]
                h = roi[3]

                hist, bins = np.histogram(img[y:y + h, x:x + w], bins='auto')
                cv2.rectangle(ori_img, (x, y), (x + w, y + h), (0, 0, 0), 3)

            else:
                hist, bins = np.histogram(img, bins='auto')

            hmax = np.amax(bins)
            alpha = 65536 / float(hmax)
            finalcorrected = np.asarray(np.where(img <= hmax, np.multiply(alpha, img), 65536), np.uint16)

        # Calculates index of maximum of histogram and finds alpha based on the peak

        if debug == 'print':
            print_image(ori_img, (str(device) + '_whitebalance_roi.png'))
            print_image(finalcorrected, (str(device) + '_whitebalance.png'))

        elif debug == 'plot':
            plot_image(ori_img, cmap='gray')
            plot_image(finalcorrected, cmap='gray')

        return device, finalcorrected
# Function to scan for pseudolandmarks along the x-axis

import cv2
import numpy as np
from . import plot_image


def x_axis_pseudolandmarks(obj, mask, img, device, debug=None):
    """Divide up object contour into 20 equidistance segments and generate landmarks for each

    Inputs:
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    device   = a counter variable
    debug    = True/False. If True, print image

    Returns:
    device   = pipeline step counter
    top      =
    bottom   =
    center_v =

    :param obj: list
    :param mask: ndarray
    :param img: ndarray
    :param device: int
    :param debug: str
    :return device: int
    :return top:
    :return bottom:
    :return center_v:
    """

    # Lets get some landmarks scanning along the x-axis
    device += 1
    if not np.any(obj):
        return device, ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA')
    x, y, width, height = cv2.boundingRect(obj)
    extent = width
    # If width is greater than 21 pixels make 20 increments (5% intervals)
    if extent >= 21:
        inc = extent / 21
        # Define variable for max points and min points
        pts_max = []
        pts_min = []
        # Get max and min points for each of the intervals
        for i in range(1, 21):
            if (i == 1):
                pt_max = x + (inc * i)
                pt_min = x 
            else:
                pt_max = x + (inc * i)
                pt_min = x + (inc * (i - 1))
            # Put these in an array
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        # Combine max and min into a set of tuples
        point_range = list(zip(pts_min, pts_max))
        # define some list variables to fill
        col_median = []
        col_ave = []
        max_height = []
        top_points = []
        bottom_points = []
        x_vals = []
        x_centroids = []
        y_centroids = []
        # For each of the 20 intervals
        for pt in point_range:
            # Get the left and right bounds    
            left_point, right_point = pt
            # Get all cols within these two points
            cols = []
            ups = []
            bps = []
            # Get a continuous list of the values between the left and the right of the interval save as vals
            vals = list(range(left_point, right_point))
            # For each col... get all coordinates from object contour that match col
            for v in vals:
                # Value is all entries that match the col
                value = obj[v == obj[:, 0, 0]]
                if len(value) > 0:
                    # Could potentially be more than two points in all contour in each pixel row
                    # Grab largest y coordinate (row)
                    largest = value[:, 0, 1].max()
                    # Grab smallest y coordinate (row)
                    smallest = value[:, 0, 1].min()
                    # Take the difference between the two (this is how far across the object is on this plane)
                    col_width = largest - smallest
                    # Append this value to a list
                    cols.append(col_width)
                    ups.append(smallest)
                    bps.append(largest)
                if len(value) == 0:
                    col_width = 1
                    cols.append(col_width)
                    ups.append(1)
                    bps.append(1)
            # For each of the points find the median and average width
            col_median.append(np.median(np.array(cols)))
            col_ave.append(np.mean(np.array(cols)))
            max_height.append(np.max(np.array(cols)))
            top_points.append(np.mean(smallest))
            bottom_points.append(np.mean(largest))
            xval = int((left_point + right_point) / 2)
            x_vals.append(xval)
            # Make a copy of the mask; we want to get landmark points from this
            window = np.copy(mask)
            window[:, :left_point] = 0
            window[:, right_point:] = 0
            s = cv2.moments(window)
            # Centroid (center of mass x, center of mass y)
            if largest - smallest > 3:
                    if s['m00'] > 0.001:
                        smx, smy = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                        x_centroids.append(int(smx))
                        y_centroids.append(int(smy))
                    if s['m00'] < 0.001:
                        smx, smy = (s['m10'] / 0.001, s['m01'] / 0.001)
                        x_centroids.append(int(smx))
                        y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = xval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        # Get the indicie of the largest median/average y-axis value (if there is a tie it takes largest index)
        # indice_median = col_median.index(max(col_median))
        # indice_ave = col_ave.index(max(col_ave))
        # median_value = col_median[indice_median]
        # ave_value = col_ave[indice_ave]
        # max_value = max_width[indice_ave]
        top = zip(x_vals, top_points)
        top = np.array(top)
        top.shape = (20, 1, 2)
        bottom = list(zip(x_vals, bottom_points))
        bottom = np.array(bottom)
        bottom.shape = (20, 1, 2)
        center_v = list(zip(x_centroids, y_centroids))
        center_v = np.array(center_v)
        center_v.shape = (20, 1, 2)
        if debug == 'plot':
            img2 = np.copy(img)
            for i in top:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 0), -1)
            for i in bottom:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 255), -1)
            for i in center_v:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (0, 79, 255), -1)
            # print_image(img2, (str(device) + '_x_axis_pseudolandmarks.png'))
            plot_image(img2)
        return device, top, bottom, center_v
        
    if extent < 21:
        # If the width of the object is less than 20 pixels just make the object a 20 pixel rectangle
        x, y, width, height = cv2.boundingRect(obj)
        x_coords = list(range(x, x + 20))
        u_points = [y] * 20
        top = list(zip(x_coords, u_points))
        top = np.array(top)
        top.shape = (20, 1, 2)
        b_points = [y + width] * 20
        bottom = list(zip(x_coords, b_points))
        bottom = np.array(bottom)
        bottom.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        # Centroid (center of mass x, center of mass y)
        cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
        c_points = [cmy] * 20
        center_v = list(zip(x_coords, c_points))
        center_v = np.array(center_v)
        center_v.shape = (20, 1, 2)
        if debug == "plot":
            img2 = np.copy(img)
            for i in top:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 0), -1)
            for i in bottom:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (255, 0, 255), -1)
            for i in center_v:
                x = i[0, 0]
                y = i[0, 1]
                cv2.circle(img2, (int(x), int(y)), 10, (0, 79, 255), -1)
            # print_image(img2, (str(device) + '_x_axis_pseudolandmarks.png'))
            plot_image(img2)
        return device, top, bottom, center_v
# Analyze signal data in NIR image

import os
import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import plot_colorbar
from . import binary_threshold
from . import apply_mask


def analyze_NIR_intensity(img, rgbimg, mask, bins, device, histplot=False, debug=None, filename=False):
    """This function calculates the intensity of each pixel associated with the plant and writes the values out to
       a file. It can also print out a histogram plot of pixel intensity and a pseudocolor image of the plant.

    Inputs:
    img          = input image original NIR image
    rgbimg      = RGB NIR image
    mask         = mask made from selected contours
    bins         = number of classes to divide spectrum into
    device       = device number. Used to count steps in the pipeline
    histplot     = if True plots histogram of intensity values
    debug        = None, print, or plot. Print = save to file, Plot = print to screen.
    filename     = False or image name. If defined print image

    Returns:
    device       = device number
    hist_header  = NIR histogram data table headers
    hist_data    = NIR histogram data table values
    analysis_img = output image

    :param img: numpy array
    :param rgbimg: numpy array
    :param mask: numpy array
    :param bins: int
    :param device: int
    :param histplot: bool
    :param debug: str
    :param filename: str
    :return device: int
    :return hist_header: list
    :return hist_data: list
    :return analysis_img: str
    """

    device += 1

    # apply plant shaped mask to image
    device, mask1 = binary_threshold(mask, 0, 255, 'light', device, None)
    mask1 = (mask1 / 255)
    masked = np.multiply(img, mask1)

    # calculate histogram
    if img.dtype == 'uint16':
        maxval = 65536
    else:
        maxval = 256

    hist_nir, hist_bins = np.histogram(masked, bins, (1, maxval), False, None, None)

    hist_bins1 = hist_bins[:-1]
    hist_bins2 = [l for l in hist_bins1]

    hist_nir1 = [l for l in hist_nir]

    # make hist percentage for plotting
    pixels = cv2.countNonZero(mask1)
    hist_percent = (hist_nir / float(pixels)) * 100

    # report histogram data
    hist_header = [
        'HEADER_HISTOGRAM',
        'bin-number',
        'bin-values',
        'nir'
    ]

    hist_data = [
        'HISTOGRAM_DATA',
        bins,
        hist_bins2,
        hist_nir1
    ]

    analysis_img = []

    # make mask to select the background
    mask_inv = cv2.bitwise_not(mask)
    img_back = cv2.bitwise_and(rgbimg, rgbimg, mask=mask_inv)
    img_back1 = cv2.applyColorMap(img_back, colormap=1)

    # mask the background and color the plant with color scheme 'jet'
    cplant = cv2.applyColorMap(rgbimg, colormap=2)
    device, masked1 = apply_mask(cplant, mask, 'black', device, debug=None)
    cplant_back = cv2.add(masked1, img_back1)

    if filename:
        path = os.path.dirname(filename)
        fig_name = 'NIR_pseudocolor_colorbar.svg'
        if not os.path.isfile(path + '/' + fig_name):
            plot_colorbar(path, fig_name, bins)

        fig_name_pseudo = (str(filename[0:-4]) + '_nir_pseudo_col.jpg')
        print_image(cplant_back, fig_name_pseudo)
        analysis_img.append(['IMAGE', 'pseudo', fig_name_pseudo])

    if debug is not None:
        if debug == "print":
            print_image(masked1, (str(device) + "_nir_pseudo_plant.jpg"))
            print_image(cplant_back, (str(device) + "_nir_pseudo_plant_back.jpg"))
        if debug == "plot":
            plot_image(masked1)
            plot_image(cplant_back)


    if histplot is True:
        import matplotlib
        matplotlib.use('Agg')
        from matplotlib import pyplot as plt

        # plot hist percent
        plt.plot(hist_percent, color='green', label='Signal Intensity')
        plt.xlim([0, (bins - 1)])
        plt.xlabel(('Grayscale pixel intensity (0-' + str(bins) + ")"))
        plt.ylabel('Proportion of pixels (%)')

        if filename:
            fig_name_hist = (str(filename[0:-4]) + '_nir_hist.svg')
            plt.savefig(fig_name_hist)
        if debug == "print":
            plt.savefig((str(device) + "_nir_histogram.jpg"))
        if debug == "plot":
            plt.figure()
        plt.clf()

        analysis_img.append(['IMAGE', 'hist', fig_name_hist])


    return device, hist_header, hist_data, analysis_img
#!/usr/bin/python
import argparse
import os
import sqlite3 as sq


# Parse command-line arguments
def options():
    parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
    parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
    parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
    parser.add_argument("-t", "--tv", help="Does this experiment use top-view imaging?", action="store_true")
    parser.add_argument("-s", "--sv", help="If this experiment uses side-view imaging, list the angles used",
                        default=False)
    parser.add_argument("-D", "--debug", help="Turn on debugging mode", action="store_true")
    args = parser.parse_args()
    return args


# Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


# Main pipeline
def main():
    global connect, out

    # Get options
    args = options()

    # Does the database exist?
    if not os.path.exists(args.database):
        raise ("The database file " + str(args.database) + " does not exist")

    # Open a connection
    try:
        connect = sq.connect(args.database)
    except sq.Error, e:
        print("Error %s:" % e.args[0])

    # Open output file
    try:
        out = open(args.outfile, 'w')
    except IOError:
        print("IO error")

    # Replace the row_factory result constructor with a dictionary constructor
    connect.row_factory = dict_factory
    # Change the text output format from unicode to UTF-8
    connect.text_factory = str

    # Database handler
    db = connect.cursor()

    # Get database schema
    feature_names = ['zoom']
    for row in (db.execute("SELECT * FROM `features` LIMIT 1")):
        feature_names += row.keys()

    # Header
    output_header = ['plantbarcode', 'timestamp']
    if args.tv:
        tv_header = ['tv0_zoom']
        for feature in feature_names:
            tv_header += ['tv0_' + feature]
        output_header += tv_header
    if args.sv:
        angles = args.sv.split(',')
        for angle in angles:
            sv_header = ['sv' + angle + '_zoom']
            for feature in feature_names:
                sv_header += ['sv' + str(angle) + '_' + feature]
            output_header += sv_header

    out.write(','.join(map(str, output_header)) + '\n')

    # Retrieve snapshot IDs from the database
    snapshots = []
    for row in (db.execute('SELECT DISTINCT(`timestamp`) FROM `metadata` WHERE `imgtype` = "VIS"')):
        snapshots.append(row['timestamp'])
    if args.debug:
        print('Found ' + str(len(snapshots)) + ' snapshots')

    # Retrieve snapshots and process data
    for snapshot in snapshots:
        data = {'plantbarcode' : '', 'timestamp' : ''}
        for feature in feature_names:
            data[feature] = ''

        for row in (db.execute('SELECT * FROM `metadata` NATURAL JOIN `features` WHERE `timestamp` = "%s" AND `imgtype` = "VIS"' % snapshot)):
            data['plantbarcode'] = row['plantbarcode']
            data['timestamp'] = row['timestamp']
            row['camera'] = row['camera'].lower()

            if row['frame'] == 'none':
                row['frame'] = '0'

            for feature in feature_names:
                data[row['camera'] + row['frame'] + '_' + feature] = row[feature]

        output = []
        for field in output_header:
            output.append(data[field])
        out.write(','.join(map(str, output)) + '\n')

if __name__ == '__main__':
    main()
#!/usr/bin/env python

import argparse
import sys, os
import re
import cv2
import shutil
import numpy as np

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get file names to run FASTQC over")
  parser.add_argument("-d", "--directory", help="directory to run script over.")
  parser.add_argument("-o", "--outdir", help="out directory to move files to")
  parser.add_argument("-r", "--resize_factor", help="out directory to move files to")
  args = parser.parse_args()
  return args

def image_resize(directory,outdir,resize_factor):

    if not os.path.exists(outdir):
        os.makedirs(outdir)

    files=os.listdir(directory)

    for x in files:
        if re.search("png$",x):
            outfile=outdir+"/"+str(x[:-4])+"_downsized.png"
            orimg = str(directory) + str(x)
            img = cv2.imread(orimg, -1)
            thumbnail=cv2.resize(img,None, fx=float(resize_factor),fy=float(resize_factor),interpolation=cv2.INTER_AREA)
            cv2.imwrite(outfile, thumbnail)
        elif re.search("jpg$", x):
            outfile =outdir+"/"+str(x[:-4]) + "_downsized.jpg"
            orimg=str(directory)+str(x)
            img=cv2.imread(orimg,-1)
            thumbnail=cv2.resize(img,None, fx=float(resize_factor),fy=float(resize_factor),interpolation=cv2.INTER_AREA)
            cv2.imwrite(outfile, thumbnail)

    
### Main pipeline
def main():
  # Get options
  args = options()

  image_resize(args.directory,args.outdir,args.resize_factor)
  

if __name__ == '__main__':
  main()
# Plot colorbar for pseudocolored images


def plot_colorbar(outdir, filename, bins):
    """Plot colorbar for pseudocolored images

    Inputs:
    outdir   = Output directory
    filename = Figure filename
    bins     = Number of bins for color mapping

    :param outdir: str
    :param filename: str
    :param bins: int
    :return:
    """
    import matplotlib
    matplotlib.use('Agg')
    from matplotlib import pyplot as plt
    from matplotlib import cm as cm
    from matplotlib import colors as colors
    from matplotlib import colorbar as colorbar

    fig_name = outdir + '/' + filename
    fig = plt.figure()
    ax1 = fig.add_axes([0.05, 0.80, 0.9, 0.15])
    valmin = -0
    valmax = (bins - 1)
    norm = colors.Normalize(vmin=valmin, vmax=valmax)
    colorbar.ColorbarBase(ax1, cmap=cm.jet, norm=norm, orientation='horizontal')
    fig.savefig(fig_name, bbox_inches='tight')
    fig.clf()
# RGB -> HSV -> Gray

import cv2
import numpy as np
from . import print_image
from . import plot_image


def rotate_img(img, rotation_deg, device, debug=None):
    """Rotate image, sometimes it is necessary to rotate image, especially when clustering for
       multiple plants is needed.

    Inputs:
    img          = image object, RGB colorspace (either single or three channel)
    rotation_deg = rotation angle in degrees, should be an integer, can be a negative number,
                   positive values move counter clockwise.
    device       = device number. Used to count steps in the pipeline
    debug        = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device       = device number
    rotated_img  = rotated image

    :param img: numpy array
    :param rotation_deg: int
    :param device: int
    :param debug: str
    :return device: int
    :return rotated_img: numpy array
    """
    device += 1

    if len(np.shape(img)) == 3:
        iy, ix, iz = np.shape(img)
    else:
        iy, ix = np.shape(img)

    M = cv2.getRotationMatrix2D((ix / 2, iy / 2), rotation_deg, 1)
    rotated_img = cv2.warpAffine(img, M, (ix, iy))

    if debug == 'print':
        print_image(rotated_img, (str(device) + '_rotated_img.png'))

    elif debug == 'plot':
        if len(np.shape(img)) == 3:
            plot_image(rotated_img)
        else:
            plot_image(rotated_img, cmap='gray')

    return device, rotated_img
# RGB -> LAB -> Gray

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def rgb2gray_lab(img, channel, device, debug=None):
    """Convert image from RGB colorspace to LAB colorspace. Returns the specified subchannel as a gray image.

    Inputs:
    img       = image object, RGB colorspace
    channel   = color subchannel (l = lightness, a = green-magenta, b = blue-yellow)
    device    = device number. Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    l | a | b = grayscale image from one LAB color channel

    :param img: numpy array
    :param channel: str
    :param device: int
    :param debug: str
    :return device: int
    :return channel: numpy array
    """
    # Auto-increment the device counter
    device += 1
    # The allowable channel inputs are l, a or b
    names = {"l": "lightness", "a": "green-magenta", "b": "blue-yellow"}
    if channel not in names:
        fatal_error("Channel " + str(channel) + " is not l, a or b!")

    # Convert the input BGR image to LAB colorspace
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    # Split LAB channels
    l, a, b = cv2.split(lab)
    # Create a channel dictionaries for lookups by a channel name index
    channels = {"l": l, "a": a, "b": b}

    if debug == "print":
        print_image(channels[channel], str(device) + "_lab_" + names[channel] + ".png")
    elif debug == "plot":
        plot_image(channels[channel], cmap="gray")

    return device, channels[channel]
#!/usr/bin/env python
import argparse
import sys
import os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-c", "--csv", help="PhenoFront CSV file.")
  parser.add_argument("-f", "--file", help="File containing plant IDs.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=True)
  parser.add_argument("--vis", help="Images are class VIS.", action='store_true')
  parser.add_argument("--nir", help="Images are class NIR.", action='store_true')
  parser.add_argument("--flu", help="Images are class FLU.", action='store_true')
  parser.add_argument("-t", "--type", help="Image format type.", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Database image lookup method
def db_lookup(database, ids, outdir, type, vis=False, nir=False, flu=False):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  
  for plant_id in ids:
    for row in (db.execute('SELECT * FROM `snapshots` WHERE `plant_id` = "%s"' % plant_id)):
      dt = datetime.datetime.fromtimestamp(row['datetime']).strftime('%Y-%m-%d_%H:%M:%S')
      if (vis):
        if (row['camera'] == 'vis_sv' or row['camera'] == 'vis_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
          #print(args.outdir + '/' + row['plant_id'])
      if (nir):
        if (row['camera'] == 'nir_sv' or row['camera'] == 'nir_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
      if (flu):
        if (row['camera'] == 'flu_tv'):
          images = row['image_path'].split(',')
          for i in enumerate(images):
            img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + str(i) + '_' + dt + '.' + type
            copy(images[i], outdir)
  
### CSV image lookup method
def csv_lookup(csv, ids, outdir, type, vis=False, nir=False, flu=False):
  # Regexs
  vis_pattern = re.compile('^vis', re.IGNORECASE)
  nir_pattern = re.compile('^nir', re.IGNORECASE)
  flu_pattern = re.compile('^Flu', re.IGNORECASE)
  
  path, img = os.path.split(csv)
  
  # Open CSV file
  with open(csv) as snapshots:
    for row in snapshots:
      snapshot = row.rstrip('\n')
      data = snapshot.split(',')
      date = data[4].split(' ')
      if (data[2] in ids):
        tiles = data[11].split(';')
        for tile in tiles:
          img_name = outdir + '/' + data[2] + '_' + tile + '_' + date[0] + '_' + date[1] + '.' + type
          if (vis):
            if (vis_pattern.match(tile)):
              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)
          if (nir):
            if (nir_pattern.match(tile)):
              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)
          if (flu):
            if (flu_pattern.match(tile)):
              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)

### Create dictionary of plant_id from file
def dict_plant_id(infile):
  # Store plant IDs
  ids = {}
  
  # Open input file
  with open(infile) as plant_ids:
    for plant_id in plant_ids:
      pid = plant_id.rstrip('\n')
      ids[pid] = 1
  
  return ids

### Main pipeline
def main():
  # Get options
  args = options()
  
  plant_ids = dict_plant_id(args.file)
  
  if (args.database):
    db_lookup(args.database, plant_ids, args.outdir, args.type, args.vis, args.nir, args.flu)
  elif (args.csv):
    csv_lookup(args.csv, plant_ids, args.outdir, args.type, args.vis, args.nir, args.flu)
  

if __name__ == '__main__':
  main()
# Join images (AND)

import cv2
from . import print_image
from . import plot_image


def logical_and(img1, img2, device, debug=None):
    """Join two images using the bitwise AND operator.

    Inputs:
    img1   = image object1, grayscale
    img2   = image object2, grayscale
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    merged = joined image

    :param img1: numpy array
    :param img2: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return merged: numpy array
    """

    device += 1
    merged = cv2.bitwise_and(img1, img2)
    if debug == 'print':
        print_image(merged, (str(device) + '_and_joined.png'))
    elif debug == 'plot':
        plot_image(merged, cmap='gray')
    return device, merged
# Object composition

import numpy as np
import cv2
from . import print_image
from . import plot_image


def object_composition(img, contours, hierarchy, device, debug=None):
    """Groups objects into a single object, usually done after object filtering.

    Inputs:
    contours = object list
    device   = device number. Used to count steps in the pipeline
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    group    = grouped contours list
    mask     = image mask

    :param img: numpy array
    :param contours: list
    :param hierarchy: list
    :param device: int
    :param debug: str
    :return device: int
    :return group: list
    :return mask: numpy array
    """

    device += 1
    ori_img = np.copy(img)

    stack = np.zeros((len(contours), 1))
    r, g, b = cv2.split(ori_img)
    mask = np.zeros(g.shape, dtype=np.uint8)

    for c, cnt in enumerate(contours):
        # if hierarchy[0][c][3] == -1:
        if hierarchy[0][c][2] == -1 and hierarchy[0][c][3] > -1:
            stack[c] = 0
            # stack[c] = 1
            # cv2.drawContours(img, cnt, -1, color_palette(1)[0], 3)
            # np.append(group, np.vstack(cnt))
        else:
            stack[c] = 1
            # cv2.drawContours(img, contours, -1, color_palette(1)[0], -1, hierarchy=hierarchy)
            # stack[c] = 0
    ids = np.where(stack == 1)[0]
    if len(ids) > 0:
        group = np.vstack(contours[i] for i in ids)
        cv2.drawContours(mask, contours, -1, (255), -1, hierarchy=hierarchy)

        if debug is not None:
            cv2.drawContours(ori_img, group, -1, (255, 0, 0), 4)
            for cnt in contours:
                cv2.drawContours(ori_img, cnt, -1, (255, 0, 0), 4)
            if debug == 'print':
                print_image(ori_img, (str(device) + '_objcomp.png'))
                print_image(ori_img, (str(device) + '_objcomp_mask.png'))
            elif debug == 'plot':
                plot_image(ori_img)
        return device, group, mask
    else:
        print("Warning: Invalid contour.")
        return device, None, None
# Crop position mask

import cv2
import numpy as np
import math
from . import print_image
from . import plot_image
from . import fatal_error


def crop_position_mask(img, mask, device, x, y, v_pos="top", h_pos="right", debug=None):
    """Crop position mask

    Inputs:
    img     = image to mask
    mask    = mask to use (must be correct size, if, not use make_resize_mask function)
    x       = x position
    y       = y position
    v_pos   = push from "top" or "bottom"
    h_pos   = push to "right" or "left"
    device  = device counter
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device  = device number
    newmask = image mask

    :param img: numpy array
    :param mask: numpy array
    :param device: int
    :param x: int
    :param y: int
    :param v_pos: str
    :param h_pos: str
    :param debug: str
    :return device: int
    :return newmask: numpy array
    """

    ori_mask = np.copy(mask)

    device += 1

    if x < 0 or y < 0:
        fatal_error("x and y cannot be negative numbers or non-integers")

    # get the sizes of the images
    # subtract 1 from x and y since python counts start from 0
    if y != 0:
        y = y - 1
    if x != 0:
        x = x - 1

    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
        ori_img = np.copy(img)
    else:
        ix, iy = np.shape(img)
        ori_img = np.dstack((img, img, img))

    if len(np.shape(mask)) == 3:
        mx, my, mz = np.shape(mask)
        mask = mask[0]
    else:
        mx, my = np.shape(mask)

    npimg = np.zeros((ix, iy), dtype=np.uint8)

    # resize the images so they are equal in size and centered
    if mx >= ix:
        r = mx - ix
        if r % 2 == 0:
            r1 = int(np.rint(r / 2.0))
            r2 = r1
        else:
            r1 = int(np.rint(r / 2.0))
            r2 = r1 - 1
        mask = mask[r1:mx - r2, 0:my]
    if my >= iy:
        r = my - iy
        if r % 2 == 0:
            r1 = int(np.rint(r / 2.0))
            r2 = r1
        else:
            r1 = int(np.rint(r / 2.0))
            r2 = r1 - 1
        mask = mask[0:mx, r1:my - r2]

    # get he sizes of the images again since you might have changed them.
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
        ori_img = np.copy(img)
    else:
        ix, iy = np.shape(img)
        ori_img = np.dstack((img, img, img))

    if len(np.shape(mask)) == 3:
        mx, my, mz = np.shape(mask)
    else:
        mx, my = np.shape(mask)

    if v_pos == "top":
        # Add rows to the top
        top = np.zeros((x, my), dtype=np.uint8)

        maskv = np.vstack((top, mask))

        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        if mx >= ix:
            maskv = maskv[0:ix, 0:my]

        if mx < ix:
            r = ix - mx
            if r % 2 == 0:
                r1 = int(r / 2.0)
                rows1 = np.zeros((r1, my), dtype=np.uint8)
                maskv = np.vstack((rows1, maskv, rows1))
            else:
                r1 = int(math.ceil(r / 2.0))
                r2 = r1 - 1
                rows1 = np.zeros((r1, my), dtype=np.uint8)
                rows2 = np.zeros((r2, my), dtype=np.uint8)
                maskv = np.vstack((rows1, maskv, rows2))
        if debug == 'print':
            print_image(maskv, (str(device) + "_push-top_.png"))
        elif debug == 'plot':
            plot_image(maskv, cmap='gray')

    if v_pos == "bottom":
        # Add rows to the bottom
        bottom = np.zeros((x, my), dtype=np.uint8)

        maskv = np.vstack((mask, bottom))
        # print_image(maskv,(str(device)+"_push-bottom-test.png"))

        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        if mx >= ix:
            maskdiff = mx - ix
            maskv = maskv[maskdiff:mx, 0:my]
            # print_image(maskv,(str(device)+"_push-bottom-test.png"))

        if mx < ix:
            r = ix - mx
            if r % 2 == 0:
                r1 = int(r / 2.0)
                rows1 = np.zeros((r1, my), dtype=np.uint8)
                maskv = np.vstack((rows1, maskv, rows1))
            else:
                r1 = int(math.ceil(r / 2.0))
                r2 = r1 - 1
                rows1 = np.zeros((r1, my), dtype=np.uint8)
                rows2 = np.zeros((r2, my), dtype=np.uint8)
                maskv = np.vstack((rows1, maskv, rows2))
        if debug == 'print':
            print_image(maskv, (str(device) + "_push-bottom.png"))
        elif debug == 'plot':
            plot_image(maskv, cmap='gray')

    if h_pos == "left":
        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        # Add rows to the left
        left = np.zeros((mx, y), dtype=np.uint8)
        maskv = np.hstack((left, maskv))

        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        if my >= iy:
            maskv = maskv[0:mx, 0:iy]

        if my < iy:
            c = iy - my
            if c % 2 == 0:
                c1 = int(c / 2.0)
                col = np.zeros((mx, c1), dtype=np.uint8)
                maskv = np.hstack((col, maskv, col))
            else:
                c1 = int(math.ceil(c / 2.0))
                c2 = c1 - 1
                col1 = np.zeros((mx, c1), dtype=np.uint8)
                col2 = np.zeros((mx, c2), dtype=np.uint8)
                maskv = np.hstack((col1, maskv, col2))
        if debug == 'print':
            print_image(maskv, (str(device) + "_push-left.png"))
        elif debug == 'plot':
            plot_image(maskv, cmap='gray')

    if h_pos == "right":
        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        # Add rows to the left
        right = np.zeros((mx, y), dtype=np.uint8)
        maskv = np.hstack((maskv, right))

        if len(np.shape(maskv)) == 3:
            mx, my, mz = np.shape(maskv)
        else:
            mx, my = np.shape(maskv)

        if my >= iy:
            ex = my - iy
            maskv = maskv[0:mx, ex:my]

        if my < iy:
            c = iy - my
            if c % 2 == 0:
                c1 = int(c / 2.0)
                col = np.zeros((mx, c1), dtype=np.uint8)
                maskv = np.hstack((col, maskv, col))
            else:
                c1 = int(math.ceil(c / 2.0))
                c2 = c1 - 1
                col1 = np.zeros((mx, c1), dtype=np.uint8)
                col2 = np.zeros((mx, c2), dtype=np.uint8)
                maskv = np.hstack((col1, maskv, col2))
        if debug == 'print':
            print_image(maskv, (str(device) + "_push-right.png"))
        elif debug == 'plot':
            plot_image(maskv, cmap='gray')

    newmask = np.array(maskv)
    if debug is not None:
        if debug == 'print':
            print_image(newmask, (str(device) + "_newmask.png"))
        elif debug == 'plot':
            plot_image(newmask, cmap='gray')
        objects, hierarchy = cv2.findContours(newmask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        for i, cnt in enumerate(objects):
            cv2.drawContours(ori_img, objects, i, (255, 102, 255), -1, lineType=8, hierarchy=hierarchy)
        if debug == 'print':
            print_image(ori_img, (str(device) + '_mask_overlay.png'))
        elif debug == 'plot':
            plot_image(ori_img)

    return device, newmask
# Analyzes an object and outputs numeric properties

import cv2
import numpy as np
from . import fatal_error
from . import print_image
from . import plot_image
from . import rgb2gray_hsv
from . import find_objects
from . import binary_threshold
from . import define_roi
from . import roi_objects
from . import object_composition


def report_size_marker_area(img, shape, device, debug, marker='define', x_adj=0, y_adj=0, w_adj=0, h_adj=0,
                            base='white', objcolor='dark', thresh_channel=None, thresh=None, filename=False):
    """Outputs numeric properties for an input object (contour or grouped contours).

    Inputs:
    img             = image object (most likely the original), color(RGB)
    shape           = 'rectangle', 'circle', 'ellipse'
    device          = device number. Used to count steps in the pipeline
    debug           = None, print, or plot. Print = save to file, Plot = print to screen.
    marker          = define or detect, if define it means you set an area, if detect it means you want to
                      detect within an area
    x_adj           = x position of shape, integer
    y_adj           = y position of shape, integer
    w_adj           = width
    h_adj           = height
    base            = background color 'white' is default
    objcolor        = object color is 'dark' or 'light'
    thresh_channel  = 'h', 's','v'
    thresh          = integer value
    filename        = name of file

    Returns:
    device          = device number
    marker_header    = shape data table headers
    marker_data      = shape data table values
    analysis_images = list of output images

    :param img: numpy array
    :param shape: str
    :param device: int
    :param debug: str
    :param marker: str
    :param x_adj:int
    :param y_adj:int
    :param w_adj:int
    :param h_adj:int
    :param h_adj:int
    :param base:str
    :param objcolor: str
    :param thresh_channel:str
    :param thresh:int
    :param filename: str
    :return: device: int
    :return: marker_header: str
    :return: marker_data: int
    :return: analysis_images: list
    """

    device += 1
    ori_img = np.copy(img)
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
    else:
        ix, iy = np.shape(img)

    size = ix, iy
    roi_background = np.zeros(size, dtype=np.uint8)
    roi_size = (ix - 5), (iy - 5)
    roi = np.zeros(roi_size, dtype=np.uint8)
    roi1 = roi + 1
    roi_contour, roi_heirarchy = cv2.findContours(roi1, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    cv2.drawContours(roi_background, roi_contour[0], -1, (255, 0, 0), 5)

    if (x_adj > 0 and w_adj > 0) or (y_adj > 0 and h_adj > 0) or (x_adj < 0 or y_adj < 0):
        fatal_error('Adjusted ROI position is out of frame, this will cause problems in detecting objects')

    for cnt in roi_contour:
        size1 = ix, iy, 3
        background = np.zeros(size1, dtype=np.uint8)
        if shape == 'rectangle':
            x, y, w, h = cv2.boundingRect(cnt)
            x1 = x + x_adj
            y1 = y + y_adj
            w1 = w + w_adj
            h1 = h + h_adj
            cv2.rectangle(background, (x1, y1), (x + w1, y + h1), (1, 1, 1), -1)
        elif shape == 'circle':
            x, y, w, h = cv2.boundingRect(cnt)
            x1 = x + x_adj
            y1 = y + y_adj
            w1 = w + w_adj
            h1 = h + h_adj
            center = (int((w + x1) / 2), int((h + y1) / 2))
            if h > w:
                radius = int(w1 / 2)
                cv2.circle(background, center, radius, (1, 1, 1), -1)
            else:
                radius = int(h1 / 2)
                cv2.circle(background, center, radius, (1, 1, 1), -1)
        elif shape == 'ellipse':
            x, y, w, h = cv2.boundingRect(cnt)
            x1 = x + x_adj
            y1 = y + y_adj
            w1 = w + w_adj
            h1 = h + h_adj
            center = (int((w + x1) / 2), int((h + y1) / 2))
            if w > h:
                cv2.ellipse(background, center, (w1 / 2, h1 / 2), 0, 0, 360, (1, 1, 1), -1)
            else:
                cv2.ellipse(background, center, (h1 / 2, w1 / 2), 0, 0, 360, (1, 1, 1), -1)
        else:
            fatal_error('Shape' + str(shape) + ' is not "rectangle", "circle", or "ellipse"!')

    markerback = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
    shape_contour, hierarchy = cv2.findContours(markerback, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    cv2.drawContours(ori_img, shape_contour, -1, (255, 255, 0), 5)
    
    if debug is 'print':
        print_image(ori_img, (str(device) + '_marker_roi.png'))
    elif debug is 'plot':
        plot_image(ori_img)

    if marker == 'define':
        m = cv2.moments(markerback, binaryImage=True)
        area = m['m00']
        device, id_objects, obj_hierarchy = find_objects(img, markerback, device, debug)
        device, obj, mask = object_composition(img, id_objects, obj_hierarchy, device, debug)
        center, axes, angle = cv2.fitEllipse(obj)
        major_axis = np.argmax(axes)
        minor_axis = 1 - major_axis
        major_axis_length = axes[major_axis]
        minor_axis_length = axes[minor_axis]
        eccentricity = np.sqrt(1 - (axes[minor_axis] / axes[major_axis]) ** 2)

    elif marker == 'detect':
        if thresh_channel is not None and thresh is not None:
            if base == 'white':
                masked = cv2.multiply(img, background)
                marker1 = markerback * 255
                mask1 = cv2.bitwise_not(marker1)
                markstack = np.dstack((mask1, mask1, mask1))
                added = cv2.add(masked, markstack)
            else:
                added = cv2.multiply(img, background)
            device, maskedhsv = rgb2gray_hsv(added, thresh_channel, device, debug)
            device, masked2a_thresh = binary_threshold(maskedhsv, thresh, 255, objcolor, device, debug)
            device, id_objects, obj_hierarchy = find_objects(added, masked2a_thresh, device, debug)
            device, roi1, roi_hierarchy = define_roi(added, shape, device, None, 'default', debug, True, x_adj, y_adj,
                                                     w_adj, h_adj)
            device, roi_o, hierarchy3, kept_mask, obj_area = roi_objects(img, 'partial', roi1, roi_hierarchy,
                                                                         id_objects, obj_hierarchy, device, debug)
            device, obj, mask = object_composition(img, roi_o, hierarchy3, device, debug)

            cv2.drawContours(ori_img, roi_o, -1, (0, 255, 0), -1, lineType=8, hierarchy=hierarchy3)
            m = cv2.moments(mask, binaryImage=True)
            area = m['m00']

            center, axes, angle = cv2.fitEllipse(obj)
            major_axis = np.argmax(axes)
            minor_axis = 1 - major_axis
            major_axis_length = axes[major_axis]
            minor_axis_length = axes[minor_axis]
            eccentricity = np.sqrt(1 - (axes[minor_axis] / axes[major_axis]) ** 2)

        else:
            fatal_error('thresh_channel and thresh must be defined in detect mode')
    else:
        fatal_error("marker must be either in 'detect' or 'define' mode")
    
    analysis_images = []
    if filename:
        out_file = str(filename[0:-4]) + '_sizemarker.jpg'
        print_image(ori_img, out_file)
        analysis_images.append(['IMAGE', 'marker', out_file])
    if debug is 'print':
        print_image(ori_img, (str(device) + '_marker_shape.png'))
    elif debug is 'plot':
        plot_image(ori_img)

    marker_header = (
        'HEADER_MARKER',
        'marker_area',
        'marker_major_axis_length',
        'marker_minor_axis_length',
        'marker_eccentricity'
    )

    marker_data = (
        'MARKER_DATA',
        area,
        major_axis_length,
        minor_axis_length,
        eccentricity
    )

    return device, marker_header, marker_data, analysis_images
# Erosion filter

import cv2
import numpy as np
from . import print_image
from . import plot_image


def erode(img, kernel, i, device, debug=None):
    """Perform morphological 'erosion' filtering. Keeps pixel in center of the kernel if conditions set in kernel are
       true, otherwise removes pixel.

    Inputs:
    img    = input image
    kernel = filtering window, you'll need to make your own using as such:
             kernal = np.zeros((x,y), dtype=np.uint8), then fill the kernal with appropriate values
    i      = interations, i.e. number of consecutive filtering passes
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    er_img = eroded image

    :param img: numpy array
    :param kernel: numpy array
    :param i: int
    :param device: int
    :param debug: str
    :return device: int
    :return er_img: numpy array
    """

    kernel1 = int(kernel)
    kernel2 = np.ones((kernel1, kernel1), np.uint8)
    er_img = cv2.erode(src=img, kernel=kernel2, iterations=i)
    device += 1
    if debug == 'print':
        print_image(er_img, str(device) + '_er_image_' + 'itr_' + str(i) + '.png')
    elif debug == 'plot':
        plot_image(er_img, cmap='gray')
    return device, er_img
# Laplace filtering

import cv2
from . import print_image
from . import plot_image


def laplace_filter(img, k, scale, device, debug=None):
    """This is a filtering method used to identify and highlight fine edges based on the 2nd derivative. A very
       sensetive method to highlight edges but will also amplify background noise. ddepth = -1 specifies that the
       dimensions of output image will be the same as the input image.

    Inputs:
    img         = input image
    k           = apertures size used to calculate 2nd derivative filter, specifies the size of the kernel
                  (must be an odd integer: 1,3,5...)
    scale       = scaling factor applied (multiplied) to computed Laplacian values (scale = 1 is unscaled)
    device      = device number. Used to count steps in the pipeline
    debug       = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device      = device number
    lp_filtered = laplacian filtered image

    :param img: numpy array
    :param k: int
    :param scale: int
    :param device: int
    :param debug: str
    :return device: int
    :return lp_filtered: numpy array
    """

    lp_filtered = cv2.Laplacian(src=img, ddepth=-1, ksize=k, scale=scale)
    device += 1
    if debug == 'print':
        print_image(lp_filtered, str(device) + '_lp_out_k' + str(k) + '_scale' + str(scale) + '.png')
    elif debug == 'plot':
        plot_image(lp_filtered, cmap='gray')
    return device, lp_filtered
# Resize image

import cv2
import numpy as np
from . import print_image
from . import plot_image


def auto_crop(device, img, objects, padding_x=0, padding_y=0, color='black', debug=None):
    """Resize image.

    Inputs:
    device    = device counter
    img       = image
    objects   = contours
    padding_x = padding in the x direction
    padding_y = padding in the y direction
    color     = either 'black' or 'white'
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    cropped   = cropped image

    :param device: int
    :param img: numpy array
    :param objects: list
    :param padding_x: int
    :param padding_y: int
    :param color: str
    :param debug: str
    :return device: str
    :return cropped: numpy array
    """

    device += 1
    img_copy = np.copy(img)

    x, y, w, h = cv2.boundingRect(objects)
    cv2.rectangle(img_copy, (x, y), (x + w, y + h), (0, 255, 0), 5)

    crop_img = img[y:y + h, x:x + w]

    offsetx = int(np.rint((padding_x)))
    offsety = int(np.rint((padding_y)))

    if color == 'black':
        colorval = (0, 0, 0)
    elif color == 'white':
        colorval = (255, 255, 255)

    cropped = cv2.copyMakeBorder(crop_img, offsety, offsety, offsetx, offsetx, cv2.BORDER_CONSTANT, value=colorval)

    if debug == 'print':
        print_image(img_copy, (str(device) + "_crop_area.png"))
        print_image(cropped, (str(device) + "_auto_cropped.png"))
    elif debug == 'plot':
        if len(np.shape(img_copy)) == 3:
            plot_image(img_copy)
            plot_image(cropped)
        else:
            plot_image(img_copy, cmap='gray')
            plot_image(cropped, cmap='gray')

    return device, cropped
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  #parser.add_argument("-c", "--csv", help="PhenoFront CSV file.")
  parser.add_argument("-o", "--outdir", help="Output directory.", required=True)
  #parser.add_argument("--vis", help="Images are class VIS.", action='store_true')
  #parser.add_argument("--nir", help="Images are class NIR.", action='store_true')
  #parser.add_argument("--flu", help="Images are class FLU.", action='store_true')
  parser.add_argument("-t", "--type", help="Image format type (png, jpg).", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Database image lookup method
def db_lookup(database, outdir, type):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  
  ids = {}
  for row in (db.execute('SELECT DISTINCT(`plant_id`) FROM `snapshots`')):
    ids[row['plant_id']] = 1
  
  for plant_id in ids:
    for row in (db.execute('SELECT MAX(`datetime`) as max FROM `snapshots` WHERE `plant_id` = "%s"' % plant_id)):
      ids[plant_id] = row['max']
  
  for plant_id, timestamp in ids.iteritems():
    plant_outdir = outdir + '/' + plant_id
    os.makedirs(plant_outdir)
    for row in (db.execute('SELECT * FROM `snapshots` WHERE `datetime` = {0}'.format(timestamp))):
      dt = datetime.datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d_%H:%M:%S')
      if (row['camera'] == 'vis_sv' or row['camera'] == 'vis_tv'):
        img_name = plant_outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
        copy(row['image_path'], img_name)
        print(img_name)
  
### CSV image lookup method
#def csv_lookup(csv, ids, outdir, type, vis=False, nir=False, flu=False):
#  # Regexs
#  vis_pattern = re.compile('^vis', re.IGNORECASE)
#  nir_pattern = re.compile('^nir', re.IGNORECASE)
#  flu_pattern = re.compile('^Flu', re.IGNORECASE)
#  
#  path, img = os.path.split(csv)
#  
#  # Open CSV file
#  with open(csv) as snapshots:
#    for row in snapshots:
#      snapshot = row.rstrip('\n')
#      data = snapshot.split(',')
#      date = data[4].split(' ')
#      if (data[2] in ids):
#        tiles = data[11].split(';')
#        for tile in tiles:
#          img_name = outdir + '/' + data[2] + '_' + tile + '_' + date[0] + '_' + date[1] + '.' + type
#          if (vis):
#            if (vis_pattern.match(tile)):
#              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)
#          if (nir):
#            if (nir_pattern.match(tile)):
#              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)
#          if (flu):
#            if (flu_pattern.match(tile)):
#              copy(path + '/snapshot' + data[1] + '/' + tile + '.' + type, img_name)

### Main pipeline
def main():
  # Get options
  args = options()
  
  if (args.database):
    db_lookup(args.database, args.outdir, args.type)
  #elif (args.csv):
  #  csv_lookup(args.csv, plant_ids, args.outdir, args.type, args.vis, args.nir, args.flu)
  

if __name__ == '__main__':
  main()
# Join images (OR)

import cv2
from . import print_image
from . import plot_image


def logical_or(img1, img2, device, debug=None):
    """Join two images using the bitwise OR operator.

    Inputs:
    img1   = image object1, grayscale
    img2   = image object2, grayscale
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    merged = joined image

    :param img1: numpy array
    :param img2: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return merged: numpy array
    """

    device += 1
    merged = cv2.bitwise_or(img1, img2)
    if debug == 'print':
        print_image(merged, (str(device) + '_or_joined.png'))
    elif debug == 'plot':
        plot_image(merged, cmap='gray')
    return device, merged
# Flip image

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def flip(img, direction, device, debug=None):
    """Flip image.

    Inputs:
    img       = image to be flipped
    direction = "horizontal" or "vertical"
    device    = device counter
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    vh_img    = flipped image

    :param img: numpy array
    :param direction: str
    :param device: int
    :param debug: str
    :return device: int
    :return vh_img: numpy array
    """
    device += 1
    if direction == "vertical":
        vh_img = cv2.flip(img, 1)
    elif direction == "horizontal":
        vh_img = cv2.flip(img, 0)
    else:
        fatal_error(str(direction) + " is not a valid direction, must be horizontal or vertical")

    if debug == 'print':
        print_image(vh_img, (str(device) + "_flipped.png"))
    elif debug == 'plot':
        if len(np.shape(vh_img)) == 3:
            plot_image(vh_img)
        else:
            plot_image(vh_img, cmap='gray')

    return device, vh_img
# Fluorescence Analysis

import os
import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import plot_colorbar
from . import fatal_error


def fluor_fvfm(fdark, fmin, fmax, mask, device, filename, bins=1000, debug=None):
    """Analyze PSII camera images.

    Inputs:
    fdark       = 16-bit grayscale fdark image
    fmin        = 16-bit grayscale fmin image
    fmax        = 16-bit grayscale fmax image
    mask        = mask of plant (binary,single channel)
    device      = counter for debug
    filename    = name of file
    bins        = number of bins from 0 to 65,536 (default is 1000)
    debug       = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device      = device number
    hist_header = fvfm data table headers
    hist_data   = fvfm data table values

    :param fdark: numpy array
    :param fmin: numpy array
    :param fmax: numpy array
    :param mask: numpy array
    :param device: int
    :param filename: str
    :param bins: int
    :param debug: str
    :return device: int
    :return hist_header: list
    :return hist_data: list
    """

    # Auto-increment the device counter
    device += 1
    # Check that fdark, fmin, and fmax are grayscale (single channel)
    if not all(len(np.shape(i)) == 2 for i in [fdark, fmin, fmax]):
        fatal_error("The fdark, fmin, and fmax images must be grayscale images.")
    # Check that fdark, fmin, and fmax are 16-bit images
    if not all(i.dtype == "uint16" for i in [fdark, fmin, fmax]):
        fatal_error("The fdark, fmin, and fmax images must be 16-bit images.")

    # QC Fdark Image
    fdark_mask = cv2.bitwise_and(fdark, fdark, mask=mask)
    if np.amax(fdark_mask) > 2000:
        qc_fdark = False
    else:
        qc_fdark = True

    # Mask Fmin and Fmax Image
    fmin_mask = cv2.bitwise_and(fmin, fmin, mask=mask)
    fmax_mask = cv2.bitwise_and(fmax, fmax, mask=mask)

    # Calculate Fvariable, where Fv = Fmax - Fmin (masked)
    fv = np.subtract(fmax_mask, fmin_mask)

    # When Fmin is greater than Fmax, a negative value is returned.
    # Because the data type is unsigned integers, negative values roll over, resulting in nonsensical values
    # Wherever Fmin is greater than Fmax, set Fv to zero
    fv[np.where(fmax_mask < fmin_mask)] = 0

    # Calculate Fv/Fm (Fvariable / Fmax) where Fmax is greater than zero
    # By definition above, wherever Fmax is zero, Fvariable will also be zero
    # To calculate the divisions properly we need to change from unit16 to float64 data types
    fvfm = fv.astype(np.float64)
    fmax_flt = fmax_mask.astype(np.float64)
    fvfm[np.where(fmax_mask > 0)] /= fmax_flt[np.where(fmax_mask > 0)]

    # Calculate the median Fv/Fm value for non-zero pixels
    fvfm_median = np.median(fvfm[np.where(fvfm > 0)])

    # Calculate the histogram of Fv/Fm non-zero values
    fvfm_hist, fvfm_bins = np.histogram(fvfm[np.where(fvfm > 0)], bins, range=(0, 1))
    # fvfm_bins is a bins + 1 length list of bin endpoints, so we need to calculate bin midpoints so that
    # the we have a one-to-one list of x (FvFm) and y (frequency) values.
    # To do this we add half the bin width to each lower bin edge x-value
    midpoints = fvfm_bins[:-1] + 0.5 * np.diff(fvfm_bins)

    # Calculate which non-zero bin has the maximum Fv/Fm value
    max_bin = midpoints[np.argmax(fvfm_hist)]

    # Store Fluorescence Histogram Data
    hist_header = (
        'HEADER_HISTOGRAM',
        'bin-number',
        'fvfm_bins',
        'fvfm_hist',
        'fvfm_hist_peak',
        'fvfm_median',
        'fdark_passed_qc'
    )

    hist_data = (
        'FLU_DATA',
        bins,
        np.around(midpoints, decimals=len(str(bins))).tolist(),
        fvfm_hist.tolist(),
        float(max_bin),
        float(np.around(fvfm_median, decimals=4)),
        qc_fdark
    )

    if filename:
        import matplotlib
        matplotlib.use('Agg')
        from matplotlib import pyplot as plt
        from matplotlib import cm as cm

        # Print F-variable image
        print_image(fv, (str(filename[0:-4]) + '_fv_img.png'))
        print('\t'.join(map(str, ('IMAGE', 'fv', str(filename[0:-4]) + '_fv_img.png'))))

        # Create Histogram Plot, if you change the bin number you might need to change binx so that it prints
        # an appropriate number of labels
        binx = bins / 50
        plt.plot(midpoints, fvfm_hist, color='green', label='Fv/Fm')
        plt.xticks(list(midpoints[0::binx]), rotation='vertical', size='xx-small')
        plt.legend()
        ax = plt.subplot(111)
        ax.set_ylabel('Plant Pixels')
        ax.text(0.05, 0.95, ('Peak Bin Value: ' + str(max_bin)), transform=ax.transAxes, verticalalignment='top')
        plt.grid()
        plt.title('Fv/Fm of ' + str(filename[0:-4]))
        fig_name = (str(filename[0:-4]) + '_fvfm_hist.svg')
        plt.savefig(fig_name)
        plt.clf()
        print('\t'.join(map(str, ('IMAGE', 'hist', fig_name))))

        # Pseudocolored Fv/Fm image
        fvfm_8bit = fvfm * 255
        fvfm_8bit = fvfm_8bit.astype(np.uint8)
        plt.imshow(fvfm_8bit, vmin=0, vmax=1, cmap=cm.jet_r)
        plt.subplot(111)
        mask_inv = cv2.bitwise_not(mask)
        background = np.dstack((mask, mask, mask, mask_inv))
        my_cmap = plt.get_cmap('binary_r')
        plt.imshow(background, cmap=my_cmap)
        plt.axis('off')
        fig_name = (str(filename[0:-4]) + '_pseudo_fvfm.png')
        plt.savefig(fig_name, dpi=600, bbox_inches='tight')
        plt.clf()
        print('\t'.join(map(str, ('IMAGE', 'pseudo', fig_name))))

        path = os.path.dirname(filename)
        fig_name = 'FvFm_pseudocolor_colorbar.svg'
        if not os.path.isfile(path + '/' + fig_name):
            plot_colorbar(path, fig_name, 2)

    if debug == 'print':
        print_image(fmin_mask, (str(device) + '_fmin_mask.png'))
        print_image(fmax_mask, (str(device) + '_fmax_mask.png'))
        print_image(fv, (str(device) + '_fv_convert.png'))
    elif debug == 'plot':
        plot_image(fmin_mask, cmap='gray')
        plot_image(fmax_mask, cmap='gray')
        plot_image(fv, cmap='gray')

    return device, hist_header, hist_data
#!/usr/bin/env python
from __future__ import division
import sys, traceback
import os
import re
import sqlite3 as sq
import distutils.core
import cv2
import numpy as np
import argparse
import string
from datetime import datetime

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Sitching and Ordering Image Slices")
  parser.add_argument("-d", "--database", help="Database to query.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory for image files.", required=True)
  #parser.add_argument("-D", "--debug", help="Turn on debug, prints intermediate images.", action="store_true")
  args = parser.parse_args()
  return args

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

def color_export(sqlitedb,outdir,signal_type='vis', camera_label='vis_sv',channels='rgb',average_angles='on'):
  #sqlitedb = sqlite database to query (path to db)
  #outdir = path to outdirectory
  #camera_type='vis','nir' or 'fluor'
  #camera_label = either 'vis_tv','vis_sv',or 'fluor_tv'
  #channels = signal,'rgb' 'lab' or 'hsv'
  #average_angles = if on side angles for a plant are averaged
  #spacer = either 'on' or 'off', adds a white line between day breaks
  #makefig = either 'yes' or 'no', adds labels to days and a title
  #cat_treat = either 'yes','no',or 'all', if yes concatenates figures by treatment, if all all slice plots are put together, this only works properly if plots are roughly similar in size
  
  # Makes folder in specified directory for the slice figures and images
  i=datetime.now()
  timenow=i.strftime('%m-%d-%Y_%H:%M:%S')
  newfolder="slice_figs_and_images_"+(str(timenow))
  
  os.mkdir((str(outdir)+newfolder))
  outdir_name=str(outdir)+str(newfolder)+"/"
  
  # Connect to sqlite database
  connect=sq.connect(sqlitedb)
  connect.row_factory = dict_factory
  connect.text_factory=str
  c = connect.cursor()
  h = connect.cursor()
  m = connect.cursor()
  
  # Find first day of experiment, this is needed to calculate the days in integer values instead of epoch time
  for date in c.execute('select min(datetime) as first from snapshots'):
    firstday=date['first']
  
  # Query database to get plant ids
  if signal_type=='vis':
    signal=c.execute('select * from snapshots inner join vis_colors on snapshots.image_id =vis_colors.image_id order by plant_id asc')
  elif signal_type=='nir':
    signal=c.execute('select * from snapshots inner join nir_signal on snapshots.image_id =nir_signal.image_id order by plant_id asc')
  elif signal_type=='fluor':
    signal=c.execute('select * from snapshots inner join flu_signal on snapshots.image_id =flu_signal.image_id order by plant_id asc')
  
  barcode_array=[]
  group_id=[]
  just_id=[]
  ch1_total_array=[]
  ch2_total_array=[]
  ch3_total_array=[]
  
  # find unique ids so that angles can be averaged
  
  for i, group in enumerate(signal):
    bins=int(group['bins'])
    plant_id=group['plant_id']
    barcode_array.append(plant_id,)
  barcode_unique=np.unique(barcode_array)

  # slices can be made from color histogram data or from fluor/nir signal histogram data stored in the database
  if channels=='rgb':
    channel1='blue'
    channel2='green'
    channel3='red'
  elif channels=='lab':
    channel1='lightness'
    channel2='green-magenta'
    channel3='blue-yellow'
  elif channels=='hsv':
    channel1='hue'
    channel2='saturation'
    channel3='value'
  else:
    channel1='signal'
    channel2='signal'
    channel3='signal'
  
  # Make first lines of empty arrays the header titles
  
  ch1_headers=[]
  ch2_headers=[]
  ch3_headers=[]
  bin_nums=np.transpose((np.arange(0,bins, step=1)))
  ch1_headers.append('day')
  ch2_headers.append('day')
  ch3_headers.append('day')
  ch1_headers.append('barcode')
  ch2_headers.append('barcode')
  ch3_headers.append('barcode')
  ch1_headers.append('frame')
  ch2_headers.append('frame')
  ch3_headers.append('frame')
  ch1_headers.append('date_time')
  ch2_headers.append('date_time')
  ch3_headers.append('date_time')

        
  if signal_type=='vis':
    for i,bn in enumerate(bin_nums):
      b=(str(channel1)+"_bin_"+str(bn))
      g=(str(channel2)+"_bin_"+str(bn))
      r=(str(channel3)+"_bin_"+str(bn))
      ch1_headers.append(b)
      ch2_headers.append(g)
      ch3_headers.append(r)
  elif signal_type=='nir':
    for i,bn in enumerate(bin_nums):
      b="nir_bin_"+str(bn)
      ch1_headers.append(b)
  elif signal_type=='fluor':
    for i,bn in enumerate(bin_nums):
      b="fluor_bin_"+str(bn)
      ch1_headers.append(b)
  
  # Initialize the txt files which are compatible with imput into R
  
  if signal_type=='vis':
    header1_fin=','.join(map(str,ch1_headers))
    header2_fin=','.join(map(str,ch2_headers))
    header3_fin=','.join(map(str,ch3_headers))
    filename1=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel1)+"_"+str(timenow)+".txt"
    filename2=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel2)+"_"+str(timenow)+".txt"
    filename3=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel3)+"_"+str(timenow)+".txt"
    signal_file1= os.open(filename1,os.O_RDWR|os.O_CREAT)
    signal_file2= os.open(filename2,os.O_RDWR|os.O_CREAT)
    signal_file3= os.open(filename3,os.O_RDWR|os.O_CREAT)
    os.write(signal_file1, header1_fin)
    os.write(signal_file2, header2_fin)
    os.write(signal_file3, header3_fin)
    os.write(signal_file1, os.linesep)
    os.write(signal_file2, os.linesep)
    os.write(signal_file3, os.linesep)
  else:
    header1_fin=','.join(map(str,ch1_headers))
    filename1=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel1)+"_"+str(timenow)+".txt"
    signal_file1= os.open(filename1,os.O_RDWR|os.O_CREAT)
    os.write(signal_file1, header1_fin)
    os.write(signal_file1, os.linesep)
  
  # For each plant id find the unique timestamp, this will be used to group snapshot angles
  
  for barcode_label in barcode_unique: 
    time_array=[]
    if signal_type=='vis':
      database=h.execute('select * from snapshots inner join vis_colors on snapshots.image_id=vis_colors.image_id where plant_id= ? and camera=? order by datetime asc', (barcode_label,camera_label,))
    elif signal_type=='nir':
      database=h.execute('select * from snapshots inner join nir_signal on snapshots.image_id=nir_signal.image_id where plant_id= ? and camera=? order by datetime asc', (barcode_label,camera_label,))
    elif signal_type=='flu':
      database=h.execute('select * from snapshots inner join flu_signal on snapshots.image_id=flu_signal.image_id where plant_id= ? and camera=? order by datetime asc', (barcode_label,camera_label,))
    
    for i,t in enumerate(database):
      date=(t['datetime'])          
      time_array.append(date,)
    unique_time=np.unique(time_array)
  
  # For each unique time grab the histogram data and either use each individual angle or averaged angles
    
    for time in unique_time:
      dim1_all=[]
      dim2_all=[]
      dim3_all=[]
      date_int=((time-firstday)/86400) 
      if signal_type=='vis':
        database_time=h.execute('select * from snapshots inner join vis_colors on snapshots.image_id=vis_colors.image_id inner join vis_shapes on snapshots.image_id=vis_shapes.image_id where plant_id=? and camera=? and datetime=?',(barcode_label, camera_label,str(time),))
      elif signal_type=='nir':
        database_time=h.execute('select * from snapshots inner join nir_signal on snapshots.image_id=nir_signal.image_id where plant_id=? and camera=? and datetime=?',(barcode_label, camera_label,str(time),))
      elif signal_type=='flu':
        database_time=h.execute('select * from snapshots inner join flu_signal on snapshots.image_id=flu_signal.image_id where plant_id=? and camera=? and datetime=?',(barcode_label, camera_label,str(time),))
      for i, data in enumerate(database_time):
        dim1=np.matrix(data[channel1])
        dim2=np.matrix(data[channel2])
        dim3=np.matrix(data[channel3])
        norm_area=data['area']        
              
        dim1_norm1=[]
        dim2_norm1=[]
        dim3_norm1=[]
        
        for i,x in enumerate(dim1):
          norm_x=(x/norm_area)*100
          dim1_norm1.append(norm_x)
        for i,x in enumerate(dim2):
          norm_x=(x/norm_area)*100
          dim2_norm1.append(norm_x)         
        for i,x in enumerate(dim3):
          norm_x=(x/norm_area)*100
          dim3_norm1.append(norm_x)

        
        dim1_all.append(dim1_norm1)
        dim2_all.append(dim2_norm1)
        dim3_all.append(dim3_norm1)
        
        if average_angles=='off':
          ch1=[]
          ch2=[]
          ch3=[]
          frame=data['frame']
          date_time=data['datetime']
          
          ch1.append(date_int)
          ch2.append(date_int)
          ch3.append(date_int)
          ch1.append(barcode_label)
          ch2.append(barcode_label)
          ch3.append(barcode_label)
          ch1.append(frame)
          ch2.append(frame)
          ch3.append(frame)
          ch1.append(date_time)
          ch2.append(date_time)
          ch3.append(date_time)
          
          dim1_t=np.transpose(dim1_norm1)
          dim2_t=np.transpose(dim2_norm1)
          dim3_t=np.transpose(dim3_norm1)
          
          for i,c in enumerate(dim1_t):
            b=float(c)
            ch1.append(b)
          for i,c in enumerate(dim2_t):
            b=float(c)
            ch2.append(b)
          for i,c in enumerate(dim3_t):
            b=float(c)
            ch3.append(b)
          
          ch1_total_array.append(ch1)
          ch2_total_array.append(ch2)
          ch3_total_array.append(ch3)
          
         
          ch1_join=','.join(map(str,ch1))
          ch2_join=','.join(map(str,ch2))
          ch3_join=','.join(map(str,ch3))
          os.write(signal_file1, ch1_join)
          os.write(signal_file2, ch2_join)
          os.write(signal_file3, ch3_join)
          os.write(signal_file1, os.linesep)
          os.write(signal_file2, os.linesep)
          os.write(signal_file3, os.linesep)
                
      if average_angles=='on':
        ch1=[]
        ch2=[]
        ch3=[]
        frame='all_avg'
        date_time=data['datetime']
        
        ch1_avg=np.transpose(np.average(dim1_all,axis=0))
        ch2_avg=np.transpose(np.average(dim2_all,axis=0))
        ch3_avg=np.transpose(np.average(dim3_all,axis=0))

        ch1.append(date_int)
        ch2.append(date_int)
        ch3.append(date_int)
        ch1.append(barcode_label)
        ch2.append(barcode_label)
        ch3.append(barcode_label)
        ch1.append(frame)
        ch2.append(frame)
        ch3.append(frame)
        ch1.append(date_time)
        ch2.append(date_time)
        ch3.append(date_time)
          
        for i,c in enumerate(ch1_avg):
          b=float(c)
          ch1.append(b)
        for i,c in enumerate(ch2_avg):
          b=float(c)
          ch2.append(b)
        for i,c in enumerate(ch3_avg):
          b=float(c)
          ch3.append(b)
                  
        ch1_total_array.append(ch1)
        ch2_total_array.append(ch2)
        ch3_total_array.append(ch3)
        
        ch1_join=','.join(map(str,ch1))
        ch2_join=','.join(map(str,ch2))
        ch3_join=','.join(map(str,ch3))
        os.write(signal_file1, ch1_join)
        os.write(signal_file2, ch2_join)
        os.write(signal_file3, ch3_join)
        os.write(signal_file1, os.linesep)
        os.write(signal_file2, os.linesep)
        os.write(signal_file3, os.linesep)
  
  os.close(signal_file1)
  os.close(signal_file2)
  os.close(signal_file3)     
  
  return outdir_name

### Main pipeline
def main():
  # Get options
  args = options()

  img_file_dir =color_export(args.database,args.outdir,'vis','vis_sv','hsv','on')
  #img_file_dir='/home/mgehan/LemnaTec/out_folder/slice_figs_and_images_04-21-2014_16:59:07/'
  #avr.cat_fig(args.outdir,img_file_dir)

if __name__ == '__main__':
  main()
        # Gaussian blur device

import cv2
import numpy as np
from . import print_image
from . import plot_image


def gaussian_blur(device, img, ksize, sigmax=0, sigmay=None, debug=None):
    """Applies a Gaussian blur filter.

    Inputs:
    # device  = device number. Used to count steps in the pipeline
    # img     = img object
    # ksize   = kernel size => ksize x ksize box, e.g. (5,5)
    # sigmax = standard deviation in X direction; if 0, calculated from kernel size
    # sigmay = standard deviation in Y direction; if sigmaY is None, sigmaY is taken to equal sigmaX
    # debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    img_gblur = blurred image

    :param img: numpy array
    :param ksize: tuple
    :param sigmax: int
    :param sigmay: str or int
    :param device: int
    :param debug: str
    :return device: int
    :return img_gblur: numpy array
    """

    img_gblur = cv2.GaussianBlur(img, ksize, sigmax, sigmay)

    device += 1
    if debug == 'print':
        print_image(img_gblur, (str(device) + '_gaussian_blur.png'))
    elif debug == 'plot':
        if len(img_gblur) == 3:
            plot_image(img_gblur)
        else:
            plot_image(img_gblur, cmap='gray')

    return device, img_gblur
# Median blur device

import cv2
from . import print_image
from . import plot_image


def median_blur(img, ksize, device, debug=None):
    """Applies a median blur filter (applies median value to central pixel within a kernel size ksize x ksize).

    Inputs:
    # img     = img object
    # ksize   = kernel size => ksize x ksize box
    # device  = device number. Used to count steps in the pipeline
    # debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    img_mblur = blurred image

    :param img: numpy array
    :param ksize: int
    :param device: int
    :param debug: str
    :return device: int
    :return img_mblur: numpy array
    """

    img_mblur = cv2.medianBlur(img, ksize)
    device += 1
    if debug == 'print':
        print_image(img_mblur, (str(device) + '_median_blur' + str(ksize) + '.png'))
    elif debug == 'plot':
        plot_image(img_mblur, cmap='gray')
    return device, img_mblur
# Find Objects Partially Inside Region of Interest or Cut Objects to Region of Interest

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def roi_objects(img, roi_type, roi_contour, roi_hierarchy, object_contour, obj_hierarchy, device, debug=None):
    """Find objects partially inside a region of interest or cut objects to the ROI.

    Inputs:
    img            = img to display kept objects
    roi_type       = 'cutto' or 'partial' (for partially inside)
    roi_contour    = contour of roi, output from "View and Adjust ROI" function
    roi_hierarchy  = contour of roi, output from "View and Adjust ROI" function
    object_contour = contours of objects, output from "Identifying Objects" function
    obj_hierarchy  = hierarchy of objects, output from "Identifying Objects" function
    device         = device number.  Used to count steps in the pipeline
    debug          = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device         = device number
    kept_cnt       = kept contours
    hierarchy      = contour hierarchy list
    mask           = mask image
    obj_area       = total object pixel area

    :param img: numpy array
    :param roi_type: str
    :param roi_contour: list
    :param roi_hierarchy: list
    :param object_contour: list
    :param obj_hierarchy: list
    :param device: int
    :param debug: str
    :return device: int
    :return kept_cnt: list
    :return hierarchy: list
    :return mask: numpy array
    :return obj_area: int
    """

    device += 1
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
    else:
        ix, iy = np.shape(img)

    size = ix, iy, 3
    background = np.zeros(size, dtype=np.uint8)
    ori_img = np.copy(img)
    w_back = background + 255
    background1 = np.zeros(size, dtype=np.uint8)
    background2 = np.zeros(size, dtype=np.uint8)

    # Allows user to find all objects that are completely inside or overlapping with ROI
    if roi_type == 'partial':
        for c, cnt in enumerate(object_contour):
            length = (len(cnt) - 1)
            stack = np.vstack(cnt)
            test = []
            keep = False
            for i in range(0, length):
                pptest = cv2.pointPolygonTest(roi_contour[0], (stack[i][0], stack[i][1]), False)
                if int(pptest) != -1:
                    keep = True
            if keep == True:
                if obj_hierarchy[0][c][3] > -1:
                    cv2.drawContours(w_back, object_contour, c, (255, 255, 255), -1, lineType=8,
                                     hierarchy=obj_hierarchy)
                else:
                    cv2.drawContours(w_back, object_contour, c, (0, 0, 0), -1, lineType=8, hierarchy=obj_hierarchy)
            else:
                cv2.drawContours(w_back, object_contour, c, (255, 255, 255), -1, lineType=8, hierarchy=obj_hierarchy)

        kept = cv2.cvtColor(w_back, cv2.COLOR_RGB2GRAY)
        kept_obj = cv2.bitwise_not(kept)
        mask = np.copy(kept_obj)
        obj_area = cv2.countNonZero(kept_obj)
        kept_cnt, hierarchy = cv2.findContours(kept_obj, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(ori_img, kept_cnt, -1, (0, 255, 0), -1, lineType=8, hierarchy=hierarchy)
        cv2.drawContours(ori_img, roi_contour, -1, (255, 0, 0), 5, lineType=8, hierarchy=roi_hierarchy)

    # Allows user to cut objects to the ROI (all objects completely outside ROI will not be kept)
    elif roi_type == 'cutto':
        cv2.drawContours(background1, object_contour, -1, (255, 255, 255), -1, lineType=8, hierarchy=obj_hierarchy)
        roi_points = np.vstack(roi_contour[0])
        cv2.fillPoly(background2, [roi_points], (255, 255, 255))
        obj_roi = cv2.multiply(background1, background2)
        kept_obj = cv2.cvtColor(obj_roi, cv2.COLOR_RGB2GRAY)
        mask = np.copy(kept_obj)
        obj_area = cv2.countNonZero(kept_obj)
        kept_cnt, hierarchy = cv2.findContours(kept_obj, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(w_back, kept_cnt, -1, (0, 0, 0), -1)
        cv2.drawContours(ori_img, kept_cnt, -1, (0, 255, 0), -1, lineType=8, hierarchy=hierarchy)
        cv2.drawContours(ori_img, roi_contour, -1, (255, 0, 0), 5, lineType=8, hierarchy=roi_hierarchy)

    else:
        fatal_error('ROI Type' + str(roi_type) + ' is not "cutto" or "partial"!')

    if debug == 'print':
        print_image(w_back, (str(device) + '_roi_objects.png'))
        print_image(ori_img, (str(device) + '_obj_on_img.png'))
        print_image(mask, (str(device) + '_roi_mask.png'))
    elif debug == 'plot':
        plot_image(w_back)
        plot_image(ori_img)
        plot_image(mask, cmap='gray')
        # print ('Object Area=', obj_area)

    return device, kept_cnt, hierarchy, mask, obj_area
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
import pandas as pd
from random import randrange
from shutil import copy

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-r", "--random", help="number of random images you would like", type=int, default=50)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=True)
  parser.add_argument("-t", "--imgtype", help= "VIS, NIR, or None", default="VIS")
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Grab Random Image Ids From Database
def grab_random(database,random, outdir, imgtype):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])

  ######################################################################################
  db = sq.connect(database)

  if imgtype=="VIS":
  # Get the headers for metadata and headers for features these will be the headers of features table
    metaimages = pd.read_sql_query('SELECT * FROM metadata INNER JOIN analysis_images ON '
                                  'metadata.image_id=analysis_images.image_id where analysis_images.type=="mask" '
                                 'and metadata.imgtype=="VIS"', db)
  elif imgtype=="NIR":
  # Get the headers for metadata and headers for features these will be the headers of features table
    metaimages = pd.read_sql_query('SELECT * FROM metadata INNER JOIN analysis_images ON '
                                  'metadata.image_id=analysis_images.image_id where analysis_images.type=="mask" '
                                 'and metadata.imgtype=="NIR"', db)
  elif imgtype=="None":
  # Get the headers for metadata and headers for features these will be the headers of features table
    metaimages = pd.read_sql_query('SELECT * FROM metadata INNER JOIN analysis_images ON '
                                  'metadata.image_id=analysis_images.image_id where analysis_images.type=="mask" '
                                 'and metadata.imgtype=="none"', db)
  else:
    pcv.fatal_error("imgtype is not VIS, NIR or None")

  sorted=metaimages.sort_values(['timestamp'])

  nrowsorted=sorted.shape[0]

  chunksfactor=nrowsorted/float(random)

  random_index_list=[]

  for i in range(2,random+1,1):
      x=int((i-1)*chunksfactor)
      y=int((i)*chunksfactor)
      randnum=randrange(x,y)
      random_index_list.append(randnum)

  maskdir=str(outdir)+"/training-mask-images"
  oridir=str(outdir)+"/training-ori-images"

  if not os.path.exists(maskdir):
    os.makedirs(maskdir)
  if not os.path.exists(oridir):
    os.makedirs(oridir)

  for x in random_index_list:
    selectmask=str(sorted.iloc[[x-1]]['image_path'].item())
    selectori=str(sorted.iloc[[x-1]]['image'].item())
    copy(selectmask, maskdir+"/")
    copy(selectori, oridir+"/")

### Main pipeline
def main():
  # Get options
  args = options()

  grab_random(args.database, args.random, args.outdir, args.imgtype)


if __name__ == '__main__':
  main()
#!/usr/bin/env python
from __future__ import print_function
import os
import argparse


# Parse command-line arguments
###########################################
def options():
    """Parse command line options.

    Args:

    Returns:
        argparse object.
    Raises:

    """

    parser = argparse.ArgumentParser(description="Checks a PhenoFront download for completeness.",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("-d", "--dir", help="Input PhenoFront download directory.", required=True)
    parser.add_argument("-f", "--format", help="Image format.", default="png")
    parser.add_argument("-o", "--outfile", help="Output file for missing images list.", default="missing_images.txt")
    args = parser.parse_args()

    return args


###########################################

# Main
###########################################
def main():
    """Main program.

    Args:

    Returns:

    Raises:
    """

    # Get options
    args = options()

    # Open the SnapshotInfo.csv file
    csvfile = open(args.dir + '/SnapshotInfo.csv', 'rU')

    # Read the first header line
    header = csvfile.readline()
    header = header.rstrip('\n')

    # Remove whitespace from the field name
    header = header.replace(" ", "")

    # Table column order
    cols = header.split(',')
    colnames = {}
    for i, col in enumerate(cols):
        colnames[col] = i

    total_imgs = 0
    downloaded_imgs = 0
    missing_imgs = []

    # Read through the CSV file
    for row in csvfile:
        row = row.rstrip('\n')
        data = row.split(',')
        img_list = data[colnames['tiles']]
        img_list = img_list[:-1]
        imgs = img_list.split(';')
        for img in imgs:
            if len(img) != 0:
                total_imgs += 1
                dirpath = args.dir + '/snapshot' + data[colnames['id']]
                filename = img + '.' + args.format
                if not os.path.exists(dirpath + '/' + filename):
                    missing_imgs.append(dirpath + '/' + filename)
                else:
                    downloaded_imgs += 1

    print("Total images in experiment: " + str(total_imgs) + '\n')
    print("Total downloaded images: " + str(downloaded_imgs) + '\n')

    # Missing files
    if len(missing_imgs) > 0:
        outfile = open(args.outfile, 'w')
        for img in missing_imgs:
            outfile.write(img + '\n')


if __name__ == '__main__':
    main()
#!/usr/bin/env python

import pytest
import os
import shutil
import numpy as np
import cv2
import plantcv as pcv
import plantcv.learn
# Import matplotlib and use a null Template to block plotting to screen
# This will let us test debug = "plot"
import matplotlib
matplotlib.use('Template')

TEST_DATA = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
TEST_TMPDIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", ".cache")
TEST_COLOR_DIM = (2056, 2454, 3)
TEST_GRAY_DIM = (2056, 2454)
TEST_BINARY_DIM = TEST_GRAY_DIM
TEST_INPUT_COLOR = "input_color_img.jpg"
TEST_INPUT_GRAY = "input_gray_img.jpg"
TEST_INPUT_BINARY = "input_binary_img.png"
TEST_INPUT_ROI = "input_roi.npz"
TEST_INPUT_CONTOURS = "input_contours.npz"
TEST_VIS = "VIS_SV_0_z300_h1_g0_e85_v500_93054.png"
TEST_NIR = "NIR_SV_0_z300_h1_g0_e15000_v500_93059.png"
TEST_VIS_TV = "VIS_TV_0_z300_h1_g0_e85_v500_93054.png"
TEST_NIR_TV = "NIR_TV_0_z300_h1_g0_e15000_v500_93059.png"
TEST_INPUT_MASK = "input_mask.png"
TEST_INPUT_NIR_MASK = "input_nir.png"
TEST_INPUT_FDARK = "FLUO_TV_dark.png"
TEST_INPUT_FMIN = "FLUO_TV_min.png"
TEST_INPUT_FMAX = "FLUO_TV_max.png"
TEST_INPUT_FMASK = "FLUO_TV_MASK.png"
TEST_INTPUT_GREENMAG = "input_green-magenta.jpg"
TEST_INTPUT_MULTI = "multi_ori_image.jpg"
TEST_INPUT_MULTI_CONTOUR = "roi_objects.npz"
TEST_INPUT_ClUSTER_CONTOUR = "clusters_i.npz"
TEST_INPUT_CROPPED = 'cropped_img.jpg'
TEST_INPUT_CROPPED_MASK = 'cropped-mask.png'
TEST_INPUT_MARKER = 'seed-image.jpg'
TEST_FOREGROUND = "TEST_FOREGROUND.jpg"
TEST_BACKGROUND = "TEST_BACKGROUND.jpg"
TEST_PDFS = "naive_bayes_pdfs.txt"
TEST_VIS_SMALL = "setaria_small_vis.png"
TEST_MASK_SMALL = "setaria_small_mask.png"
TEST_VIS_COMP_CONTOUR = "setaria_composed_contours.npz"
TEST_ACUTE_RESULT = np.asarray([[[119, 285]], [[151, 280]], [[168, 267]], [[168, 262]], [[171, 261]], [[224, 269]],
                                [[246, 271]], [[260, 277]], [[141, 248]], [[183, 194]], [[188, 237]], [[173, 240]],
                                [[186, 260]], [[147, 244]], [[163, 246]], [[173, 268]], [[170, 272]], [[151, 320]],
                                [[195, 289]], [[228, 272]], [[210, 272]], [[209, 247]], [[210, 232]]])
TEST_VIS_SMALL_PLANT = "setaria_small_plant_vis.png"
TEST_MASK_SMALL_PLANT = "setaria_small_plant_mask.png"
TEST_VIS_COMP_CONTOUR_SMALL_PLANT = "setaria_small_plant_composed_contours.npz"
TEST_SAMPLED_RGB_POINTS = "sampled_rgb_points.txt"

if not os.path.exists(TEST_TMPDIR):
    os.mkdir(TEST_TMPDIR)

# ##########################
# Tests for the main package
# ##########################


def test_plantcv_acute():
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "print"
    _ = pcv.acute(obj=obj_contour, win=5, thresh=15, mask=mask, device=0, debug="print")
    # Test with debug = None
    device, homology_pts = pcv.acute(obj=obj_contour, win=5, thresh=15, mask=mask, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(homology_pts), (29, 1, 2)))


def test_plantcv_acute_vertex():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL))
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "print"
    _ = pcv.acute_vertex(obj=obj_contour, win=5, thresh=15, sep=5, img=img, device=0, debug="print")
    os.rename("1_acute_vertices.png", os.path.join(TEST_TMPDIR, "1_acute_vertices.png"))
    # Test with debug = "plot"
    _ = pcv.acute_vertex(obj=obj_contour, win=5, thresh=15, sep=5, img=img, device=0, debug="plot")
    # Test with debug = None
    device, acute = pcv.acute_vertex(obj=obj_contour, win=5, thresh=15, sep=5, img=img, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(acute), np.shape(TEST_ACUTE_RESULT)))


def test_plantcv_acute_vertex_bad_obj():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL))
    obj_contour = np.array([])
    result = pcv.acute_vertex(obj=obj_contour, win=5, thresh=15, sep=5, img=img, device=0, debug=None)
    assert all([i == j] for i, j in zip(result, [0, ("NA", "NA")]))


def test_plantcv_adaptive_threshold():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with threshold type = "mean"
    _ = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="mean", object_type="light", device=0, debug=None)
    # Test with debug = "print"
    _ = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="gaussian", object_type="light", device=0,
                               debug="print")
    os.rename("1_adaptive_threshold_gaussian.png", os.path.join(TEST_TMPDIR, "1_adaptive_threshold_gaussian.png"))
    # Test with debug = "plot"
    _ = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="gaussian", object_type="light", device=0,
                               debug="plot")
    # Test with debug = None
    device, binary_img = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="gaussian", object_type="light",
                                                device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(binary_img), TEST_GRAY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(binary_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_adaptive_threshold_incorrect_threshold_type():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="gauss", object_type="light", device=0, debug=None)


def test_plantcv_adaptive_threshold_incorrect_object_type():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.adaptive_threshold(img=img, maxValue=255, thres_type="mean", object_type="lite", device=0, debug=None)


def test_plantcv_analyze_bound():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_INPUT_CONTOURS))
    object_contours = contours_npz['arr_0']
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_COLOR)
    _ = pcv.analyze_bound(img=img, imgname="img", obj=object_contours[0], mask=mask, line_position=300, device=0,
                          debug="print", filename=outfile)
    os.rename("1_boundary_on_img.jpg", os.path.join(TEST_TMPDIR, "1_boundary_on_img.jpg"))
    os.rename("1_boundary_on_white.jpg", os.path.join(TEST_TMPDIR, "1_boundary_on_white.jpg"))
    # Test with debug = "plot"
    _ = pcv.analyze_bound(img=img, imgname="img", obj=object_contours[0], mask=mask, line_position=300, device=0,
                          debug="plot", filename=False)
    # Test with debug = None
    device, boundary_header, boundary_data, boundary_img1 = pcv.analyze_bound(img=img, imgname="img",
                                                                              obj=object_contours[0], mask=mask,
                                                                              line_position=300, device=0,
                                                                              debug=None, filename=False)
    assert boundary_data[3] == 596347


def test_plantcv_analyze_color():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_COLOR)
    _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="print", hist_plot_type="all",
                          pseudo_channel="v", pseudo_bkg="img", resolution=300, filename=outfile)
    os.rename("1_img_pseudocolor.jpg", os.path.join(TEST_TMPDIR, "1_img_pseudocolor.jpg"))
    os.rename("1_all_hist.svg", os.path.join(TEST_TMPDIR, "1_all_hist.svg"))
    # Test with debug = "plot"
    _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="plot", hist_plot_type=None,
                          pseudo_channel="v", pseudo_bkg="img", resolution=300, filename=False)
    # Test with debug = "plot" and pseudo_bkg = "white"
    _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="plot", hist_plot_type=None,
                          pseudo_channel="v", pseudo_bkg="white", resolution=300, filename=False)
    # Test with debug = None
    device, color_header, color_data, analysis_images = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256,
                                                                          device=0, debug=None, hist_plot_type=None,
                                                                          pseudo_channel="v", pseudo_bkg="img",
                                                                          resolution=300, filename=False)
    assert np.sum(color_data[3]) != 0


def test_plantcv_analyze_color_incorrect_pseudo_channel():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="plot", hist_plot_type=None,
                              pseudo_channel="x", pseudo_bkg="white", resolution=300, filename=False)


def test_plantcv_analyze_color_incorrect_pseudo_background():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="plot", hist_plot_type=None,
                              pseudo_channel="v", pseudo_bkg="black", resolution=300, filename=False)


def test_plantcv_analyze_color_incorrect_hist_plot_type():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.analyze_color(img=img, imgname="img", mask=mask, bins=256, device=0, debug="plot", hist_plot_type="bgr",
                              pseudo_channel="v", pseudo_bkg="white", resolution=300, filename=False)


def test_plantcv_analyze_nir():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR), 0)
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_COLOR)
    _ = pcv.analyze_NIR_intensity(img=img, rgbimg=img, mask=mask, bins=256, device=0, histplot=False, debug="print",
                                  filename=outfile)
    os.rename("3_nir_pseudo_plant.jpg", os.path.join(TEST_TMPDIR, "3_nir_pseudo_plant.jpg"))
    os.rename("3_nir_pseudo_plant_back.jpg", os.path.join(TEST_TMPDIR, "3_nir_pseudo_plant_back.jpg"))
    # Test with debug = "plot"
    _ = pcv.analyze_NIR_intensity(img=img, rgbimg=img, mask=mask, bins=256, device=0, histplot=False, debug="plot",
                                  filename=False)
    # Test with debug = None
    device, hist_header, hist_data, h_norm = pcv.analyze_NIR_intensity(img=img, rgbimg=img, mask=mask, bins=256,
                                                                       device=0, histplot=False, debug=None,
                                                                       filename=False)
    assert np.sum(hist_data[3]) == 713986


def test_plantcv_analyze_object():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_INPUT_CONTOURS))
    obj_contour = contours_npz['arr_0']
    max_obj = max(obj_contour, key=len)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_COLOR)
    _ = pcv.analyze_object(img=img, imgname="img", obj=max_obj, mask=mask, device=0, debug="print", filename=outfile)
    os.rename("1_shapes.jpg", os.path.join(TEST_TMPDIR, "1_shapes.jpg"))
    # Test with debug = "plot"
    _ = pcv.analyze_object(img=img, imgname="img", obj=max_obj, mask=mask, device=0, debug="plot", filename=False)
    # Test with debug = None
    device, obj_header, obj_data, obj_images = pcv.analyze_object(img=img, imgname="img", obj=max_obj, mask=mask,
                                                                  device=0, debug=None, filename=False)
    assert obj_data[1] != 0


def test_plantcv_apply_mask_white():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.apply_mask(img=img, mask=mask, mask_color="white", device=0, debug="print")
    os.rename("1_wmasked.png", os.path.join(TEST_TMPDIR, "1_wmasked.png"))
    # Test with debug = "plot"
    _ = pcv.apply_mask(img=img, mask=mask, mask_color="white", device=0, debug="plot")
    # Test with debug = None
    device, masked_img = pcv.apply_mask(img=img, mask=mask, mask_color="white", device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(masked_img), TEST_COLOR_DIM))


def test_plantcv_apply_mask_black():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.apply_mask(img=img, mask=mask, mask_color="black", device=0, debug="print")
    os.rename("1_bmasked.png", os.path.join(TEST_TMPDIR, "1_bmasked.png"))
    # Test with debug = "plot"
    _ = pcv.apply_mask(img=img, mask=mask, mask_color="black", device=0, debug="plot")
    # Test with debug = None
    device, masked_img = pcv.apply_mask(img=img, mask=mask, mask_color="black", device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(masked_img), TEST_COLOR_DIM))


def test_plantcv_auto_crop():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INTPUT_MULTI), -1)
    contours = np.load(os.path.join(TEST_DATA, TEST_INPUT_MULTI_CONTOUR))
    roi_contours = contours['arr_0']
    # Test with debug = "print"
    _ = pcv.auto_crop(device=0, img=img1, objects=roi_contours[48], padding_x=20, padding_y=20, color='black',
                      debug="print")
    os.rename("1_crop_area.png", os.path.join(TEST_TMPDIR, "1_crop_area.png"))
    os.rename("1_auto_cropped.png", os.path.join(TEST_TMPDIR, "1_auto_cropped.png"))
    # Test with debug = "plot"
    _ = pcv.auto_crop(device=0, img=img1, objects=roi_contours[48], padding_x=20, padding_y=20, color='black',
                      debug="plot")
    # Test with debug = None
    device, cropped = pcv.auto_crop(device=0, img=img1, objects=roi_contours[48], padding_x=20, padding_y=20,
                                    color='black', debug=None)
    x, y, z = np.shape(img1)
    x1, y1, z1 = np.shape(cropped)
    assert x > x1


def test_plantcv_binary_threshold():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with object type = dark
    _ = pcv.binary_threshold(img=img, threshold=25, maxValue=255, object_type="dark", device=0, debug=None)
    # Test with debug = "print"
    _ = pcv.binary_threshold(img=img, threshold=25, maxValue=255, object_type="light", device=0, debug="print")
    os.rename("1_binary_threshold25.png", os.path.join(TEST_TMPDIR, "1_binary_threshold25.png"))
    # Test with debug = "plot"
    _ = pcv.binary_threshold(img=img, threshold=25, maxValue=255, object_type="light", device=0, debug="plot")
    # Test with debug = None
    device, binary_img = pcv.binary_threshold(img=img, threshold=25, maxValue=255, object_type="light",
                                              device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(binary_img), TEST_GRAY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(binary_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_binary_threshold_incorrect_object_type():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    with pytest.raises(RuntimeError):
        _ = pcv.binary_threshold(img=img, threshold=25, maxValue=255, object_type="lite", device=0, debug=None)


def test_plantcv_cluster_contours():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INTPUT_MULTI), -1)
    contours = np.load(os.path.join(TEST_DATA, TEST_INPUT_MULTI_CONTOUR))
    roi_contours = contours['arr_0']
    # Test with debug = "print"
    _ = pcv.cluster_contours(device=0, img=img1, roi_objects=roi_contours, nrow=4, ncol=6, debug="print")
    os.rename("1_clusters.png", os.path.join(TEST_TMPDIR, "1_clusters.png"))
    # Test with debug = "plot"
    _ = pcv.cluster_contours(device=0, img=img1, roi_objects=roi_contours, nrow=4, ncol=6, debug="plot")
    # Test with debug = None
    device, clusters_i, contours = pcv.cluster_contours(device=0, img=img1, roi_objects=roi_contours, nrow=4, ncol=6,
                                                        debug=None)
    lenori = len(roi_contours)
    lenclust = len(clusters_i)
    assert lenori > lenclust


def test_plantcv_cluster_contours_splitimg():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INTPUT_MULTI), -1)
    contours = np.load(os.path.join(TEST_DATA, TEST_INPUT_MULTI_CONTOUR))
    clusters = np.load(os.path.join(TEST_DATA, TEST_INPUT_ClUSTER_CONTOUR))
    roi_contours = contours['arr_0']
    cluster_contours = clusters['arr_0']
    # Test with debug = "print"
    _ = pcv.cluster_contour_splitimg(device=0, img=img1, grouped_contour_indexes=cluster_contours,
                                     contours=roi_contours, outdir=TEST_TMPDIR, file=None, filenames=None,
                                     debug="print")
    for i in range(1, 19):
        os.rename(str(i) + "_clusters.png", os.path.join(TEST_TMPDIR, str(i) + "_clusters.png"))
        os.rename(str(i) + "_wmasked.png", os.path.join(TEST_TMPDIR, str(i) + "_wmasked.png"))
    # Test with debug = "plot"
    _ = pcv.cluster_contour_splitimg(device=0, img=img1, grouped_contour_indexes=cluster_contours,
                                     contours=roi_contours, outdir=None, file=None, filenames=None, debug="plot")
    # Test with debug = None
    device, output_path = pcv.cluster_contour_splitimg(device=0, img=img1, grouped_contour_indexes=cluster_contours,
                                                       contours=roi_contours, outdir=None, file=None,
                                                       filenames=None, debug=None)
    assert len(output_path) != 0


def test_plantcv_color_palette():
    # Collect assertions
    truths = []

    # Return one random color
    colors = pcv.color_palette(1)
    # Colors should be a list of length 1, containing a tuple of length 3
    truths.append(len(colors) == 1)
    truths.append(len(colors[0]) == 3)

    # Return ten random colors
    colors = pcv.color_palette(10)
    # Colors should be a list of length 10
    truths.append(len(colors) == 10)
    # All of these should be true for the function to pass testing.
    assert (all(truths))


def test_plantcv_crop_position_mask():
    nir, path1, filename1 = pcv.readimage(os.path.join(TEST_DATA, TEST_INPUT_NIR_MASK))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_MASK), -1)
    # Test with debug = "print"
    _ = pcv.crop_position_mask(nir, mask, device=0, x=40, y=3, v_pos="top", h_pos="right", debug="print")
    os.rename("1_mask_overlay.png", os.path.join(TEST_TMPDIR, "1_mask_overlay.png"))
    os.rename("1_newmask.png", os.path.join(TEST_TMPDIR, "1_newmask.png"))
    os.rename("1_push-right.png", os.path.join(TEST_TMPDIR, "1_push-right.png"))
    os.rename("1_push-top_.png", os.path.join(TEST_TMPDIR, "1_push-top_.png"))
    # Test with debug = "plot"
    _ = pcv.crop_position_mask(nir, mask, device=0, x=40, y=3, v_pos="top", h_pos="right", debug="plot")
    # Test with debug = None
    device, newmask = pcv.crop_position_mask(nir, mask, device=0, x=40, y=3, v_pos="top", h_pos="right", debug=None)
    assert np.sum(newmask) == 641517


def test_plantcv_define_roi_rectangle():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default", debug="print", adjust=True,
                       x_adj=600, y_adj=300, w_adj=-500, h_adj=-600)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default", debug="plot", adjust=True,
                       x_adj=600, y_adj=300, w_adj=-500, h_adj=-600)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=True, x_adj=600, y_adj=500, w_adj=-300, h_adj=-600)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 2 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_rectangle_no_adjust():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default", debug="print", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default", debug="plot", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=False, x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 2 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_circle():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default", debug="print", adjust=True,
                       x_adj=0, y_adj=300, w_adj=0, h_adj=-900)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default", debug="plot", adjust=True,
                       x_adj=0, y_adj=300, w_adj=0, h_adj=-900)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=True, x_adj=0, y_adj=300, w_adj=0, h_adj=-900)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 1 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_circle_no_adjust():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default", debug="print", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default", debug="plot", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="circle", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=False, x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 1 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_ellipse():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default", debug="print", adjust=True,
                       x_adj=0, y_adj=300, w_adj=-1000, h_adj=-900)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default", debug="plot", adjust=True,
                       x_adj=0, y_adj=300, w_adj=-1000, h_adj=-900)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=True, x_adj=0, y_adj=300, w_adj=-1000, h_adj=-900)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 2 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_ellipse_no_adjust():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default", debug="print", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default", debug="plot", adjust=False,
                       x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Test with debug = None
    device, contours, hierarchy = pcv.define_roi(img=img, shape="ellipse", device=0, roi=None, roi_input="default",
                                                 debug=None, adjust=False, x_adj=0, y_adj=0, w_adj=0, h_adj=0)
    # Assert the contours and hierarchy lists contain only the ROI
    if len(contours) == 2 and len(hierarchy) == 1:
        assert 1
    else:
        assert 0


def test_plantcv_define_roi_bad_adjust_values():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    with pytest.raises(RuntimeError):
        _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="default", debug=None, adjust=True,
                           x_adj=1, y_adj=0, w_adj=1, h_adj=0)


def test_plantcv_define_roi_bad_roi_input():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    with pytest.raises(RuntimeError):
        _ = pcv.define_roi(img=img, shape="rectangle", device=0, roi=None, roi_input="test", debug=None, adjust=True,
                           x_adj=0, y_adj=0, w_adj=0, h_adj=0)


def test_plantcv_dilate():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.dilate(img=img, kernel=5, i=1, device=0, debug="print")
    os.rename("1_dil_image_itr_1.png", os.path.join(TEST_TMPDIR, "1_dil_image_itr_1.png"))
    # Test with debug = "plot"
    _ = pcv.dilate(img=img, kernel=5, i=1, device=0, debug="plot")
    # Test with debug = None
    device, dilate_img = pcv.dilate(img=img, kernel=5, i=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(dilate_img), TEST_BINARY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(dilate_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_erode():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.erode(img=img, kernel=5, i=1, device=0, debug="print")
    os.rename("1_er_image_itr_1.png", os.path.join(TEST_TMPDIR, "1_er_image_itr_1.png"))
    # Test with debug = "plot"
    _ = pcv.erode(img=img, kernel=5, i=1, device=0, debug="plot")
    # Test with debug = None
    device, erode_img = pcv.erode(img=img, kernel=5, i=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(erode_img), TEST_BINARY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(erode_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_fatal_error():
    # Verify that the fatal_error function raises a RuntimeError
    with pytest.raises(RuntimeError):
        pcv.fatal_error("Test error")


def test_plantcv_fill():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    mask = np.copy(img)
    _ = pcv.fill(img=img, mask=mask, size=1, device=0, debug="print")
    os.rename("1_fill1.png", os.path.join(TEST_TMPDIR, "1_fill1.png"))
    # Test with debug = "plot"
    mask = np.copy(img)
    _ = pcv.fill(img=img, mask=mask, size=1, device=0, debug="plot")
    # Test with debug = None
    mask = np.copy(img)
    device, fill_img = pcv.fill(img=img, mask=mask, size=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    assert all([i == j] for i, j in zip(np.shape(fill_img), TEST_BINARY_DIM))


def test_plantcv_find_objects():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.find_objects(img=img, mask=mask, device=0, debug="print")
    os.rename("1_id_objects.png", os.path.join(TEST_TMPDIR, "1_id_objects.png"))
    # Test with debug = "plot"
    _ = pcv.find_objects(img=img, mask=mask, device=0, debug="plot")
    # Test with debug = None
    device, contours, hierarchy = pcv.find_objects(img=img, mask=mask, device=0, debug=None)
    # Assert the correct number of contours are found
    assert len(contours) == 7341


def test_plantcv_flip():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.flip(img=img, direction="horizontal", device=0, debug="print")
    os.rename("1_flipped.png", os.path.join(TEST_TMPDIR, "1_flipped.png"))
    # Test with debug = "plot"
    _ = pcv.flip(img=img, direction="vertical", device=0, debug="plot")
    # Test with debug = None
    device, flipped_img = pcv.flip(img=img, direction="horizontal", device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(flipped_img), TEST_COLOR_DIM))


def test_plantcv_flip_bad_input():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    with pytest.raises(RuntimeError):
        _ = pcv.flip(img=img, direction="vert", device=0, debug=None)


def test_plantcv_fluor_fvfm():
    fdark = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_FDARK), -1)
    fmin = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_FMIN), -1)
    fmax = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_FMAX), -1)
    fmask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_FMASK), -1)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_FMAX)
    _ = pcv.fluor_fvfm(fdark=fdark, fmin=fmin, fmax=fmax, mask=fmask, device=0, filename=outfile, bins=1000,
                       debug="print")
    os.rename("1_fmin_mask.png", os.path.join(TEST_TMPDIR, "1_fmin_mask.png"))
    os.rename("1_fmax_mask.png", os.path.join(TEST_TMPDIR, "1_fmax_mask.png"))
    os.rename("1_fv_convert.png", os.path.join(TEST_TMPDIR, "1_fv_convert.png"))
    # Test with debug = "plot"
    _ = pcv.fluor_fvfm(fdark=fdark, fmin=fmin, fmax=fmax, mask=fmask, device=0, filename=False, bins=1000, debug="plot")
    # Test with debug = None
    device, fvfm_header, fvfm_data = pcv.fluor_fvfm(fdark=fdark, fmin=fmin, fmax=fmax, mask=fmask, device=0,
                                                    filename=False, bins=1000, debug=None)
    assert fvfm_data[4] > 0.66


def test_plantcv_gaussian_blur():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.gaussian_blur(device=0, img=img, ksize=(51, 51), sigmax=0, sigmay=None, debug="print")
    os.rename("1_gaussian_blur.png", os.path.join(TEST_TMPDIR, "1_gaussian_blur.png"))
    # Test with debug = "plot"
    _ = pcv.gaussian_blur(device=0, img=img, ksize=(51, 51), sigmax=0, sigmay=None, debug="plot")
    # Test with debug = None
    device, gaussian_img = pcv.gaussian_blur(device=0, img=img, ksize=(51, 51), sigmax=0, sigmay=None, debug=None)
    imgavg = np.average(img)
    gavg = np.average(gaussian_img)
    assert gavg != imgavg


def test_plantcv_get_nir_sv():
    device, nirpath = pcv.get_nir(TEST_DATA, TEST_VIS, device=0, debug=None)
    nirpath1 = os.path.join(TEST_DATA, TEST_NIR)
    assert nirpath == nirpath1


def test_plantcv_get_nir_tv():
    device, nirpath = pcv.get_nir(TEST_DATA, TEST_VIS_TV, device=0, debug=None)
    nirpath1 = os.path.join(TEST_DATA, TEST_NIR_TV)
    assert nirpath == nirpath1


def test_plantcv_hist_equalization():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.hist_equalization(img=img, device=0, debug="print")
    os.rename("1_hist_equal_img.png", os.path.join(TEST_TMPDIR, "1_hist_equal_img.png"))
    # Test with debug = "plot"
    _ = pcv.hist_equalization(img=img, device=0, debug="plot")
    # Test with debug = None
    device, hist = pcv.hist_equalization(img=img, device=0, debug=None)
    histavg = np.average(hist)
    imgavg = np.average(img)
    assert histavg != imgavg


def test_plantcv_image_add():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    img2 = np.copy(img1)
    # Test with debug = "print"
    _ = pcv.image_add(img1=img1, img2=img2, device=0, debug="print")
    os.rename("1_added.png", os.path.join(TEST_TMPDIR, "1_added.png"))
    # Test with debug = "plot"
    _ = pcv.image_add(img1=img1, img2=img2, device=0, debug="plot")
    # Test with debug = None
    device, added_img = pcv.image_add(img1=img1, img2=img2, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(added_img), TEST_BINARY_DIM))


def test_plantcv_image_subtract():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    img2 = np.copy(img1)
    # Test with debug = "print"
    _ = pcv.image_subtract(img1=img1, img2=img2, device=0, debug="print")
    os.rename("1_subtracted.png", os.path.join(TEST_TMPDIR, "1_subtracted.png"))
    # Test with debug = "plot"
    _ = pcv.image_subtract(img1=img1, img2=img2, device=0, debug="plot")
    # Test with debug = None
    device, subtract_img = pcv.image_subtract(img1=img1, img2=img2, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(subtract_img), TEST_BINARY_DIM))


def test_plantcv_invert():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.invert(img=img, device=0, debug="print")
    os.rename("1_invert.png", os.path.join(TEST_TMPDIR, "1_invert.png"))
    # Test with debug = "plot"
    _ = pcv.invert(img=img, device=0, debug="plot")
    # Test with debug = None
    device, inverted_img = pcv.invert(img=img, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(inverted_img), TEST_BINARY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(inverted_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_landmark_reference_pt_dist():
    points_rescaled = [(0.0139, 0.2569), (0.2361, 0.2917), (0.3542, 0.3819), (0.3542, 0.4167), (0.375, 0.4236),
                       (0.7431, 0.3681), (0.8958, 0.3542), (0.9931, 0.3125), (0.1667, 0.5139), (0.4583, 0.8889),
                       (0.4931, 0.5903), (0.3889, 0.5694), (0.4792, 0.4306), (0.2083, 0.5417), (0.3194, 0.5278),
                       (0.3889, 0.375), (0.3681, 0.3472), (0.2361, 0.0139), (0.5417, 0.2292), (0.7708, 0.3472),
                       (0.6458, 0.3472), (0.6389, 0.5208), (0.6458, 0.625)]
    centroid_rescaled = (0.4685, 0.4945)
    bottomline_rescaled = (0.4685, 0.2569)
    results = pcv.landmark_reference_pt_dist(points_r=points_rescaled, centroid_r=centroid_rescaled,
                                             bline_r=bottomline_rescaled, device=0, debug=None)
    assert len(results) == 9


def test_plantcv_laplace_filter():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.laplace_filter(img=img, k=1, scale=1, device=0, debug="print")
    os.rename("1_lp_out_k1_scale1.png", os.path.join(TEST_TMPDIR, "1_lp_out_k1_scale1.png"))
    # Test with debug = "plot"
    _ = pcv.laplace_filter(img=img, k=1, scale=1, device=0, debug="plot")
    # Test with debug = None
    device, lp_img = pcv.laplace_filter(img=img, k=1, scale=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    assert all([i == j] for i, j in zip(np.shape(lp_img), TEST_GRAY_DIM))


def test_plantcv_logical_and():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    img2 = np.copy(img1)
    # Test with debug = "print"
    _ = pcv.logical_and(img1=img1, img2=img2, device=0, debug="print")
    os.rename("1_and_joined.png", os.path.join(TEST_TMPDIR, "1_and_joined.png"))
    # Test with debug = "plot"
    _ = pcv.logical_and(img1=img1, img2=img2, device=0, debug="plot")
    # Test with debug = None
    device, and_img = pcv.logical_and(img1=img1, img2=img2, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(and_img), TEST_BINARY_DIM))


def test_plantcv_logical_or():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    img2 = np.copy(img1)
    # Test with debug = "print"
    _ = pcv.logical_or(img1=img1, img2=img2, device=0, debug="print")
    os.rename("1_or_joined.png", os.path.join(TEST_TMPDIR, "1_or_joined.png"))
    # Test with debug = "plot"
    _ = pcv.logical_or(img1=img1, img2=img2, device=0, debug="plot")
    # Test with debug = None
    device, or_img = pcv.logical_or(img1=img1, img2=img2, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(or_img), TEST_BINARY_DIM))


def test_plantcv_logical_xor():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    img2 = np.copy(img1)
    # Test with debug = "print"
    _ = pcv.logical_xor(img1=img1, img2=img2, device=0, debug="print")
    os.rename("1_xor_joined.png", os.path.join(TEST_TMPDIR, "1_xor_joined.png"))
    # Test with debug = "plot"
    _ = pcv.logical_xor(img1=img1, img2=img2, device=0, debug="plot")
    # Test with debug = None
    device, xor_img = pcv.logical_xor(img1=img1, img2=img2, device=0, debug=None)
    assert all([i == j] for i, j in zip(np.shape(xor_img), TEST_BINARY_DIM))


def test_plantcv_median_blur():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.median_blur(img=img, ksize=5, device=0, debug="print")
    os.rename("1_median_blur5.png", os.path.join(TEST_TMPDIR, "1_median_blur5.png"))
    # Test with debug = "plot"
    _ = pcv.median_blur(img=img, ksize=5, device=0, debug="plot")
    # Test with debug = None
    device, blur_img = pcv.median_blur(img=img, ksize=5, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(blur_img), TEST_BINARY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(blur_img), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_naive_bayes_classifier():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.naive_bayes_classifier(img=img, pdf_file=os.path.join(TEST_DATA, TEST_PDFS), device=0, debug="print")
    os.rename("1_naive_bayes_plant_mask.jpg", os.path.join(TEST_TMPDIR, "1_naive_bayes_plant_mask.jpg"))
    os.rename("1_naive_bayes_background_mask.jpg", os.path.join(TEST_TMPDIR, "1_naive_bayes_background_mask.jpg"))
    # Test with debug = "plot"
    _ = pcv.naive_bayes_classifier(img=img, pdf_file=os.path.join(TEST_DATA, TEST_PDFS), device=0, debug="plot")
    # Test with debug = None
    device, mask = pcv.naive_bayes_classifier(img=img, pdf_file=os.path.join(TEST_DATA, TEST_PDFS), device=0,
                                              debug=None)

    # Assert that the output image has the dimensions of the input image
    if all([i == j] for i, j in zip(np.shape(mask), TEST_GRAY_DIM)):
        # Assert that the image is binary
        if all([i == j] for i, j in zip(np.unique(mask), [0, 255])):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_object_composition():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_INPUT_CONTOURS))
    object_contours = contours_npz['arr_0']
    object_hierarchy = contours_npz['arr_1']
    # Test with debug = "print"
    _ = pcv.object_composition(img=img, contours=object_contours, hierarchy=object_hierarchy, device=0, debug="print")
    os.rename("1_objcomp.png", os.path.join(TEST_TMPDIR, "1_objcomp.png"))
    os.rename("1_objcomp_mask.png", os.path.join(TEST_TMPDIR, "1_objcomp_mask.png"))
    # Test with debug = "plot"
    _ = pcv.object_composition(img=img, contours=object_contours, hierarchy=object_hierarchy, device=0, debug="plot")
    # Test with debug = None
    device, contours, mask = pcv.object_composition(img=img, contours=object_contours, hierarchy=object_hierarchy,
                                                    device=0, debug=None)
    # Assert that the objects have been combined
    contour_shape = np.shape(contours)
    assert contour_shape[1] == 1


def test_plantcv_otsu_threshold():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INTPUT_GREENMAG), -1)
    # Test with object set to light
    _ = pcv.otsu_auto_threshold(img=img, maxValue=255, object_type="light", device=0, debug=None)
    # Test with debug = "print"
    _ = pcv.otsu_auto_threshold(img=img, maxValue=255, object_type='dark', device=0, debug="print")
    os.rename("1_otsu_auto_threshold_125.0_inv.png", os.path.join(TEST_TMPDIR, "1_otsu_auto_threshold_125.0_inv.png"))
    # Test with debug = "plot"
    _ = pcv.otsu_auto_threshold(img=img, maxValue=255, object_type='dark', device=0, debug="plot")
    # Test with debug = None
    device, threshold_otsu = pcv.otsu_auto_threshold(img=img, maxValue=255, object_type='dark', device=0, debug=None)
    assert np.max(threshold_otsu) == 255


def test_plantcv_output_mask():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_BINARY), -1)
    # Test with debug = "print"
    _ = pcv.output_mask(device=0, img=img, mask=mask, filename='test.png', outdir=TEST_TMPDIR, mask_only=False,
                        debug="print")
    os.rename("1_mask-img.png", os.path.join(TEST_TMPDIR, "1_mask-img.png"))
    os.rename("1_ori-img.png", os.path.join(TEST_TMPDIR, "1_ori-img.png"))
    # Test with debug = "plot"
    _ = pcv.output_mask(device=0, img=img, mask=mask, filename='test.png', outdir=TEST_TMPDIR, mask_only=False,
                        debug="plot")
    # Test with debug = None
    device, imgpath, maskpath, analysis_images = pcv.output_mask(device=0, img=img, mask=mask, filename='test.png',
                                                                 outdir=TEST_TMPDIR, mask_only=False, debug=None)
    assert all([os.path.exists(imgpath) is True, os.path.exists(maskpath) is True])


def test_plantcv_plot_hist():
    # Test in 16-bit image mode
    img16bit = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_NIR_MASK), -1)
    _ = pcv.plot_hist(img=img16bit, name=os.path.join(TEST_TMPDIR, "hist_nir_uint16"))
    # Test in 8-bit image mode
    img8bit = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR), -1)
    bins, hist = pcv.plot_hist(img=img8bit, name=os.path.join(TEST_TMPDIR, "hist_rgb_uint8"))
    assert len(hist) == 256


def test_plantcv_print_image():
    img, path, img_name = pcv.readimage(filename=os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    filename = os.path.join(TEST_TMPDIR, 'plantcv_print_image.jpg')
    pcv.print_image(img=img, filename=filename)
    # Assert that the file was created
    assert os.path.exists(filename) is True


def test_plantcv_print_image_bad_type():
    with pytest.raises(RuntimeError):
        pcv.print_image(img=[], filename="/dev/null")


def test_plantcv_print_results():
    header = ['field1', 'field2', 'field3']
    data = ['value1', 'value2', 'value3']
    pcv.print_results(filename='not_used', header=header, data=data)


def test_plantcv_readimage():
    # Test with debug = "print"
    _ = pcv.readimage(filename=os.path.join(TEST_DATA, TEST_INPUT_COLOR), debug="print")
    os.rename("input_image.png", os.path.join(TEST_TMPDIR, "input_image.png"))
    # Test with debug = "plot"
    _ = pcv.readimage(filename=os.path.join(TEST_DATA, TEST_INPUT_COLOR), debug="plot")
    img, path, img_name = pcv.readimage(filename=os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Assert that the image name returned equals the name of the input image
    # Assert that the path of the image returned equals the path of the input image
    # Assert that the dimensions of the returned image equals the expected dimensions
    if img_name == TEST_INPUT_COLOR and path == TEST_DATA:
        if all([i == j] for i, j in zip(np.shape(img), TEST_COLOR_DIM)):
            assert 1
        else:
            assert 0
    else:
        assert 0


def test_plantcv_readimage_bad_file():
    with pytest.raises(RuntimeError):
        _ = pcv.readimage(filename=TEST_INPUT_COLOR)


def test_plantcv_rectangle_mask():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.rectangle_mask(img=img, p1=(0, 0), p2=(2454, 2056), device=0, debug="print", color="white")
    os.rename("1_roi.png", os.path.join(TEST_TMPDIR, "1_roi.png"))
    # Test with debug = "plot"
    _ = pcv.rectangle_mask(img=img, p1=(0, 0), p2=(2454, 2056), device=0, debug="plot", color="gray")
    # Test with debug = None
    device, masked, hist, contour, heir = pcv.rectangle_mask(img=img, p1=(0, 0), p2=(2454, 2056), device=0, debug=None,
                                                             color="black")
    maskedsum = np.sum(masked)
    imgsum = np.sum(img)
    assert maskedsum < imgsum


def test_plantcv_report_size_marker():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_MARKER), -1)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_MARKER)
    _ = pcv.report_size_marker_area(img=img, shape='rectangle', device=0, debug="print", marker='detect', x_adj=3500,
                                    y_adj=600, w_adj=-100, h_adj=-1500, base='white', objcolor='light',
                                    thresh_channel='s', thresh=120, filename=outfile)
    for filename in ["1_marker_roi.png", "2_hsv_saturation.png", "3_binary_threshold120.png", "4_id_objects.png",
                     "5_roi.png", "6_obj_on_img.png", "6_roi_mask.png", "6_roi_objects.png", "7_marker_shape.png",
                     "7_objcomp.png", "7_objcomp_mask.png"]:
        os.rename(filename, os.path.join(TEST_TMPDIR, filename))

    # Test with debug = "plot"
    _ = pcv.report_size_marker_area(img=img, shape='rectangle', device=0, debug="plot", marker='detect', x_adj=3500,
                                    y_adj=600, w_adj=-100, h_adj=-1500, base='white', objcolor='light',
                                    thresh_channel='s', thresh=120, filename=False)
    # Test with debug = None
    device, marker_header, marker_data, images = pcv.report_size_marker_area(img=img, shape='rectangle', device=0,
                                                                             debug=None, marker='detect', x_adj=3500,
                                                                             y_adj=600, w_adj=-100, h_adj=-1500,
                                                                             base='white', objcolor='light',
                                                                             thresh_channel='s', thresh=120,
                                                                             filename=False)
    assert marker_data[1] > 100


def test_plantcv_resize():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.resize(img=img, resize_x=0.5, resize_y=0.5, device=0, debug="print")
    os.rename("1_resize1.png", os.path.join(TEST_TMPDIR, "1_resize1.png"))
    # Test with debug = "plot"
    _ = pcv.resize(img=img, resize_x=0.5, resize_y=0.5, device=0, debug="plot")
    # Test with debug = None
    device, resized_img = pcv.resize(img=img, resize_x=0.5, resize_y=0.5, device=0, debug=None)
    ix, iy, iz = np.shape(img)
    rx, ry, rz = np.shape(resized_img)
    assert ix > rx


def test_plantcv_rgb2gray_hsv():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.rgb2gray_hsv(img=img, channel="s", device=0, debug="print")
    os.rename("1_hsv_saturation.png", os.path.join(TEST_TMPDIR, "1_hsv_saturation.png"))
    # Test with debug = "plot"
    _ = pcv.rgb2gray_hsv(img=img, channel="s", device=0, debug="plot")
    # Test with debug = None
    device, s = pcv.rgb2gray_hsv(img=img, channel="s", device=0, debug=None)
    # Assert that the output image has the dimensions of the input image but is only a single channel
    assert all([i == j] for i, j in zip(np.shape(s), TEST_GRAY_DIM))


def test_plantcv_rgb2gray_lab():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.rgb2gray_lab(img=img, channel='b', device=0, debug="print")
    os.rename("1_lab_blue-yellow.png", os.path.join(TEST_TMPDIR, "1_lab_blue-yellow.png"))
    # Test with debug = "plot"
    _ = pcv.rgb2gray_lab(img=img, channel='b', device=0, debug="plot")
    # Test with debug = None
    device, b = pcv.rgb2gray_lab(img=img, channel='b', device=0, debug=None)
    # Assert that the output image has the dimensions of the input image but is only a single channel
    assert all([i == j] for i, j in zip(np.shape(b), TEST_GRAY_DIM))


def test_plantcv_rgb2gray():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.rgb2gray(img=img, device=0, debug="print")
    os.rename("1_gray.png", os.path.join(TEST_TMPDIR, "1_gray.png"))
    # Test with debug = "plot"
    _ = pcv.rgb2gray(img=img, device=0, debug="plot")
    # Test with debug = None
    device, gray = pcv.rgb2gray(img=img, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image but is only a single channel
    assert all([i == j] for i, j in zip(np.shape(gray), TEST_GRAY_DIM))


def test_plantcv_roi_objects():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    roi_npz = np.load(os.path.join(TEST_DATA, TEST_INPUT_ROI))
    roi_contour = roi_npz['arr_0']
    roi_hierarchy = roi_npz['arr_1']
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_INPUT_CONTOURS))
    object_contours = contours_npz['arr_0']
    object_hierarchy = contours_npz['arr_1']
    # Test with debug = "print"
    _ = pcv.roi_objects(img=img, roi_type="partial", roi_contour=roi_contour, roi_hierarchy=roi_hierarchy,
                        object_contour=object_contours, obj_hierarchy=object_hierarchy, device=0, debug="print")
    os.rename("1_obj_on_img.png", os.path.join(TEST_TMPDIR, "1_obj_on_img.png"))
    os.rename("1_roi_mask.png", os.path.join(TEST_TMPDIR, "1_roi_mask.png"))
    os.rename("1_roi_objects.png", os.path.join(TEST_TMPDIR, "1_roi_objects.png"))
    # Test with debug = "plot"
    _ = pcv.roi_objects(img=img, roi_type="partial", roi_contour=roi_contour, roi_hierarchy=roi_hierarchy,
                        object_contour=object_contours, obj_hierarchy=object_hierarchy, device=0, debug="plot")
    # Test with debug = None and roi_type = cutto
    _ = pcv.roi_objects(img=img, roi_type="cutto", roi_contour=roi_contour, roi_hierarchy=roi_hierarchy,
                        object_contour=object_contours, obj_hierarchy=object_hierarchy, device=0, debug=None)
    # Test with debug = None
    device, kept_contours, kept_hierarchy, mask, area = pcv.roi_objects(img=img, roi_type="partial",
                                                                        roi_contour=roi_contour,
                                                                        roi_hierarchy=roi_hierarchy,
                                                                        object_contour=object_contours,
                                                                        obj_hierarchy=object_hierarchy,
                                                                        device=0, debug=None)
    # Assert that the contours were filtered as expected
    assert len(kept_contours) == 1046


def test_plantcv_rotate_img():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.rotate_img(img=img, rotation_deg=45, device=0, debug="print")
    os.rename("1_rotated_img.png", os.path.join(TEST_TMPDIR, "1_rotated_img.png"))
    # Test with debug = "plot"
    _ = pcv.rotate_img(img=img, rotation_deg=45, device=0, debug="plot")
    # Test with debug = None
    device, rotated = pcv.rotate_img(img=img, rotation_deg=45, device=0, debug=None)
    imgavg = np.average(img)
    rotateavg = np.average(rotated)
    assert rotateavg != imgavg


def test_plantcv_rotate_img_gray():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "plot"
    _ = pcv.rotate_img(img=img, rotation_deg=45, device=0, debug="plot")
    # Test with debug = None
    device, rotated = pcv.rotate_img(img=img, rotation_deg=45, device=0, debug=None)
    imgavg = np.average(img)
    rotateavg = np.average(rotated)
    assert rotateavg != imgavg


def test_plantcv_scale_features():
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "print"
    _ = pcv.scale_features(obj=obj_contour, mask=mask, points=TEST_ACUTE_RESULT, boundary_line=50, device=0,
                           debug="print")
    os.rename("1_feature_scaled.png", os.path.join(TEST_TMPDIR, "1_feature_scaled.png"))
    # Test with debug = "plot"
    _ = pcv.scale_features(obj=obj_contour, mask=mask, points=TEST_ACUTE_RESULT, boundary_line=50, device=0,
                           debug="plot")
    # Test with debug = None
    device, points_rescaled, centroid_rescaled, bottomline_rescaled = pcv.scale_features(obj=obj_contour, mask=mask,
                                                                                         points=TEST_ACUTE_RESULT,
                                                                                         boundary_line=50, device=0,
                                                                                         debug=None)
    assert len(points_rescaled) == 23


def test_plantcv_scale_features_bad_input():
    mask = np.array([])
    obj_contour = np.array([])
    result = pcv.scale_features(obj=obj_contour, mask=mask, points=TEST_ACUTE_RESULT, boundary_line=50, device=0,
                                debug=None)
    assert all([i == j] for i, j in zip(result, [0, ("NA", "NA"), ("NA", "NA"), ("NA", "NA")]))


def test_plantcv_scharr_filter():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.scharr_filter(img=img, dX=1, dY=0, scale=1, device=0, debug="print")
    os.rename("1_sr_img_dx1_dy0_scale1.png", os.path.join(TEST_TMPDIR, "1_sr_img_dx1_dy0_scale1.png"))
    # Test with debug = "plot"
    _ = pcv.scharr_filter(img=img, dX=1, dY=0, scale=1, device=0, debug="plot")
    # Test with debug = None
    device, scharr_img = pcv.scharr_filter(img=img, dX=1, dY=0, scale=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    assert all([i == j] for i, j in zip(np.shape(scharr_img), TEST_GRAY_DIM))


def test_plantcv_shift_img():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_COLOR))
    # Test with debug = "print"
    _ = pcv.shift_img(img=img, device=0, number=300, side="top", debug="print")
    os.rename("1_shifted_img.png", os.path.join(TEST_TMPDIR, "1_shifted_img.png"))
    # Test with debug = "plot"
    _ = pcv.shift_img(img=img, device=0, number=300, side="top", debug="plot")
    # Test with debug = None
    device, rotated = pcv.shift_img(img=img, device=0, number=300, side="top", debug=None)
    imgavg = np.average(img)
    shiftavg = np.average(rotated)
    assert shiftavg != imgavg


def test_plantcv_sobel_filter():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.sobel_filter(img=img, dx=1, dy=0, k=1, device=0, debug="print")
    os.rename("1_sb_img_dx1_dy0_k1.png", os.path.join(TEST_TMPDIR, "1_sb_img_dx1_dy0_k1.png"))
    # Test with debug = "plot"
    _ = pcv.sobel_filter(img=img, dx=1, dy=0, k=1, device=0, debug="plot")
    # Test with debug = None
    device, sobel_img = pcv.sobel_filter(img=img, dx=1, dy=0, k=1, device=0, debug=None)
    # Assert that the output image has the dimensions of the input image
    assert all([i == j] for i, j in zip(np.shape(sobel_img), TEST_GRAY_DIM))


def test_plantcv_triangle_threshold():
    img1 = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_GRAY), -1)
    # Test with debug = "print"
    _ = pcv.triangle_auto_threshold(device=0, img=img1, maxvalue=255, object_type="light", xstep=10, debug="print")
    os.rename("1_triangle_thresh_hist_30.0.png", os.path.join(TEST_TMPDIR, "1_triangle_thresh_hist_30.0.png"))
    os.rename("1_triangle_thresh_img_30.0.png", os.path.join(TEST_TMPDIR, "1_triangle_thresh_img_30.0.png"))
    # Test with debug = "plot"
    _ = pcv.triangle_auto_threshold(device=0, img=img1, maxvalue=255, object_type="light", xstep=10, debug="plot")
    # Test with debug = None
    device, thresholded = pcv.triangle_auto_threshold(device=0, img=img1, maxvalue=255, object_type="light", xstep=10,
                                                      debug=None)
    thresholdedavg = np.average(thresholded)
    imgavg = np.average(img1)
    assert thresholdedavg > imgavg


def test_plantcv_watershed_segmentation():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_CROPPED))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_CROPPED_MASK), -1)
    # Test with debug = "print"
    outfile = os.path.join(TEST_TMPDIR, TEST_INPUT_CROPPED)
    _ = pcv.watershed_segmentation(device=0, img=img, mask=mask, distance=10, filename=outfile, debug="print")
    os.rename("1_watershed_dist_img.png", os.path.join(TEST_TMPDIR, "1_watershed_dist_img.png"))
    os.rename("1_watershed_img.png", os.path.join(TEST_TMPDIR, "1_watershed_img.png"))
    # Test with debug = "plot"
    _ = pcv.watershed_segmentation(device=0, img=img, mask=mask, distance=10, filename=False, debug="plot")
    # Test with debug = None
    device, watershed_header, watershed_data, images = pcv.watershed_segmentation(device=0, img=img, mask=mask,
                                                                                  distance=10, filename=False,
                                                                                  debug=None)
    assert watershed_data[1] > 9


def test_plantcv_white_balance_gray_16bit():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_NIR_MASK), -1)
    # Test with debug = "print"
    _ = pcv.white_balance(device=0, img=img, debug="print", roi=(5, 5, 80, 80))
    os.rename("1_whitebalance_roi.png", os.path.join(TEST_TMPDIR, "1_whitebalance_roi.png"))
    os.rename("1_whitebalance.png", os.path.join(TEST_TMPDIR, "1_whitebalance.png"))
    # Test with debug = "plot"
    _ = pcv.white_balance(device=0, img=img, debug="plot", roi=(5, 5, 80, 80))
    # Test without an ROI
    _ = pcv.white_balance(device=0, img=img, debug=None, roi=None)
    # Test with debug = None
    device, white_balanced = pcv.white_balance(device=0, img=img, debug=None, roi=(5, 5, 80, 80))
    imgavg = np.average(img)
    balancedavg = np.average(white_balanced)
    assert balancedavg != imgavg


def test_plantcv_white_balance_gray_8bit():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_NIR_MASK))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Test with debug = "print"
    _ = pcv.white_balance(device=0, img=img, debug="print", roi=(5, 5, 80, 80))
    os.rename("1_whitebalance_roi.png", os.path.join(TEST_TMPDIR, "1_whitebalance_roi.png"))
    os.rename("1_whitebalance.png", os.path.join(TEST_TMPDIR, "1_whitebalance.png"))
    # Test with debug = "plot"
    _ = pcv.white_balance(device=0, img=img, debug="plot", roi=(5, 5, 80, 80))
    # Test without an ROI
    _ = pcv.white_balance(device=0, img=img, debug=None, roi=None)
    # Test with debug = None
    device, white_balanced = pcv.white_balance(device=0, img=img, debug=None, roi=(5, 5, 80, 80))
    imgavg = np.average(img)
    balancedavg = np.average(white_balanced)
    assert balancedavg != imgavg


def test_plantcv_white_balance_rgb():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_INPUT_MARKER))
    # Test with debug = "print"
    _ = pcv.white_balance(device=0, img=img, debug="print", roi=(5, 5, 80, 80))
    os.rename("1_whitebalance_roi.png", os.path.join(TEST_TMPDIR, "1_whitebalance_roi.png"))
    os.rename("1_whitebalance.png", os.path.join(TEST_TMPDIR, "1_whitebalance.png"))
    # Test with debug = "plot"
    _ = pcv.white_balance(device=0, img=img, debug="plot", roi=(5, 5, 80, 80))
    # Test without an ROI
    _ = pcv.white_balance(device=0, img=img, debug=None, roi=None)
    # Test with debug = None
    device, white_balanced = pcv.white_balance(device=0, img=img, debug=None, roi=(5, 5, 80, 80))
    imgavg = np.average(img)
    balancedavg = np.average(white_balanced)
    assert balancedavg != imgavg


def test_plantcv_x_axis_pseudolandmarks():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "plot"
    _ = pcv.x_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0, debug="plot")
    # Test with debug = None
    device, top, bottom, center_v = pcv.x_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0,
                                                               debug=None)
    assert all([all([i == j] for i, j in zip(np.shape(top), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(bottom), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(center_v), (20, 1, 2)))])


def test_plantcv_x_axis_pseudolandmarks_small_obj():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL_PLANT))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL_PLANT), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR_SMALL_PLANT))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "plot"
    device, top, bottom, center_v = pcv.x_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0,
                                                               debug="plot")
    assert all([all([i == j] for i, j in zip(np.shape(top), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(bottom), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(center_v), (20, 1, 2)))])


def test_plantcv_x_axis_pseudolandmarks_bad_input():
    img = np.array([])
    mask = np.array([])
    obj_contour = np.array([])
    result = pcv.x_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0, debug=None)
    assert all([i == j] for i, j in zip(result, [0, ("NA", "NA"), ("NA", "NA"), ("NA", "NA")]))


def test_plantcv_y_axis_pseudolandmarks():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "plot"
    _ = pcv.y_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0, debug="plot")
    # Test with debug = None
    device, left, right, center_h = pcv.x_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0,
                                                               debug=None)
    assert all([all([i == j] for i, j in zip(np.shape(left), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(right), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(center_h), (20, 1, 2)))])


def test_plantcv_y_axis_pseudolandmarks_small_obj():
    img = cv2.imread(os.path.join(TEST_DATA, TEST_VIS_SMALL_PLANT))
    mask = cv2.imread(os.path.join(TEST_DATA, TEST_MASK_SMALL_PLANT), -1)
    contours_npz = np.load(os.path.join(TEST_DATA, TEST_VIS_COMP_CONTOUR_SMALL_PLANT))
    obj_contour = contours_npz['arr_0']
    # Test with debug = "plot"
    device, left, right, center_h = pcv.y_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0,
                                                               debug="plot")
    assert all([all([i == j] for i, j in zip(np.shape(left), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(right), (20, 1, 2))),
               all([i == j] for i, j in zip(np.shape(center_h), (20, 1, 2)))])


def test_plantcv_y_axis_pseudolandmarks_bad_input():
    img = np.array([])
    mask = np.array([])
    obj_contour = np.array([])
    result = pcv.y_axis_pseudolandmarks(obj=obj_contour, mask=mask, img=img, device=0, debug=None)
    assert all([i == j] for i, j in zip(result, [0, ("NA", "NA"), ("NA", "NA"), ("NA", "NA")]))


def test_plantcv_background_subtraction():
    # List to hold result of all tests.
    truths = []
    fg_img = cv2.imread(os.path.join(TEST_DATA, TEST_FOREGROUND))
    bg_img = cv2.imread(os.path.join(TEST_DATA, TEST_BACKGROUND))
    # Testing if background subtraction is actually still working.
    # This should return an array whose sum is greater than one
    device, fgmask = pcv.background_subtraction(background_image=bg_img, foreground_image=fg_img, device=0, debug=None)
    truths.append(np.sum(fgmask) > 0)
    # The same foreground subtracted from itself should be 0
    device, fgmask = pcv.background_subtraction(background_image=fg_img, foreground_image=fg_img, device=0, debug=None)
    truths.append(np.sum(fgmask) == 0)
    # The same background subtracted from itself should be 0
    device, fgmask = pcv.background_subtraction(background_image=bg_img, foreground_image=bg_img, device=0, debug=None)
    truths.append(np.sum(fgmask) == 0)
    # All of these should be true for the function to pass testing.
    assert (all(truths))


def test_plantcv_background_subtraction_debug():
    # List to hold result of all tests.
    truths = []
    fg_img = cv2.imread(os.path.join(TEST_DATA, TEST_FOREGROUND))
    bg_img = cv2.imread(os.path.join(TEST_DATA, TEST_BACKGROUND))
    # Test with debug = "print"
    device, fgmask = pcv.background_subtraction(background_image=bg_img, foreground_image=fg_img, device=0,
                                                debug="print")
    truths.append(np.sum(fgmask) > 0)
    os.rename("1_background_subtraction.png", os.path.join(TEST_TMPDIR, "1_background_subtraction.png"))
    # Test with debug = "plot"
    device, fgmask = pcv.background_subtraction(background_image=bg_img, foreground_image=fg_img, device=0,
                                                debug="plot")
    truths.append(np.sum(fgmask) > 0)
    # All of these should be true for the function to pass testing.
    assert (all(truths))


def test_plantcv_background_subtraction_bad_img_type():
    fg_color = cv2.imread(os.path.join(TEST_DATA, TEST_FOREGROUND))
    bg_gray = cv2.imread(os.path.join(TEST_DATA, TEST_BACKGROUND), 0)
    with pytest.raises(RuntimeError):
        _ = pcv.background_subtraction(background_image=bg_gray, foreground_image=fg_color, device=0, debug=None)


def test_plantcv_background_subtraction_different_sizes():
    fg_img = cv2.imread(os.path.join(TEST_DATA, TEST_FOREGROUND))
    bg_img = cv2.imread(os.path.join(TEST_DATA, TEST_BACKGROUND))
    bg_shp = np.shape(bg_img)
    bg_img_resized = cv2.resize(bg_img, (bg_shp[0] / 2, bg_shp[1] / 2), interpolation=cv2.INTER_AREA)
    device, fgmask = pcv.background_subtraction(background_image=bg_img_resized, foreground_image=fg_img, device=0,
                                                debug=None)
    assert np.sum(fgmask > 0)

# ##############################
# Tests for the learn subpackage
# ##############################


def test_plantcv_learn_naive_bayes():
    # Make image and mask directories in the cache directory
    imgdir = os.path.join(TEST_TMPDIR, "images")
    maskdir = os.path.join(TEST_TMPDIR, "masks")
    if not os.path.exists(imgdir):
        os.mkdir(imgdir)
    if not os.path.exists(maskdir):
        os.mkdir(maskdir)
    # Copy and image and mask to the image/mask directories
    shutil.copyfile(os.path.join(TEST_DATA, TEST_VIS_SMALL), os.path.join(imgdir, "image.png"))
    shutil.copyfile(os.path.join(TEST_DATA, TEST_MASK_SMALL), os.path.join(maskdir, "image.png"))
    # Run the naive Bayes training module
    outfile = os.path.join(TEST_TMPDIR, "naive_bayes_pdfs.txt")
    plantcv.learn.naive_bayes(imgdir=imgdir, maskdir=maskdir, outfile=outfile, mkplots=True)
    assert os.path.exists(outfile)


def test_plantcv_learn_naive_bayes_multiclass():
    # Run the naive Bayes multiclass training module
    outfile = os.path.join(TEST_TMPDIR, "naive_bayes_multiclass_pdfs.txt")
    plantcv.learn.naive_bayes_multiclass(samples_file=os.path.join(TEST_DATA, TEST_SAMPLED_RGB_POINTS), outfile=outfile,
                                         mkplots=True)
    assert os.path.exists(outfile)
# Plot image to screen
import cv2


def plot_image(img, cmap=None):
    """Plot an image to the screen.

    :param cmap: str
    :param img: numpy array
    :return:
    """
    from matplotlib import pyplot as plt
    
    if cmap == None:
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.show()
    else:
        plt.imshow(img, cmap=cmap)
        plt.show()
# Print Numerical Data


def print_results(filename, header, data):
    """Print result table

    Inputs:
    filename = filename
    header   = result data table headers
    data     = result data table values

    :param filename: str
    :param header: list
    :param data: list
    :return:
    """
    print('\t'.join(map(str, header)))
    print('\t'.join(map(str, data)))
#!/usr/bin/env python
from __future__ import division
import sys, traceback
import os
import re
import sqlite3 as sq
import distutils.core
import cv2
import numpy as np
import argparse
import string
from datetime import datetime

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Sitching and Ordering Image Slices")
  parser.add_argument("-d", "--database", help="Database to query.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory for image files.", required=True)
  #parser.add_argument("-D", "--debug", help="Turn on debug, prints intermediate images.", action="store_true")
  args = parser.parse_args()
  return args

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

def color_export(sqlitedb,outdir,signal_type='vis', camera='SV',channels='rgb',average_angles='on'):
  #sqlitedb = sqlite database to query (path to db)
  #outdir = path to outdirectory
  #signal_type='VIS','NIR' or 'FLU'
  #camera = either 'SV', or 'TV'
  #channels = signal,'rgb', 'lab','hsv','nir', or 'flu'
  #average_angles = if on side angles for a plant are averaged
  #spacer = either 'on' or 'off', adds a white line between day breaks
  #makefig = either 'yes' or 'no', adds labels to days and a title
  #cat_treat = either 'yes','no',or 'all', if yes concatenates figures by treatment, if all all slice plots are put together, this only works properly if plots are roughly similar in size
  
  # Makes folder in specified directory for the slice figures and images
  i=datetime.now()
  timenow=i.strftime('%m-%d-%Y_%H:%M:%S')
  newfolder="color_data_"+(str(timenow))
   
  os.mkdir((str(outdir)+newfolder))
  outdir_name=str(outdir)+str(newfolder)+"/"
  
  # Connect to sqlite database
  connect=sq.connect(sqlitedb)
  connect.row_factory = dict_factory
  connect.text_factory=str
  c = connect.cursor()
  h = connect.cursor()
  m = connect.cursor()
  
  # Find first day of experiment, this is needed to calculate the days in integer values instead of epoch time
  for date in c.execute('select min(timestamp) as first from metadata'):
    firstday=date['first']
  # Query database to get plant ids
  signal=c.execute('select * from metadata inner join signal on metadata.image_id =signal.image_id where camera=? and imgtype=? order by plantbarcode asc',(camera,signal_type,))
  
  barcode_array=[]
  group_id=[]
  just_id=[]
  
  # find unique ids so that angles can be averaged
  
  for i, group in enumerate(signal):
    bins=int(group['bin-number'])
    plant_id=group['plantbarcode']
    barcode_array.append(plant_id,)
  barcode_unique=np.unique(barcode_array)
  
  # slices can be made from color histogram data or from fluor/nir signal histogram data stored in the database
  if channels=='rgb':
    channel1='blue'
    channel2='green'
    channel3='red'
  elif channels=='lab':
    channel1='lightness'
    channel2='green-magenta'
    channel3='blue-yellow'
  elif channels=='hsv':
    channel1='hue'
    channel2='saturation'
    channel3='value'
  elif channels=='nir':
    channel1='nir'
    channel2='nir'
    channel3='nir'
  else:
    channel1='flu'
    channel2='flu'
    channel3='flu'
  
  # Make first lines of empty arrays the header titles
  
  ch1_headers=[]
  ch2_headers=[]
  ch3_headers=[]
  bin_nums=np.transpose((np.arange(0,bins, step=1)))
  ch1_headers.append('barcode')
  ch2_headers.append('barcode')
  ch3_headers.append('barcode')
  ch1_headers.append('frame')
  ch2_headers.append('frame')
  ch3_headers.append('frame')
  ch1_headers.append('date_time')
  ch2_headers.append('date_time')
  ch3_headers.append('date_time')
         
  if signal_type=='VIS':
    for i,bn in enumerate(bin_nums):
      b=(str(channel1)+"_bin_"+str(bn))
      g=(str(channel2)+"_bin_"+str(bn))
      r=(str(channel3)+"_bin_"+str(bn))
      ch1_headers.append(b)
      ch2_headers.append(g)
      ch3_headers.append(r)
  elif signal_type=='NIR':
    for i,bn in enumerate(bin_nums):
      b="nir_bin_"+str(bn)
      ch1_headers.append(b)
  elif signal_type=='FLU':
    for i,bn in enumerate(bin_nums):
      b="fluor_bin_"+str(bn)
      ch1_headers.append(b)
  
  # Initialize the txt files which are compatible with input into R
  
  if signal_type=='VIS':
    header1_fin=','.join(map(str,ch1_headers))
    header2_fin=','.join(map(str,ch2_headers))
    header3_fin=','.join(map(str,ch3_headers))
    filename1=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel1)+"_"+str(timenow)+".txt"
    filename2=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel2)+"_"+str(timenow)+".txt"
    filename3=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel3)+"_"+str(timenow)+".txt"
    signal_file1= os.open(filename1,os.O_RDWR|os.O_CREAT)
    signal_file2= os.open(filename2,os.O_RDWR|os.O_CREAT)
    signal_file3= os.open(filename3,os.O_RDWR|os.O_CREAT)
    os.write(signal_file1, header1_fin)
    os.write(signal_file2, header2_fin)
    os.write(signal_file3, header3_fin)
    os.write(signal_file1, os.linesep)
    os.write(signal_file2, os.linesep)
    os.write(signal_file3, os.linesep)
  else:
    header1_fin=','.join(map(str,ch1_headers))
    filename1=str(outdir)+newfolder+"/"+str(signal_type)+"_signal_"+str(channel1)+"_"+str(timenow)+".txt"
    signal_file1= os.open(filename1,os.O_RDWR|os.O_CREAT)
    os.write(signal_file1, header1_fin)
    os.write(signal_file1, os.linesep)
  
  # For each plant id find the unique timestamp, this will be used to group snapshot angles
  
  for barcode_label in barcode_unique: 
    time_array=[]
    database=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id where plantbarcode= ? and imgtype=? and camera=? order by timestamp asc', (barcode_label,signal_type,camera,))
    
    for i,t in enumerate(database):
      date=(t['timestamp'])
      time_array.append(date,)
    unique_time=np.unique(time_array)
  
  # For each unique time grab the histogram data and either use each individual angle or averaged angles
    
    for time in unique_time:
      dim1_all=[]
      database_time1=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id inner join features on metadata.image_id=features.image_id where plantbarcode=? and imgtype=? and camera=? and timestamp=? and channel_name=?',(barcode_label, signal_type,camera,str(time),channel1,))
      for i, data in enumerate(database_time1):
        dim1=np.matrix(data['values'])
        norm_area=float(data['area'])
        dim1_norm1=[]
                
        dim1_norm1=(dim1/norm_area)*100 
        dim1_all.append(dim1_norm1)
                
        if average_angles=='off':
          ch1=[]
          frame=data['frame']
          date_time=data['timestamp']
          
          ch1.append(barcode_label)
          ch1.append(frame)
          ch1.append(date_time)
          
          dim1_t=np.transpose(dim1_norm1)
          
          for i,c in enumerate(dim1_t):
            b=float(c)
            ch1.append(b)
                  
          ch1_join=','.join(map(str,ch1))
          os.write(signal_file1, ch1_join)
          os.write(signal_file1, os.linesep)
      if average_angles=='on':
        ch1=[]
        frame='all_avg'
        date_time=data['timestamp']
        
        ix,iy,iz=np.shape(dim1_all)
        if ix==4:
          ch1_avg=np.transpose(np.average(dim1_all,axis=0))
                           
          ch1.append(barcode_label)
          ch1.append(frame)
          ch1.append(date_time)
        
          for i,c in enumerate(ch1_avg):
            b=float(c)
            ch1.append(b)
            
          ch1_join=','.join(map(str,ch1))
          os.write(signal_file1, ch1_join)
          os.write(signal_file1, os.linesep)
          
  for barcode_label in barcode_unique: 
    time_array=[]
    database=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id where plantbarcode= ? and imgtype=? and camera=? order by timestamp asc', (barcode_label,signal_type,camera,))
    
    for i,t in enumerate(database):
      date=(t['timestamp'])
      time_array.append(date,)
    unique_time=np.unique(time_array)
  
  # For each unique time grab the histogram data and either use each individual angle or averaged angles
    for time in unique_time:
      dim2_all=[]
      database_time2=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id inner join features on metadata.image_id=features.image_id where plantbarcode=? and imgtype=? and camera=? and timestamp=? and channel_name=?',(barcode_label, signal_type,camera,str(time),channel2,))
      for i, data in enumerate(database_time2):
        dim2=np.matrix(data['values'])
        norm_area=float(data['area'])
        
        dim2_norm1=[]
        
        dim2_norm1=(dim2/norm_area)*100 
        dim2_all.append(dim2_norm1)
                          
        if average_angles=='off':
          ch2=[]
          frame=data['frame']
          date_time=data['timestamp']
          
          ch2.append(barcode_label)
          ch2.append(frame)
          ch2.append(date_time)
          
          dim2_t=np.transpose(dim2_norm1)
          
          for i,c in enumerate(dim2_t):
            b=float(c)
            ch2.append(b)
                  
          ch2_join=','.join(map(str,ch2))
          os.write(signal_file2, ch2_join)
          os.write(signal_file2, os.linesep)
      if average_angles=='on':
        ch2=[]
        frame='all_avg'
        date_time=data['timestamp']
        
        ix,iy,iz=np.shape(dim2_all)
        
        if ix==4:
          ch2_avg=np.transpose(np.average(dim2_all,axis=0))
                           
          ch2.append(barcode_label)
          ch2.append(frame)
          ch2.append(date_time)
        
          for i,c in enumerate(ch2_avg):
            b=float(c)
            ch2.append(b)
            
          ch2_join=','.join(map(str,ch2))
          os.write(signal_file2, ch2_join)
          os.write(signal_file2, os.linesep)
  
  for barcode_label in barcode_unique: 
    time_array=[]
    database=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id where plantbarcode= ? and imgtype=? and camera=? order by timestamp asc', (barcode_label,signal_type,camera,))
    
    for i,t in enumerate(database):
      date=(t['timestamp'])
      time_array.append(date,)
    unique_time=np.unique(time_array)
    
    for time in unique_time:
      dim3_all=[]
      database_time3=h.execute('select * from metadata inner join signal on metadata.image_id=signal.image_id inner join features on metadata.image_id=features.image_id where plantbarcode=? and imgtype=? and camera=? and timestamp=? and channel_name=?',(barcode_label, signal_type,camera,str(time),channel3,))
      for i, data in enumerate(database_time3):
        dim3=np.matrix(data['values'])
        norm_area=float(data['area'])        
              
        dim3_norm1=[]
        
        dim3_norm1=(dim3/norm_area)*100 
        dim3_all.append(dim3_norm1)
          
        if average_angles=='off':
          ch3=[]
          frame=data['frame']
          date_time=data['timestamp']
          
          ch3.append(barcode_label)
          ch3.append(frame)
          ch3.append(date_time)
          
          dim3_t=np.transpose(dim3_norm1)
          
          for i,c in enumerate(dim3_t):
            b=float(c)
            ch3.append(b)
                  
          ch3_join=','.join(map(str,ch3))
          os.write(signal_file3, ch3_join)
          os.write(signal_file3, os.linesep)
      if average_angles=='on':
        ch3=[]
        frame='all_avg'
        date_time=data['timestamp']
        
        ix,iy,iz=np.shape(dim3_all)
        
        if ix==2:
          ch3_avg=np.transpose(np.average(dim3_all,axis=0))
                           
          ch3.append(barcode_label)
          ch3.append(frame)
          ch3.append(date_time)
        
          for i,c in enumerate(ch3_avg):
            b=float(c)
            ch3.append(b)
            
          ch3_join=','.join(map(str,ch3))
          os.write(signal_file3, ch3_join)
          os.write(signal_file3, os.linesep)        

  os.close(signal_file1)
  os.close(signal_file2)
  os.close(signal_file3)     
  
  return outdir_name

### Main pipeline
def main():
  # Get options
  args = options()

  img_file_dir =color_export(args.database,args.outdir,'VIS','SV','hsv','on')


if __name__ == '__main__':
  main()
        #!/usr/bin/env python

import argparse
import numpy as np
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get images from an SQLite database and some input information")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-f", "--file", help="text file, tab seperated, containing plant IDs and other information.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=False)
  parser.add_argument("--vis", help="Images are class VIS.", action='store_true')
  parser.add_argument("--nir", help="Images are class NIR.", action='store_true')
  parser.add_argument("--flu", help="Images are class FLU.", action='store_true')
  parser.add_argument("-t", "--type", help="Image format type.", required=True)
  args = parser.parse_args()
  return args


### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d
  
  
### Get images with more information
def dict_plant_info(infile):  
  table=np.genfromtxt(infile, dtype='str', delimiter='\t')
  table1=np.asarray(table)
  
  query=[]
  
  header=table[0].tolist()
  tablenohead=table[1:]
  y,x=tablenohead.shape
  columncount=list(range(0,x))
  split_table=np.vsplit(tablenohead,y)
  split_table= [l[0] for l in split_table]
  
  for row in split_table:
    where=[]
    col=np.hsplit(row,x)
    col=[l[0] for l in col]
    for i,h in enumerate(columncount):
      where.append(str(header[i])+'='+"'"+str(col[i]+"'"))
    where_and=' and '.join(map(str,where))
    query.append(where_and)
  return query

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Database image lookup method
def db_lookup(database, outdir, query, type, vis=False, nir=False, flu=False):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  
  for row in query:
    print row
    
    query1='select * from snapshots where ' + str(row)
    print query1
    for row in (db.execute(query1)):
      dt = datetime.datetime.fromtimestamp(row['datetime']).strftime('%Y-%m-%d %H:%M:%S')
      if (vis):
        if (row['camera'] == 'vis_sv' or row['camera'] == 'vis_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
          #print(args.outdir + '/' + row['plant_id'])
      if (nir):
        if (row['camera'] == 'nir_sv' or row['camera'] == 'nir_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
      if (flu):
        if (row['camera'] == 'flu_tv'):
          images = row['image_path'].split(',')
          for i in enumerate(images):
            img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + str(i) + '_' + dt + '.' + type
            copy(images[i], outdir)

### Main pipeline
def main():
  # Get options
  args = options()
  
  query=dict_plant_info(args.file)
  db_lookup(args.database, args.outdir, query, args.type, args.vis, args.nir, args.flu)
  

if __name__ == '__main__':
  main()# Script to identify corners/acute angles of an object

import cv2
import numpy as np
import math
from . import print_image
from . import plot_image


def acute_vertex(obj, win, thresh, sep, img, device, debug=None):
    """acute_vertex: identify corners/acute angles of an object

    For each point in contour, get a point before (pre) and after (post) the point of interest,
    calculate the angle between the pre and post point.

    Inputs:
    obj    = a contour of the plant object (this should be output from the object_composition.py fxn)
    win    = win argument specifies the pre and post point distances (a value of 30 worked well for a sample image)
    thresh = an threshold to set for acuteness; keep points with an angle more acute than the threshold (a value of 15
             worked well for sample image)
    sep    = the number of contour points to search within for the most acute value
    img    = the original image
    device = a counter variable
    debug  = True/False. If True, print image

    :param obj: ndarray
    :param win: int
    :param thresh: int
    :param sep: int
    :param img: ndarray
    :param device: int
    :param debug: str
    :return device: int
    :return acute: ndarray
    """
    device += 1
    chain = []
    if not np.any(obj):
        acute = ('NA', 'NA')
        return device, acute
    for i in range(len(obj) - win):
        x, y = obj[i].ravel()
        pre_x, pre_y = obj[i - win].ravel()
        post_x, post_y = obj[i + win].ravel()
        # print "The iterator i is currently " + str(i)
        # print "Here are the values: " + str(x) + " " + str(y)
        # print "Here are the pre values: " + str(pre_x) + " " + str(pre_y)
        # print "Here are the post values: " + str(post_x) + " " + str(post_y)
        # Angle in radians derived from Law of Cosines, converted to degrees
        P12 = np.sqrt((x-pre_x)*(x-pre_x)+(y-pre_y)*(y-pre_y))
        P13 = np.sqrt((x-post_x)*(x-post_x)+(y-post_y)*(y-post_y))
        P23 = np.sqrt((pre_x-post_x)*(pre_x-post_x)+(pre_y-post_y)*(pre_y-post_y))
        if (2*P12*P13) > 0.001:
            dot = (P12*P12 + P13*P13 - P23*P23)/(2*P12*P13)
        if (2*P12*P13) < 0.001:
            dot = (P12*P12 + P13*P13 - P23*P23)/0.001
        if dot > 1:                            # If float excedes 1 prevent arcos error and force to equal 1
            dot = 1
        elif dot < -1:                     # If float excedes -1 prevent arcos error and force to equal -1
            dot = -1            
        ang = math.degrees(math.acos(dot))
        # print "Here is the angle: " + str(ang)
        chain.append(ang)
        
    # Select points in contour that have an angle more acute than thresh
    index = []
    for c in range(len(chain)):         
        if float(chain[c]) <= thresh:
            index.append(c)
    # There oftentimes several points around tips with acute angles
    # Here we try to pick the most acute angle given a set of contiguous point
    # Sep is the number of points to evaluate the number of verticies
    out = []
    tester = []
    for i in range(len(index)-1):
        # print str(index[i])
        if index[i+1] - index[i] < sep:
            tester.append(index[i])
        if index[i+1] - index[i] >= sep:
            tester.append(index[i])
            # print(tester)
            angles = ([chain[d] for d in tester])
            keeper = angles.index(min(angles))
            t = tester[keeper]
            # print str(t)
            out.append(t)
            tester = []
        
    # Store the points in the variable acute
    flag = 0
    acute = obj[[out]]
    # If no points found as acute get the largest point
    if len(acute) == 0:
        acute = max(obj, key=cv2.contourArea)
        flag = 1
    # img2 = np.copy(img)
    # cv2.circle(img2,(int(cmx),int(cmy)),30,(0,215,255),-1)
    # cv2.circle(img2,(int(cmx),int(bly)),30,(255,0,0),-1)
    # Plot each of these tip points on the image
    # for i in acute:
    #        x,y = i.ravel()
    #        cv2.circle(img2,(x,y),15,(153,0,153),-1)
    # cv2.imwrite('tip_points_centroid_and_base.png', img2)
    if debug == 'print':
        # Lets make a plot of these values on the
        img2 = np.copy(img)
        # Plot each of these tip points on the image
        for i in acute:
            x, y = i.ravel()
            cv2.circle(img2, (x, y), 15, (255, 204, 255), -1)
        print_image(img2, (str(device) + '_acute_vertices.png'))
    elif debug == 'plot':
        # Lets make a plot of these values on the
        img2 = np.copy(img)
        # Plot each of these tip points on the image
        for i in acute:
            x, y = i.ravel()
            # cv2.circle(img2,(x,y),15,(255,204,255),-1)
            cv2.circle(img2, (x, y), 15, (0, 0, 255), -1)
        plot_image(img2)
    # If flag was true (no points found as acute) reformat output appropriate type
    if flag == 1:
        acute = np.asarray(acute)
        acute = acute.reshape(1, 1, 2)
    return device, acute
# End of function
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv
import math
import shutil
import numpy as np
from numpy import genfromtxt

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-f", "--folder", help="txt file of folder names", required=True)
  parser.add_argument("-i", "--imgfolder", help="imgfolder name", required=True)
  parser.add_argument("-o", "--outdir", help="new folder destination", required=True)
  args = parser.parse_args()
  return args


def main():
  
  args = options()
  path=os.getcwd()
  #folders=open(args.folder, 'r')
  #for l in folders:
  #  fname=l.replace("\n", "")
  #  source=str(path)+"/"+str(args.imgfolder)+"/"+str(fname)
  #  destination=str(path)+"/"+str(args.outdir)
  #  shutil.move(source, destination)
  
  snapshots=str(path)+"/"+str(args.imgfolder)+"/SnapshotInfo.csv"
  #snapshot_data = genfromtxt(snapshots, delimiter=',')
  regex="^.*B*.*$"
  select=np.fromregex(snapshots,regex)
  print snapshot_data
  
if __name__ == '__main__':
  main()
import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import color_palette


def cluster_contours(device, img, roi_objects, nrow=1, ncol=1, debug=None):

    """
    This function take a image with multiple contours and clusters them based on user input of rows and columns

    Inputs:
    img                     = An RGB image array
    roi_objects             = object contours in an image that are needed to be clustered.
    nrow                    = number of rows to cluster (this should be the approximate  number of desired rows
                              in the entire image (even if there isn't a literal row of plants)
    ncol                    = number of columns to cluster (this should be the approximate number of desired columns
                              in the entire image (even if there isn't a literal row of plants)
    file                    = output of filename from read_image function
    filenames               = input txt file with list of filenames in order from top to bottom left to right
    debug                   = print debugging images

    Returns:
    device                  = pipeline step counter
    grouped_contour_indexes = contours grouped
    contours                = All inputed contours

    :param device: int
    :param img: ndarray
    :param roi_objects: list
    :param nrow: int
    :param ncol: int
    :param debug: str
    :return device: int
    :return grouped_contour_indexes: list
    :return contours: list
    """

    device += 1

    if len(np.shape(img)) == 3:
        iy, ix, iz = np.shape(img)
    else:
        iy, ix, = np.shape(img)

    # get the break groups

    if nrow == 1:
        rbreaks = [0, iy]
    else:
        rstep = np.rint(iy / nrow)
        rstep1 = np.int(rstep)
        rbreaks = range(0, iy, rstep1)
    if ncol == 1:
        cbreaks = [0, ix]
    else:
        cstep = np.rint(ix / ncol)
        cstep1 = np.int(cstep)
        cbreaks = range(0, ix, cstep1)

    # categorize what bin the center of mass of each contour

    def digitize(a, step):
        if isinstance(step, int) == True:
            i = step
        else:
            i = len(step)
        for x in range(0, i):
            if x == 0:
                if a >= 0 and a < step[x + 1]:
                    return x + 1
            elif a >= step[x - 1] and a < step[x]:
                return x
            elif a > step[x - 1] and a > np.max(step):
                return i

    dtype = [('cx', int), ('cy', int), ('rowbin', int), ('colbin', int), ('index', int)]
    coord = []
    for i in range(0, len(roi_objects)):
        m = cv2.moments(roi_objects[i])
        if m['m00'] == 0:
            pass
        else:
            cx = int(m['m10'] / m['m00'])
            cy = int(m['m01'] / m['m00'])
            # colbin = np.digitize(cx, cbreaks)
            # rowbin = np.digitize(cy, rbreaks)
            colbin = digitize(cx, cbreaks)
            rowbin = digitize(cy, rbreaks)
            a = (cx, cy, colbin, rowbin, i)
            coord.append(a)
    coord1 = np.array(coord, dtype=dtype)
    coord2 = np.sort(coord1, order=('colbin', 'rowbin'))

    # get the list of unique coordinates and group the contours with the same bin coordinates

    groups = []
    for i, y in enumerate(coord2):
        col = y[3]
        row = y[2]
        location = str(row) + ',' + str(col)
        groups.append(location)

    unigroup = np.unique(groups)
    coordgroups = []

    for i, y in enumerate(unigroup):
        col = int(y[0])
        row = int(y[2])
        for a, b in enumerate(coord2):
            if b[2] == col and b[3] == row:
                grp = i
                contour = b[4]
                coordgroups.append((grp, contour))
            else:
                pass

    coordlist = [[y[1] for y in coordgroups if y[0] == x] for x in range(0, (len(unigroup)))]

    contours = roi_objects
    grouped_contour_indexes = coordlist

    # Debug image is rainbow printed contours

    if debug == 'print':
        if len(np.shape(img)) == 3:
            img_copy = np.copy(img)
        else:
            iy, ix = np.shape(img)
            img_copy = np.zeros((iy, ix, 3), dtype=np.uint8)

        rand_color = color_palette(len(coordlist))
        for i, x in enumerate(coordlist):
            for a in x:
                cv2.drawContours(img_copy, roi_objects, a, rand_color[i], -1, lineType=8)
        print_image(img_copy, (str(device) + '_clusters.png'))

    elif debug == 'plot':
        if len(np.shape(img)) == 3:
            img_copy = np.copy(img)
        else:
            iy, ix = np.shape(img)
            img_copy = np.zeros((iy, ix, 3), dtype=np.uint8)

        rand_color = color_palette(len(coordlist))
        for i, x in enumerate(coordlist):
            for a in x:
                cv2.drawContours(img_copy, roi_objects, a, rand_color[i], -1, lineType=8)
        plot_image(img_copy)

    return device, grouped_contour_indexes, contours
# Image addition

from . import print_image
from . import plot_image


def image_add(img1, img2, device, debug=None):
    """This is a function used to add images. The numpy addition function '+' is used. This is a modulo operation
       rather than the cv2.add fxn which is a saturation operation. ddepth = -1 specifies that the dimensions of output
       image will be the same as the input image.

    Inputs:
    img1      = input image
    img2      = second input image
    device    = device number. Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    added_img = summed images

    :param img1: numpy array
    :param img2: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return added_img: numpy array
    """

    added_img = img1 + img2
    device += 1
    if debug == 'print':
        print_image(added_img, str(device) + '_added' + '.png')
    elif debug == 'plot':
        plot_image(added_img, cmap='gray')
    return device, added_img
#!/usr/bin/python
import argparse
import sys, os
import sqlite3 as sq
import math

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
  parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
  #parser.add_argument("-p", "--height", help="Height of images in pixels", required=True, type=int)
  parser.add_argument("-D", "--debug", help="Turn on debugging mode", action="store_true")
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Main pipeline
def main():
  # Get options
  args = options()
  
  # Does the database exist?
  if not os.path.exists(args.database):
    raise("The database file " + str(args.database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(args.database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Open output file
  try:
    out=open(args.outfile, 'w')
  except IOError:
    print("IO error")
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str
  
  # Database handler
  db = connect.cursor()
  
  # Get database schema
  #for row in (db.execute("SELECT * FROM `sqlite_master` WHERE type='table' AND name='metadata'")):
  #  print(row)
  
  # Header
  #out.write(','.join(map(str, ('plant_id', 'datetime', 'sv_zoom', 'tv_zoom', 'sv0_area', 'sv90_area', 'sv180_area', 'sv270_area', 'tv_area', 'solidity', 'perimeter', 'centroid_x', 'centroid_y',
  out.write(','.join(map(str, ('plant_id', 'datetime', 'sv_zoom', 'tv_zoom', 'sv0_area', 'sv90_area', 'sv180_area','sv270_area','tv_area', 'solidity', 'perimeter', 'centroid_x', 'centroid_y',
	                                 'longest_axis', 'extent_x', 'extent_y', 'height_above_bound', 'height_below_bound',
                                   'percent_above_bound_area', 'percent_below_bound_area', 'outlier', 'boundary_line'))) + '\n')
  
  # Retrieve snapshot IDs from the database
  snapshots = []
  for row in (db.execute('SELECT DISTINCT(`timestamp`) FROM `metadata` WHERE `imgtype` = "VIS"')):
    snapshots.append(row['timestamp'])
  if (args.debug):
    print('Found ' + str(len(snapshots)) + ' snapshots')
  # Retrieve snapshots and process data
  for snapshot in snapshots:
    sv_image_count = 0
    outlier = False
    plant_id = ''
    tv_area = 0
    sv0_area = 0
    sv90_area = 0
    sv180_area = 0
    sv270_area = 0
    solidity = 0
    perimeter = 0
    centroid_x = 0
    centroid_y = 0
    longest_axis = 0
    height_above_bound = 0
    height_below_bound = 0
    #above_bound_area = 0
    percent_above_bound_area = 0
    #below_bound_area = 0
    percent_below_bound_area = 0
    extent_x = 0
    extent_y = 0
    sv_zoom = 0
    tv_zoom = 0
    boundary_line_y = 0
    
    for row in (db.execute('SELECT * FROM `metadata` NATURAL JOIN `features` WHERE `timestamp` = "%s"' % snapshot)):
      plant_id = row['plantbarcode']
      if row['in_bounds'] == 'False':
        outlier = True
      if row['imgtype'] == 'VIS' and row['camera'] == 'SV':
        sv_zoom = row['zoom']
        sv_image_count += 1
        boundary_line_y = row['y-position']
        if row['frame'] =='0':
          sv0_area = float(row['area'])
        elif row['frame'] =='90':
          sv90_area = float(row['area'])
        elif row['frame'] =='180':
          sv180_area = float(row['area'])
        elif row['frame'] =='270':
          sv270_area = float(row['area'])
        solidity += float(row['solidity'])
        perimeter += float(row['perimeter'])
        centroid_x += float(row['center-of-mass-x'])
        centroid_y += float(row['center-of-mass-y'])
        longest_axis += int(row['longest_axis'])
        extent_x += int(row['width'])
        extent_y += int(row['height'])
        height_above_bound += int(row['height_above_bound'])
        height_below_bound += int(row['height_below_bound'])
        percent_above_bound_area += float(row['percent_above_bound_area'])
        percent_below_bound_area += float(row['percent_below_bound_area'])
      elif row['imgtype'] == 'VIS' and row['camera'] == 'TV':
        tv_zoom = row['zoom']
        tv_area = int(float(row['area']))
    if sv_image_count == 4 and tv_area > 0:
      if (args.debug):
        print('Snapshot ' + str(snapshot) + ' has 5 images')
      solidity /= sv_image_count
      perimeter /= sv_image_count
      centroid_x /= sv_image_count
      centroid_y /= sv_image_count
      longest_axis /= sv_image_count
      extent_x /= sv_image_count
      extent_y /= sv_image_count
      height_above_bound /= sv_image_count
      height_below_bound /= sv_image_count
      percent_above_bound_area /= sv_image_count
      percent_below_bound_area /= sv_image_count
      #surface_area += tv_centroid_correction(tv_area, centroid_height)
      out.write(','.join(map(str, (plant_id, snapshot, sv_zoom, tv_zoom, sv0_area, sv90_area, sv180_area, sv270_area, tv_area, solidity, perimeter, centroid_x, centroid_y, longest_axis,
                           extent_x, extent_y, height_above_bound, height_below_bound, percent_above_bound_area, percent_below_bound_area, outlier, boundary_line_y))) + '\n')
    else:
      if (args.debug):
        print('Something is wrong, snapshot ' + str(snapshot) + ' has ' + str(sv_image_count) + ' SV images and TV area is ' + str(tv_area))

if __name__ == '__main__':
  main()
# Find NIR image

import os
import re
import numpy as np


def get_nir(path, filename, device, debug=None):
    """Find a corresponding NIR image from the same snapshot as the VIS image.

    Inputs:
    path     = path to vis image
    filename = vis image file name
    device   = pipeline step counter
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    nirpath  = NIR image filename and path

    :param path: str
    :param filename: str
    :param device: int
    :param debug: str
    :return device: int
    :return nirpath: str
    """

    device += 1
    visname = filename.split("_")
    allfiles = np.array(os.listdir(path))
    nirfiles = []

    targetimg = []
    cam = visname[1]

    if cam == "SV":
        angle = visname[2]

    for n in allfiles:
        if re.search("NIR", n) != None:
            nirfiles.append(n)

    if cam == "TV":
        for n in nirfiles:
            if re.search("TV", n) != None:
                nirpath = os.path.join(str(path), str(n))

    if cam == "SV":
        for n in nirfiles:
            if re.search("SV", n) != None:
                nsplit = n.split("_")
                exangle = '\\b' + str(angle) + '\\b'
                if re.search(exangle, nsplit[2]) != None:
                    nirpath = os.path.join(str(path), str(n))

    return device, nirpath
# Classify pixels as plant or non-plant using the naive Bayes method written by Arash Abbasi,
# adapted for Python by Noah Fahlgren

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def naive_bayes_classifier(img, pdf_file, device, debug=None):
    """Use the Naive Bayes classifier to output a plant binary mask.

    Inputs:
    img      = image object (NumPy ndarray), BGR colorspace
    pdf_file = filename of file containing PDFs output from the Naive Bayes training method (see plantcv-train.py)
    device   = device number. Used to count steps in the pipeline
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    mask     = binary mask (ndarray)

    :param img: ndarray
    :param pdf_file: str
    :param device: int
    :param debug: str
    :return device: int
    :return masks: dict
    """
    device += 1

    # Initialize PDF dictionary
    pdfs = {}
    # Read the PDF file
    pf = open(pdf_file, "r")
    # Read the first line (header)
    pf.readline()
    # Read each line of the file and parse the PDFs, store in the PDF dictionary
    for row in pf:
        # Remove newline character
        row = row.rstrip("\n")
        # Split the row into columns on tab characters
        cols = row.split("\t")
        # Make sure there are the correct number of columns (i.e. is this a valid PDF file?)
        if len(cols) != 258:
            fatal_error("Naive Bayes PDF file is not formatted correctly. Error on line:\n" + row)
        # Store the PDFs. Column 0 is the class, Column 1 is the color channel, the rest are p at
        # intensity values 0-255. Cast text p values as float
        class_name = cols[0]
        channel = cols[1]
        if class_name not in pdfs:
            pdfs[class_name] = {}
        pdfs[class_name][channel] = [float(i) for i in cols[2:]]

    # Split the input BGR image into component channels for BGR, HSV, and LAB colorspaces
    # b, g, r = cv2.split(img)
    h, s, v = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))
    # l, gm, by = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2LAB))

    # Calculate the dimensions of the input image
    width, height, depth = np.shape(img)

    # Initialize an empty ndarray for plant and background. These will be used to store the joint probabilities
    px_p = {}
    for class_name in pdfs.keys():
        px_p[class_name] = np.zeros([width, height])

    # Loop over the image coordinates (each i, j pixel)
    for i in range(0, width):
        for j in range(0, height):
            for class_name in pdfs.keys():
                # Calculate the joint probability that this is in the class
                px_p[class_name][i][j] = pdfs[class_name]["hue"][h[i][j]] * pdfs[class_name]["saturation"][s[i][j]] * \
                                         pdfs[class_name]["value"][v[i][j]]

    # Initialize empty masks
    masks = {}
    for class_name in pdfs.keys():
        masks[class_name] = np.zeros([width, height], dtype=np.uint8)
    # Set pixel intensities to 255 (white) for the mask where the class has the highest probability
    for class_name in masks:
        background_classes = []
        for name in masks:
            if class_name is not name:
                background_classes.append(px_p[name])
        background_class = np.maximum.reduce(background_classes)
        masks[class_name][np.where(px_p[class_name] > background_class)] = 255
    # mask[np.where(plant > bg)] = 255

    # Print or plot the mask if debug is not None
    if debug == "print":
        for class_name, mask in masks.items():
            print_image(mask, (str(device) + "_naive_bayes_" + class_name + "_mask.jpg"))
    elif debug == "plot":
        for class_name, mask in masks.items():
            plot_image(mask, cmap="gray")

    return device, masks
#!/usr/bin/env python

import argparse
import os
import sqlite3 as sq
import pandas as pd
import numpy as np
import time

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract VIS object shape data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
  parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
  parser.add_argument("-f", "--filter", help="process type, 'filter', or 'raw'", default='raw')
  parser.add_argument("-i", "--imgtype", help="Type of image either 'VIS', 'NIR', or 'BOTH', or 'NONE' ", default='BOTH')
  parser.add_argument("-a", "--angles", help="Total number of angles (TV and SV)", default=5)
  parser.add_argument("-s", "--signal", help="if true outputs signal data as well", default=False)
  parser.add_argument("-n", "--signalnorm", help="if true normalizes signal data to area of object", default=False)
  parser.add_argument("-v", "--signalavg", help="if true also output averaged sv and seperated tv files", default=False)
  parser.add_argument("-D", "--debug", help="Turn on debugging mode", action="store_true")
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

def main():

  t0=time.time()
  print("starting time...")

  # Get options
  args = options()

  # Does the database exist?
  if not os.path.exists(args.database):
    raise ("The database file " + str(args.database) + " does not exist");

  # Open a connection
  try:
    connect=sq.connect(args.database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])

######################################################################################
  db = sq.connect(args.database)

  # Get the headers for metadata and headers for features these will be the headers of features table
  metadata = pd.read_sql_query('SELECT * FROM metadata', db)
  features = pd.read_sql_query('SELECT * FROM features', db)
  mheader = []
  fheader = []
  totalangles = int(args.angles)

  for colname in metadata:
    mheader.append(colname)
  for colname in features:
    fheader.append(colname)

  # Get the headers for signal table
  cmetadata= pd.read_sql_query('SELECT * FROM metadata', db)
  cfeatures=pd.read_sql_query('SELECT features.image_id, features.area FROM features',db)
  csignal=pd.read_sql_query('SELECT * from signal',db)

  signalheader=[]

  for colname in cmetadata:
    signalheader.append(colname)
  for colname in cfeatures:
    signalheader.append(colname)
  for colname in csignal:
    signalheader.append(colname)

######################################################################################

# Get the unfiltered signal data

  if args.signal:
    if args.imgtype=='BOTH':
      signal1 = pd.read_sql_query(
       'SELECT metadata.*, features.image_id, features.area, signal.* FROM metadata INNER JOIN features ON metadata.image_id=features.image_id INNER JOIN signal ON metadata.image_id=signal.image_id WHERE imgtype="VIS"', db)
      signal2 = pd.read_sql_query(
      'SELECT metadata.*, features.image_id, features.area, signal.* FROM metadata INNER JOIN features ON metadata.image_id=features.image_id INNER JOIN signal ON metadata.image_id=signal.image_id WHERE imgtype="NIR"',db)
      sx1,sy1=signal1.shape
      sx2,sy2=signal2.shape
    else:
      signal1 = pd.read_sql_query(
        'SELECT metadata.*, features.image_id, features.area, signal.* FROM metadata INNER JOIN features ON metadata.image_id=features.image_id INNER JOIN signal ON metadata.image_id=signal.image_id',db)
      sx1,sy1=signal1.shape
      sx2=None

######################################################################################

# Get the unfiltered data
  if args.imgtype == 'BOTH':
    table1 = pd.read_sql_query(
      'SELECT * FROM metadata INNER JOIN features ON metadata.image_id=features.image_id WHERE imgtype="VIS"', db)
    table2 = pd.read_sql_query(
      'SELECT * FROM metadata INNER JOIN features ON metadata.image_id=features.image_id WHERE imgtype="NIR"', db)
    ix1, iy1 = table1.shape
    ix2, iy2 = table2.shape

  # Join metadata and features
  elif args.imgtype == 'NONE':
    table = pd.read_sql_query('SELECT * FROM metadata INNER JOIN features ON metadata.image_id=features.image_id', db)
    ix, iy = table.shape

  else:
    # Join metadata and features, and select imgtype
    table = pd.read_sql_query(
      'SELECT * FROM metadata INNER JOIN features ON metadata.image_id=features.image_id WHERE imgtype=?', db,
      params=(str(args.imgtype),))
    ix, iy = table.shape

  ######################################################################################

# Filter data by number of angles

  if args.filter=='filter':

    vissnapshots = []
    nirsnapshots = []
    snapshots= []

    finalvis = []
    finalnir = []
    finalsnapshots=[]

    removedvis = []
    removednir = []
    removedsnapshots=[]

    if args.imgtype == 'BOTH':

      # get all the unique timestamps for VIS
      time1 = pd.read_sql_query('SELECT DISTINCT(`timestamp`) FROM `metadata` WHERE `imgtype` = "VIS"', db)
      vissnapshots.append(time1['timestamp'])

      # get all the unique timestamps for NIR
      time2 = pd.read_sql_query('SELECT DISTINCT(`timestamp`) FROM `metadata` WHERE `imgtype` = "NIR"', db)
      nirsnapshots.append(time2['timestamp'])

      # get the timestamps with all angles
      for snap in vissnapshots[0]:
        #print("checking angles for "+str(snap))
        visrow = pd.read_sql_query('SELECT * from metadata INNER JOIN features on metadata.image_id=features.image_id '
                                   'WHERE imgtype="VIS" and timestamp=?', db, params=(str(snap),))
        vangles = (len(visrow))

        if totalangles == int(vangles):
          fintimestamp = snap
          finalvis.append(fintimestamp)
        else:
          fintimestamp = snap
          removedvis.append(fintimestamp)

      print("removing "+str(len(removedvis))+" timestamps from vis measurements")
      for remove in removedvis:
        table1 = table1[table1['timestamp'] != remove]

      if args.signal:
        print("removing "+str(len(removedvis))+" timestamps from signal")
        for remove in removedvis:
          signal1 = signal1[signal1['timestamp']!= remove]

      # get the timestamps with all angles
      for snap in nirsnapshots[0]:
        print("checking angles for "+str(snap))
        nirrow = pd.read_sql_query('SELECT * from metadata INNER JOIN features on metadata.image_id=features.image_id '
                                   'WHERE imgtype="NIR" and timestamp=?', db, params=(str(snap),))
        nangles = (len(nirrow))

        if totalangles == int(nangles):
          fintimestamp = snap
          finalnir.append(fintimestamp)
        else:
          fintimestamp = snap
          removednir.append(fintimestamp)
          print(len(removednir))


      print("removing "+str(len(removednir))+" timestamps from nir measurements")
      for remove in removednir:
        table2 = table2[table2['timestamp'] != remove]

      if args.signal:
        print("removing " + str(len(removednir)) + " timestamps from nir signal")
        for remove in removednir:
          signal2 = signal2[signal2['timestamp'] != remove]

      if args.debug:
        print(str(len(removedvis)) + " vis timestamps removed")
        print(str(len(removednir)) + " nir timestamps removed")

    elif args.imgtype=="NONE":
      time1= pd.read_sql_query('SELECT DISTINCT(`timestamp`) FROM `metadata`', db)
      snapshots.append(time1['timestamp'])

      # get the timestamps with all angles
      for snap in snapshots[0]:
        print("checking angles for "+str(snap))
        visrow = pd.read_sql_query('SELECT * from metadata INNER JOIN features on metadata.image_id=features.image_id '
                                   'WHERE timestamp=?', db, params=(str(snap),))
        vangles = (len(visrow))

        if totalangles == int(vangles):
          fintimestamp = snap
          finalsnapshots.append(fintimestamp)
        else:
          fintimestamp = snap
          removedsnapshots.append(fintimestamp)
          print(len(removedsnapshots))

      print("removing "+str(len(removedsnapshots))+" snapshots")
      for remove in removedsnapshots:
        print(remove)
        table = table[table['timestamp'] != remove]

      if args.signal:
        print("removing "+str(len(removedsnapshots))+" timestamps from signal")
        for remove in removedsnapshots:
          signal1 = signal1[signal1['timestamp'] != remove]

      if args.debug:
        print(str(len(removedsnapshots))+" snapshots removed")

    else:

      time1= pd.read_sql_query('SELECT DISTINCT(`timestamp`) FROM `metadata` WHERE imgtype=?', db, params=(str(args.imgtype),))
      snapshots.append(time1['timestamp'])

      # get the timestamps with all angles
      for snap in snapshots[0]:
        print("checking angles for "+str(snap))
        visrow = pd.read_sql_query('SELECT * from metadata INNER JOIN features on metadata.image_id=features.image_id '
                                   'WHERE timestamp=?', db, params=(str(snap),))
        vangles = (len(visrow))

        if totalangles == int(vangles):
          fintimestamp = snap
          finalsnapshots.append(fintimestamp)
        else:
          fintimestamp = snap
          removedsnapshots.append(fintimestamp)
          print(len(removedsnapshots))

      print("removing "+str(len(removedsnapshots))+" "+str(args.imgtype)+" snapshots")
      for remove in removedsnapshots:
        table = table[table['timestamp'] != remove]

      if args.signal:
        print("removing " + str(len(removedsnapshots)) + " timestamps from signal")
        for remove in removedsnapshots:
          signal1 = signal1[signal1['timestamp'] != remove]

      if args.debug:
        print(str(len(removedsnapshots))+" snapshots removed")

######################################################################################

# No filtering if raw is selected

  elif args.filter=='raw':

    if args.imgtype=='BOTH':
      table1=table1
      table2=table2
    else:
      table=table

######################################################################################

#Write the features data out


  if args.imgtype=='BOTH':
    if ix1==0:
      pass

    if ix1!=0:
      finalfeatures = []
      # Select the features that are not completely empty
      for x in fheader:
        if x == 'in_bounds':
          finalfeatures.append(x)
          mheader.append(x)
        elif x != 'in_bounds':
          table1[str(x)] = table1[str(x)].astype(float)
          sumcol = table1[str(x)].apply(np.sum)[1]
          if sumcol != 0.0:
            finalfeatures.append(x)
            mheader.append(x)
      # Subset table with the features that are not completely empty
      table1 = table1[mheader]
      if args.filter=='filter':
        visout = str(args.outfile) + "-vis-filtered.csv"
      else:
        visout = str(args.outfile) + "-vis-notfiltered.csv"
      table1.to_csv(str(visout), mode='a')

    if ix2==0:
      pass

    if ix2!=0:
      finalfeatures = []
      # Select the features that are not completely empty
      for x in fheader:
        if x == 'in_bounds':
          finalfeatures.append(x)
          mheader.append(x)
        elif x != 'in_bounds':
          table2[str(x)] = table2[str(x)].astype(float)
          sumcol = table2[str(x)].apply(np.sum)[1]
          if sumcol != 0.0:
            finalfeatures.append(x)
            mheader.append(x)
      # Subset table with the features that are not completely empty
      table2 = table2[mheader]
      if args.filter=='filter':
       nirout = str(args.outfile) + "-nir-filtered.csv"
      else:
        nirout = str(args.outfile) + "-nir-notfiltered.csv"
      table2.to_csv(str(nirout), mode='a')

  else:
    if ix==0:
      pass
    else:
      finalfeatures = []
      # Select the features that are not completely empty
      for x in fheader:
        if x == 'in_bounds':
          finalfeatures.append(x)
          mheader.append(x)
        elif x != 'in_bounds':
          table[str(x)] = table[str(x)].astype(float)
          sumcol = table[str(x)].apply(np.sum)[1]
          if sumcol != 0.0:
            finalfeatures.append(x)
            mheader.append(x)

      # Subset table with the features that are not completely empty
      table = table[mheader]
      if args.filter=='filter':
       tablename = str(args.outfile) + "-data-filtered.csv"
      else:
        tablename = str(args.outfile) + "-data-notfiltered.csv"
      # Write table to csv file
      table.to_csv(str(tablename), mode='a')


#######################################################################################

  if args.signal:
    if sx1!=0:
      print("starting on signal data")
      # get the channel column
      channel1=signal1['channel_name'].astype('str')
      # get number of bins
      bins=signal1['bin-number'].astype(int)
      # get unique channels
      uniquechannel=np.unique(channel1)
      uniquebin=np.unique(bins)
      binheader=[]
      # get signal data and make into sep columns make new headers based on bins and channels
      for x in range(0, uniquebin):
        bin = 'bin_' + str(x)
        binheader.append(bin)
      signalsplit=signal1['values'].apply(lambda x: pd.Series(x.split(',')))
      signalsplit.columns=binheader

      #drop the values column so that you can replace it with the seperated columns
      signaldata=signal1
      signal1=signal1.drop('values', axis=1)
      image_id=signal1['image_id'].take([0],axis=1)
      signalbase=signal1.drop('image_id',axis=1)
      signalbase=signalbase.join(image_id)
      signalbase=signalbase.drop('channel_name',axis=1)
      signalbase=signalbase.drop_duplicates('image_id')

      headers=signalbase.columns.values
      signalall = signalbase

      # get just the signal data and join it with the channel column
      signalsplit=signalsplit.join(signal1['channel_name'])

      # rename the bin columns so they can be associated with the color names
      for channel in uniquechannel:
        subset=signalsplit[signalsplit['channel_name']==str(channel)]
        subset=subset.drop('channel_name',axis=1)
        names=subset.columns.values
        newnames=[n.replace('bin',str(channel)) for n in names]
        headers=np.concatenate((headers,newnames),axis=0)
        signalall.reset_index(drop=True, inplace=True)
        subset.reset_index(drop=True, inplace=True)
        signalall=pd.concat([signalall,subset],axis=1)
      signalall.columns=headers
      signalnormbase= signalall
      normsignalheaders=signalnormbase.filter(regex='\d', axis=1).columns.values
      if args.filter=='filter':
        filename =str(args.outfile) + "_signal-filtered.csv"
      else:
        filename = str(args.outfile) + "_signal-notfiltered.csv"
      signalall.to_csv(filename,mode='w')

      if args.signalnorm:
        print("normalizing signal data")
        normsignal = []
        area = signalnormbase['area'].astype(float)
        area = area.as_matrix()
        for bin in normsignalheaders:
          datacol = signalnormbase[str(bin)].astype(float)
          datacol = datacol.as_matrix()
          normcol = np.divide(datacol, area)
          normcol *= 100
          normsignal.append(normcol)
        normsignal = np.transpose(normsignal)
        normsignal = pd.DataFrame(normsignal, columns=normsignalheaders)
        signalnormbase1=signalnormbase.filter(regex='^((?!\d).)*$', axis=1)
        signalnormbase2=pd.concat([signalnormbase1,normsignal],axis=1)
        headers1=signalnormbase.columns.values
        signalnormbase2.columns=headers1

        if args.filter=='filter':
          filename =str(args.outfile) + "_signal-filtered-normalized.csv"
        else:
          filename =str(args.outfile)  + "_signal-notfiltered-normalized.csv"
        signalnormbase2.to_csv(filename, mode='w')

        if args.signalavg:
            print("averaging normalized signal data")
            avgnormsignal=signalnormbase2
            avgnormsv=avgnormsignal[avgnormsignal['camera']=='SV']
            svx,svy=np.shape(avgnormsv)
            avgnormtv=avgnormsignal[avgnormsignal['camera']=='TV']
            tvx,tvy=np.shape(avgnormtv)
            avgnormnone=avgnormsignal[avgnormsignal['camera']=='NONE']
            nonex,noney=np.shape(avgnormnone)

            if svx!=0:
                svbase=avgnormsv.filter(regex='^((?!_).)*$', axis=1)
                svbase = svbase.join(avgnormsv['image_id'])
                svbase = svbase.drop_duplicates('timestamp')
                svbase.reset_index(drop=True, inplace=True)

                avgnormsvdata = avgnormsv.filter(regex='_', axis=1)
                avgnormsvdata = avgnormsvdata.join(avgnormsv['timestamp'])
                avgnormsvdata = avgnormsvdata.groupby(['timestamp']).mean()
                avgnormsvdata.reset_index(drop=True, inplace=True)
                svbase = pd.concat([svbase, avgnormsvdata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "_signal-sv-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "_signal-sv-notfiltered-normalized-averaged.csv"
                svbase.to_csv(filename, mode='w')

            if tvx!=0:
                tvbase=avgnormtv.filter(regex='^((?!_).)*$', axis=1)
                tvbase = tvbase.join(avgnormtv['image_id'])
                tvbase = tvbase.drop_duplicates('timestamp')
                tvbase.reset_index(drop=True, inplace=True)

                avgnormtvdata = avgnormtv.filter(regex='_', axis=1)
                avgnormtvdata = avgnormtvdata.join(avgnormtv['timestamp'])
                avgnormtvdata = avgnormtvdata.groupby(['timestamp']).mean()
                avgnormtvdata.reset_index(drop=True, inplace=True)
                tvbase = pd.concat([tvbase, avgnormtvdata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "_signal-tv-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "_signal-tv-notfiltered-normalized-averaged.csv"
                tvbase.to_csv(filename, mode='w')

            if nonex!=0:
                nonebase=avgnormnone.filter(regex='^((?!_).)*$', axis=1)
                nonebase = nonebase.join(avgnormnone['image_id'])
                nonebase = nonebase.drop_duplicates('timestamp')
                nonebase.reset_index(drop=True, inplace=True)

                avgnormnonedata = avgnormnone.filter(regex='_', axis=1)
                avgnormnonedata = avgnormnonedata.join(avgnormnone['timestamp'])
                avgnormnonedata = avgnormnonedata.groupby(['timestamp']).mean()
                datacolumns=avgnormnonedata.columns.values
                avgnormnonedata.reset_index(drop=True, inplace=True)
                avgnormnonedata.columns=datacolumns
                nonebase = pd.concat([nonebase, avgnormnonedata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "_signal-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "_signal-notfiltered-normalized-averaged.csv"
                nonebase.to_csv(filename, mode='w')

      if args.signalavg:
         print("averaging signal data")
         avgbase=signaldata
         chanelname = signalsplit['channel_name']
         signalsplit1 = signalsplit.drop('channel_name', axis=1)
         signalsplit1 = signalsplit1.astype(float)
         signalsplit1=signalsplit1.join(chanelname)
         signalsplit1=signalsplit1.join(signaldata['camera'])
         signalsplit1=signalsplit1.join(signaldata['timestamp'])

         svsubset=signalsplit1[signalsplit1['camera']=='SV']
         svx,svy=np.shape(svsubset)

         svbase=avgbase[avgbase['camera']=='SV']
         svbase = svbase.drop('values', axis=1)
         image_id = svbase['image_id'].take([0], axis=1)
         svbase = svbase.drop('image_id', axis=1)
         svbase = svbase.join(image_id)
         svbase = svbase.drop('channel_name', axis=1)
         svbase = svbase.drop_duplicates('timestamp')
         svbase.reset_index(drop=True, inplace=True)

         tvsubset=signalsplit1[signalsplit1['camera']=='TV']
         tvx,tvy=np.shape(tvsubset)

         tvbase=avgbase[avgbase['camera']=='TV']
         tvbase = tvbase.drop('values', axis=1)
         image_id = tvbase['image_id'].take([0], axis=1)
         tvbase = tvbase.drop('image_id', axis=1)
         tvbase = tvbase.join(image_id)
         tvbase = tvbase.drop('channel_name', axis=1)
         tvbase = tvbase.drop_duplicates('timestamp')
         tvbase.reset_index(drop=True, inplace=True)

         nonesubset=signalsplit1[signalsplit1['camera']=='NONE']
         nonex,noney=np.shape(nonesubset)

         nonebase=avgbase[avgbase['camera']=='NONE']
         nonebase = nonebase.drop('values', axis=1)
         image_id = nonebase['image_id'].take([0], axis=1)
         nonebase = nonebase.drop('image_id', axis=1)
         nonebase = nonebase.join(image_id)
         nonebase = nonebase.drop('channel_name', axis=1)
         nonebase = nonebase.drop_duplicates('timestamp')
         nonebase.reset_index(drop=True, inplace=True)


         for channel in uniquechannel:
           if svx!=0:
              subset = svsubset[svsubset['channel_name'] == str(channel)]
              subset = subset.drop('camera', axis=1)
              subset = subset.drop('channel_name', axis=1)
              subset=subset.groupby(['timestamp']).mean()
              names = subset.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset.columns = newnames
              subset.reset_index(drop=True, inplace=True)
              svbase.reset_index(drop=True, inplace=True)
              svbase = pd.concat([svbase, subset], axis=1)

           if tvx!=0:
              subset1 = tvsubset[tvsubset['channel_name'] == str(channel)]
              subset1 = subset1.drop('camera', axis=1)
              subset1 = subset1.drop('channel_name', axis=1)
              subset1=subset1.groupby(['timestamp']).mean()
              names = subset1.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset1.columns = newnames
              subset1.reset_index(drop=True, inplace=True)
              tvbase.reset_index(drop=True, inplace=True)
              tvbase = pd.concat([tvbase, subset1], axis=1)

           if nonex!=0:
              subset2 = nonesubset[nonesubset['channel_name']== str(channel)]
              subset2 = subset2.drop('camera', axis=1)
              subset2 = subset2.drop('channel_name', axis=1)
              subset2=subset2.groupby(['timestamp']).mean()
              names = subset2.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset2.columns = newnames
              subset2.reset_index(drop=True, inplace=True)
              nonebase.reset_index(drop=True, inplace=True)
              nonebase = pd.concat([nonebase, subset2], axis=1)

         if svx != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-signal-sv-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-signal-sv-notfiltered-averaged.csv"
            svbase.to_csv(filename, mode='w')

         if tvx != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-signal-tv-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-signal-tv-notfiltered-averaged.csv"
            tvbase.to_csv(filename, mode='w')

         if nonex != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-signal-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-signal-notfiltered-averaged.csv"
            nonebase.to_csv(filename, mode='w')

    if sx2!=None:
      print("analyzing signal 2 data")
      # get the channel column
      channel1=signal2['channel_name'].astype('str')
      # get number of bins
      bins=signal2['bin-number'].astype(int)
      # get unique channels
      uniquechannel=np.unique(channel1)
      uniquebin=np.unique(bins)
      binheader=[]
      # get signal data and make into sep columns make new headers based on bins and channels
      for x in range(0, uniquebin):
        bin = 'bin_' + str(x)
        binheader.append(bin)
      signalsplit=signal2['values'].apply(lambda x: pd.Series(x.split(',')))
      signalsplit.columns=binheader

      #drop the values column so that you can replace it with the seperated columns
      signaldata=signal2
      signal2=signal2.drop('values', axis=1)
      image_id=signal2['image_id'].take([0],axis=1)
      signalbase=signal2.drop('image_id',axis=1)
      signalbase=signalbase.join(image_id)
      signalbase=signalbase.drop('channel_name',axis=1)
      signalbase=signalbase.drop_duplicates('image_id')

      headers=signalbase.columns.values
      signalall = signalbase

      # get just the signal data and join it with the channel column
      signalsplit=signalsplit.join(signal2['channel_name'])

      # rename the bin columns so they can be associated with the color names
      for channel in uniquechannel:
        subset=signalsplit[signalsplit['channel_name']==str(channel)]
        subset=subset.drop('channel_name',axis=1)
        names=subset.columns.values
        newnames=[n.replace('bin',str(channel)) for n in names]
        headers=np.concatenate((headers,newnames),axis=0)
        signalall.reset_index(drop=True, inplace=True)
        subset.reset_index(drop=True, inplace=True)
        signalall=pd.concat([signalall,subset],axis=1)
      signalall.columns=headers
      signalnormbase= signalall
      normsignalheaders=signalnormbase.filter(regex='\d', axis=1).columns.values
      if args.filter=='filter':
        filename = str(args.outfile) + "-nir-filtered.csv"
      else:
        filename = str(args.outfile) + "-nir-notfiltered.csv"
      signalall.to_csv(filename,mode='w')

      if args.signalnorm:
        print("normalizing signal2 data")
        normsignal = []
        area = signalnormbase['area'].astype(float)
        area = area.as_matrix()
        for bin in normsignalheaders:
          datacol = signalnormbase[str(bin)].astype(float)
          datacol = datacol.as_matrix()
          normcol = np.divide(datacol, area)
          normcol *= 100
          normsignal.append(normcol)
        normsignal = np.transpose(normsignal)
        normsignal = pd.DataFrame(normsignal, columns=normsignalheaders)
        signalnormbase1=signalnormbase.filter(regex='^((?!\d).)*$', axis=1)
        signalnormbase2=pd.concat([signalnormbase1,normsignal],axis=1)
        headers1=signalnormbase.columns.values
        signalnormbase2.columns=headers1

        if args.filter=='filter':
          filename = str(args.outfile) + "-nir-filtered-normalized.csv"
        else:
          filename = str(args.outfile)  + "-nir-notfiltered-normalized.csv"
        signalnormbase2.to_csv(filename, mode='w')

        if args.signalavg:
            print("averaging normalized signal 2 data")
            avgnormsignal=signalnormbase2
            avgnormsv=avgnormsignal[avgnormsignal['camera']=='SV']
            svx,svy=np.shape(avgnormsv)
            avgnormtv=avgnormsignal[avgnormsignal['camera']=='TV']
            tvx,tvy=np.shape(avgnormtv)
            avgnormnone=avgnormsignal[avgnormsignal['camera']=='NONE']
            nonex,noney=np.shape(avgnormnone)

            if svx!=0:
                svbase=avgnormsv.filter(regex='^((?!_).)*$', axis=1)
                svbase = svbase.join(avgnormsv['image_id'])
                svbase = svbase.drop_duplicates('timestamp')
                svbase.reset_index(drop=True, inplace=True)

                avgnormsvdata = avgnormsv.filter(regex='_', axis=1)
                avgnormsvdata = avgnormsvdata.join(avgnormsv['timestamp'])
                avgnormsvdata = avgnormsvdata.groupby(['timestamp']).mean()
                avgnormsvdata.reset_index(drop=True, inplace=True)
                svbase = pd.concat([svbase, avgnormsvdata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "-nir-sv-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "-nir-sv-notfiltered-normalized-averaged.csv"
                svbase.to_csv(filename, mode='w')

            if tvx!=0:
                tvbase=avgnormtv.filter(regex='^((?!_).)*$', axis=1)
                tvbase = tvbase.join(avgnormtv['image_id'])
                tvbase = tvbase.drop_duplicates('timestamp')
                tvbase.reset_index(drop=True, inplace=True)

                avgnormtvdata = avgnormtv.filter(regex='_', axis=1)
                avgnormtvdata = avgnormtvdata.join(avgnormtv['timestamp'])
                avgnormtvdata = avgnormtvdata.groupby(['timestamp']).mean()
                avgnormtvdata.reset_index(drop=True, inplace=True)
                tvbase = pd.concat([tvbase, avgnormtvdata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "-nir-tv-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "-nir-tv-notfiltered-normalized-averaged.csv"
                tvbase.to_csv(filename, mode='w')

            if nonex!=0:
                nonebase=avgnormnone.filter(regex='^((?!_).)*$', axis=1)
                nonebase = nonebase.join(avgnormnone['image_id'])
                nonebase = nonebase.drop_duplicates('timestamp')
                nonebase.reset_index(drop=True, inplace=True)

                avgnormnonedata = avgnormnone.filter(regex='_', axis=1)
                avgnormnonedata = avgnormnonedata.join(avgnormnone['timestamp'])
                avgnormnonedata = avgnormnonedata.groupby(['timestamp']).mean()
                datacolumns=avgnormnonedata.columns.values
                avgnormnonedata.reset_index(drop=True, inplace=True)
                avgnormnonedata.columns=datacolumns
                nonebase = pd.concat([nonebase, avgnormnonedata], axis=1)

                if args.filter=='filter':
                    filename = str(args.outfile) + "-nir-filtered-normalized-averaged.csv"
                else:
                    filename = str(args.outfile) + "-nir-notfiltered-normalized-averaged.csv"
                nonebase.to_csv(filename, mode='w')

      if args.signalavg:
         print("averaging signal2 data")
         avgbase=signaldata
         chanelname = signalsplit['channel_name']
         signalsplit1 = signalsplit.drop('channel_name', axis=1)
         signalsplit1 = signalsplit1.astype(float)
         signalsplit1=signalsplit1.join(chanelname)
         signalsplit1=signalsplit1.join(signaldata['camera'])
         signalsplit1=signalsplit1.join(signaldata['timestamp'])

         svsubset=signalsplit1[signalsplit1['camera']=='SV']
         svx,svy=np.shape(svsubset)

         svbase=avgbase[avgbase['camera']=='SV']
         svbase = svbase.drop('values', axis=1)
         image_id = svbase['image_id'].take([0], axis=1)
         svbase = svbase.drop('image_id', axis=1)
         svbase = svbase.join(image_id)
         svbase = svbase.drop('channel_name', axis=1)
         svbase = svbase.drop_duplicates('timestamp')
         svbase.reset_index(drop=True, inplace=True)

         tvsubset=signalsplit1[signalsplit1['camera']=='TV']
         tvx,tvy=np.shape(tvsubset)

         tvbase=avgbase[avgbase['camera']=='TV']
         tvbase = tvbase.drop('values', axis=1)
         image_id = tvbase['image_id'].take([0], axis=1)
         tvbase = tvbase.drop('image_id', axis=1)
         tvbase = tvbase.join(image_id)
         tvbase = tvbase.drop('channel_name', axis=1)
         tvbase = tvbase.drop_duplicates('timestamp')
         tvbase.reset_index(drop=True, inplace=True)

         nonesubset=signalsplit1[signalsplit1['camera']=='NONE']
         nonex,noney=np.shape(nonesubset)

         nonebase=avgbase[avgbase['camera']=='NONE']
         nonebase = nonebase.drop('values', axis=1)
         image_id = nonebase['image_id'].take([0], axis=1)
         nonebase = nonebase.drop('image_id', axis=1)
         nonebase = nonebase.join(image_id)
         nonebase = nonebase.drop('channel_name', axis=1)
         nonebase = nonebase.drop_duplicates('timestamp')
         nonebase.reset_index(drop=True, inplace=True)


         for channel in uniquechannel:
           if svx!=0:
              subset = svsubset[svsubset['channel_name'] == str(channel)]
              subset = subset.drop('camera', axis=1)
              subset = subset.drop('channel_name', axis=1)
              subset=subset.groupby(['timestamp']).mean()
              names = subset.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset.columns = newnames
              subset.reset_index(drop=True, inplace=True)
              svbase.reset_index(drop=True, inplace=True)
              svbase = pd.concat([svbase, subset], axis=1)

           if tvx!=0:
              subset1 = tvsubset[tvsubset['channel_name'] == str(channel)]
              subset1 = subset1.drop('camera', axis=1)
              subset1 = subset1.drop('channel_name', axis=1)
              subset1=subset1.groupby(['timestamp']).mean()
              names = subset1.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset1.columns = newnames
              subset1.reset_index(drop=True, inplace=True)
              tvbase.reset_index(drop=True, inplace=True)
              tvbase = pd.concat([tvbase, subset1], axis=1)

           if nonex!=0:
              subset2 = nonesubset[nonesubset['channel_name']== str(channel)]
              subset2 = subset2.drop('camera', axis=1)
              subset2 = subset2.drop('channel_name', axis=1)
              subset2=subset2.groupby(['timestamp']).mean()
              names = subset2.columns.values
              newnames = [n.replace('bin', str(channel)) for n in names]
              subset2.columns = newnames
              subset2.reset_index(drop=True, inplace=True)
              nonebase.reset_index(drop=True, inplace=True)
              nonebase = pd.concat([nonebase, subset2], axis=1)

         if svx != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-nir-sv-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-nir-sv-notfiltered-averaged.csv"
            svbase.to_csv(filename, mode='w')

         if tvx != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-nir-tv-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-nir-tv-notfiltered-averaged.csv"
            tvbase.to_csv(filename, mode='w')

         if nonex != 0:
            if args.filter=='filter':
              filename = str(args.outfile) + "-nir-filtered-averaged.csv"
            else:
              filename = str(args.outfile) + "-nir-notfiltered-averaged.csv"
            nonebase.to_csv(filename, mode='w')

#######################################################################################

  t1 = time.time()
  total = t1 - t0
  print(str(total)+" total time elapsed")

#######################################################################################

if __name__ == '__main__':
  main()
#!/usr/bin/env python

import sys, traceback
import os
import re
import sqlite3
import distutils.core
import cv2
import numpy as np
import argparse
import string
import plantcv as pcv
import visualize_plantcv_results as avr

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Sitching and Ordering Image Slices")
  parser.add_argument("-d", "--database", help="Database to query.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory for image files.", required=True)
  #parser.add_argument("-D", "--debug", help="Turn on debug, prints intermediate images.", action="store_true")
  args = parser.parse_args()
  return args

### Main pipeline
def main():
  # Get options
  args = options()

  img_file_dir =avr.d3_color_output(args.database,args.outdir,'Dp1AA','vis','vis_sv','rgb','on','off','yes','yes','all')
  #img_file_dir='/home/mgehan/LemnaTec/out_folder/slice_figs_and_images_04-21-2014_16:59:07/'
  #avr.cat_fig(args.outdir,img_file_dir)

if __name__ == '__main__':
  main()
        


    
# Image subtraction

from . import print_image
from . import plot_image


def image_subtract(img1, img2, device, debug=None):
    """This is a function used to subtract one image from another image (img1 - img2). The numpy subtraction function
       '-' is used. This is a modulo operation rather than the cv2.subtract fxn which is a saturation operation.
       ddepth = -1 specifies that the dimensions of output image will be the same as the input image.

    Inputs:
    img1      = input image
    img2      = input image used to subtract from img1
    device    = device number. Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    subed_img = subtracted image

    :param img1: numpy array
    :param img2: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return subed_img: numpy array
    """

    subed_img = img1 - img2
    device += 1
    if debug == 'print':
        print_image(subed_img, str(device) + '_subtracted' + '.png')
    elif debug == 'plot':
        plot_image(subed_img, cmap='gray')
    return device, subed_img
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract NIR signal data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
  parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Main pipeline
def main():
  # Get options
  args = options()
  
  # Does the database exist?
  if not os.path.exists(args.database):
    pcv.fatal_error("The database file " + str(args.database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(args.database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Open output file
  try:
    out = open(args.outfile, 'w')
  except IOError:
    print("IO error")
    
  # Replace the row_factory result constructor with a dictionary constructor
  #connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str
  
  # Database handler
  db = connect.cursor()
  
  # Retrieve snapshots and process data
  db.execute('SELECT * FROM `snapshots` INNER JOIN `nir_signal` ON `snapshots`.`image_id` = `nir_signal`.`image_id`')
  names = list(map(lambda x: x[0], db.description))
  bins = 0
  names[-1] = 's0'
  for row in db.fetchall():
    if bins == 0:
      bins = row[9]
      for s in range(1,bins):
        names.append('s' + str(s))
      out.write(','.join(map(str,names)) + '\n')
    out.write(','.join(map(str, row)) + '\n')
  

if __name__ == '__main__':
  main()
# Resize image

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def resize(img, resize_x, resize_y, device, debug=None):
    """Resize image.

    Inputs:
    img      = image to resize
    resize_x = scaling factor
    resize_y = scaling factor
    device   = device counter
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    reimg    = resized image

    :param img: numpy array
    :param resize_x: int
    :param resize_y: int
    :param device: int
    :param debug: str
    :return device: int
    :return reimg: numpy array
    """

    device += 1

    reimg = cv2.resize(img, (0, 0), fx=resize_x, fy=resize_y)

    if resize_x <= 0 and resize_y <= 0:
        fatal_error("Resize values both cannot be 0 or negative values!")

    if debug == 'print':
        print_image(reimg, (str(device) + "_resize1.png"))
    elif debug == 'plot':
        plot_image(reimg, cmap='gray')

    return device, reimg
#!/usr/bin/env python

import argparse
import numpy as np
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get images from an SQLite database and some input information")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-f", "--file", help="text file, tab seperated, containing plant IDs and other information.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=False)
  parser.add_argument("--vis", help="Images are class VIS.", action='store_true')
  parser.add_argument("--nir", help="Images are class NIR.", action='store_true')
  parser.add_argument("--flu", help="Images are class FLU.", action='store_true')
  parser.add_argument("-t", "--type", help="Image format type.", required=True)
  args = parser.parse_args()
  return args


### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d
  
  
### Get images with more information
def dict_plant_info(infile):  
  table=np.genfromtxt(infile, dtype='str', delimiter='\t')
  table1=np.asarray(table)
  
  query=[]
  
  header=table[0].tolist()
  tablenohead=table[1:]
  y,x=tablenohead.shape
  columncount=list(range(0,x))
  split_table=np.vsplit(tablenohead,y)
  split_table= [l[0] for l in split_table]
  
  for row in split_table:
    where=[]
    col=np.hsplit(row,x)
    col=[l[0] for l in col]
    for i,h in enumerate(columncount):
      where.append(str(header[i])+'='+"'"+str(col[i]+"'"))
    where_and=' and '.join(map(str,where))
    query.append(where_and)
  return query

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Database image lookup method
def db_lookup(database, outdir, query, type, vis=False, nir=False, flu=False):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  
  for row in query:
    print row
    
    query1='select * from snapshots where ' + str(row)
    print query1
    for row in (db.execute(query1)):
      dt = datetime.datetime.fromtimestamp(row['datetime']).strftime('%Y-%m-%d %H:%M:%S')
      if (vis):
        if (row['camera'] == 'vis_sv' or row['camera'] == 'vis_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
          #print(args.outdir + '/' + row['plant_id'])
      if (nir):
        if (row['camera'] == 'nir_sv' or row['camera'] == 'nir_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
      if (flu):
        if (row['camera'] == 'flu_tv'):
          images = row['image_path'].split(',')
          for i in enumerate(images):
            img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + str(i) + '_' + dt + '.' + type
            copy(images[i], outdir)

### Main pipeline
def main():
  # Get options
  args = options()
  
  query=dict_plant_info(args.file)
  db_lookup(args.database, args.outdir, query, args.type, args.vis, args.nir, args.flu)
  

if __name__ == '__main__':
  main()# Print image to file
import sys
import cv2
from . import fatal_error


def print_image(img, filename):
    """Save image to file.

    Inputs:
    img      = image object
    filename = name of file to save image to

    :param img: numpy array
    :param filename: string
    :return:
    """

    try:
        cv2.imwrite(filename, img)
    except:
        fatal_error("Error writing file " + filename + ": " + str(sys.exc_info()[0]))
# Read image

import os
import cv2
from . import fatal_error
from . import print_image
from . import plot_image


def readimage(filename, debug=None):
    """Read image from file.

    Inputs:
    filename = name of image file
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    img      = image object as numpy array
    path     = path to image file
    img_name = name of image file

    :param filename: str
    :param debug: str
    :return img: numpy array
    :return path: str
    :return img_name: str
    """

    img = cv2.imread(filename)

    if img is None:
        fatal_error("Failed to open " + filename)

    # Split path from filename
    path, img_name = os.path.split(filename)

    if debug == "print":
        print_image(img, "input_image.png")
    elif debug == "plot":
        plot_image(img)

    return img, path, img_name
# Object fill device

import numpy as np
import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def fill(img, mask, size, device, debug=None):
    """Identifies objects and fills objects that are less than size.

    Inputs:
    img    = image object, grayscale. img will be returned after filling
    mask   = image object, grayscale. This image will be used to identify contours
    size   = minimum object area size in pixels (integer)
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    img    = image with objects filled

    :param img: numpy array
    :param mask: numpy array
    :param size: int
    :param device: int
    :param debug: str
    :return device: int
    :return img: numpy array
    """

    device += 1
    if len(np.shape(img)) >= 3:
        fatal_error("Image is not binary")
    else:
        ix, iy = np.shape(img)
    size1 = ix, iy
    background = np.zeros(size1, dtype=np.uint8)

    # Find contours
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

    # Loop through contours, fill contours less than or equal to size in area
    for c, cnt in enumerate(contours):
        # if hierarchy[0][c][0]==-1:
        m = cv2.moments(cnt)
        area = m['m00']
        if area <= size:
            # cv2.fillPoly(img, pts = cnt, color=(0,0,0))
            cv2.drawContours(img, contours, c, (0, 0, 0), -1, lineType=8, hierarchy=hierarchy)
    if debug == 'print':
        print_image(img, (str(device) + '_fill' + str(size) + '.png'))
    elif debug == 'plot':
        plot_image(img, cmap='gray')

    return device, img
# View and Adjust ROI

import sys
import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def define_roi(img, shape, device, roi=None, roi_input='default', debug=None, adjust=False, x_adj=0, y_adj=0,
               w_adj=0, h_adj=0):
    """Define a region of interest.

    Inputs:
    img       = img to overlay roi
    roi       = default (None) or user input ROI image, object area should be white and background should be black,
                has not been optimized for more than one ROI
    roi_input = type of file roi_base is, either 'binary', 'rgb', or 'default' (no ROI inputted)
    shape     = desired shape of final roi, either 'rectangle' or 'circle', if  user inputs rectangular roi but chooses
                'circle' for shape then a circle is fitted around rectangular roi (and vice versa)
    device    = device number.  Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.
    adjust    = either 'True' or 'False', if 'True' allows user to adjust ROI
    x_adj     = adjust center along x axis
    y_adj     = adjust center along y axis
    w_adj     = adjust width
    h_adj     = adjust height

    Returns:
    device    = device number
    contour   = object contour list
    hierarchy = contour hierarchy list

    :param img: numpy array
    :param shape: str
    :param device: int
    :param roi: numpy array
    :param roi_input: str
    :param debug: str
    :param adjust: bool
    :param x_adj: int
    :param y_adj: int
    :param w_adj: int
    :param h_adj: int
    :return device: int
    :return contour: list
    :return hierarchy: list
    """

    device += 1
    ori_img = np.copy(img)
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
    else:
        ix, iy = np.shape(img)

    # Allows user to use the default ROI or input their own RGB or binary image (made with imagej or some other program)
    #  as a base ROI (that can be adjusted below)
    if roi_input == 'rgb':
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        ret, v_img = cv2.threshold(v, 0, 255, cv2.THRESH_BINARY)
        roi_contour, hierarchy = cv2.findContours(v_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    elif roi_input == 'binary':
        roi_contour, hierarchy = cv2.findContours(roi, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    elif roi_input == 'default':
        size = ix, iy
        roi_background = np.zeros(size, dtype=np.uint8)
        roi_size = (ix - 5), (iy - 5)
        roi = np.zeros(roi_size, dtype=np.uint8)
        roi1 = roi + 1
        roi_contour, roi_heirarchy = cv2.findContours(roi1, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(roi_background, roi_contour[0], -1, (255, 0, 0), 5)
        if adjust == True:
            if (x_adj > 0 and w_adj > 0) or (y_adj > 0 and h_adj > 0) or (x_adj < 0 or y_adj < 0):
                fatal_error('Adjusted ROI position is out of frame, this will cause problems in detecting objects')
    else:
        fatal_error('ROI Input' + str(roi_input) + ' is not "binary", "rgb" or "default roi"!')

    # If the ROI is exactly in the 'correct' position
    if adjust == False:
        for cnt in roi_contour:
            size = ix, iy, 3
            background = np.zeros(size, dtype=np.uint8)
            if shape == 'rectangle':
                x, y, w, h = cv2.boundingRect(cnt)
                cv2.rectangle(background, (x, y), (x + w, y + h), (0, 255, 0), 5)
                rect = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                rect_contour, hierarchy = cv2.findContours(rect, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                cv2.drawContours(ori_img, rect_contour[0], -1, (255, 0, 0), 5)
                if debug == 'print':
                    print_image(ori_img, (str(device) + '_roi.png'))
                elif debug == 'plot':
                    plot_image(ori_img)
                return device, rect_contour, hierarchy
            elif shape == 'circle':
                x, y, w, h = cv2.boundingRect(cnt)
                center = (int(w / 2), int(h / 2))
                if h > w:
                    radius = int(w / 2)
                    cv2.circle(background, center, radius, (255, 255, 255), -1)
                    circle = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                    circle_contour, hierarchy = cv2.findContours(circle, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(ori_img, circle_contour[0], -1, (255, 0, 0), 5)
                    if debug == 'print':
                        print_image(ori_img, (str(device) + '_roi.png'))
                    elif debug == 'plot':
                        plot_image(ori_img)
                    return device, circle_contour, hierarchy
                else:
                    radius = int(h / 2)
                    cv2.circle(background, center, radius, (255, 255, 255), -1)
                    circle = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                    circle_contour, hierarchy = cv2.findContours(circle, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(ori_img, circle_contour[0], -1, (255, 0, 0), 5)
                    if debug == 'print':
                        print_image(ori_img, (str(device) + '_roi.png'))
                    elif debug == 'plot':
                        plot_image(ori_img)
                    return device, circle_contour, hierarchy
            elif shape == 'ellipse':
                x, y, w, h = cv2.boundingRect(cnt)
                center = (int(w / 2), int(h / 2))
                if w > h:
                    cv2.ellipse(background, center, (w / 2, h / 2), 0, 0, 360, (0, 255, 0), 2)
                    ellipse = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                    ellipse_contour, hierarchy = cv2.findContours(ellipse, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(ori_img, ellipse_contour[0], -1, (255, 0, 0), 5)
                    if debug == 'print':
                        print_image(ori_img, (str(device) + '_roi.png'))
                    elif debug == 'plot':
                        plot_image(ori_img)
                    return device, ellipse_contour, hierarchy
                else:
                    cv2.ellipse(ori_img, center, (h / 2, w / 2), 0, 0, 360, (0, 255, 0), 2)
                    cv2.ellipse(background, center, (h / 2, w / 2), 0, 0, 360, (0, 255, 0), 2)
                    ellipse = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                    ellipse_contour, hierarchy = cv2.findContours(ellipse, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(ori_img, ellipse_contour[0], -1, (255, 0, 0), 5)
                    if debug == 'print':
                        print_image(ori_img, (str(device) + '_roi.png'))
                    elif debug == 'plot':
                        plot_image(ori_img)
                    return device, ellipse_contour, hierarchy
            else:
                fatal_error('Shape' + str(shape) + ' is not "rectangle", "circle", or "ellipse"!')

                # If the user wants to change the size of the ROI or adjust ROI position
    if adjust == True:
        if debug is not None:
            sys.stderr.write(
                'WARNING: Make sure ROI is COMPLETELY in frame or object detection will not perform properly\n')
        if x_adj == 0 and y_adj == 0 and w_adj == 0 and h_adj == 0:
            fatal_error('If adjust is true then x_adj, y_adj, w_adj or h_adj must have a non-zero value')
        else:
            for cnt in roi_contour:
                size = ix, iy, 3
                background = np.zeros(size, dtype=np.uint8)
                if shape == 'rectangle':
                    x, y, w, h = cv2.boundingRect(cnt)
                    x1 = x + x_adj
                    y1 = y + y_adj
                    w1 = w + w_adj
                    h1 = h + h_adj
                    cv2.rectangle(background, (x1, y1), (x + w1, y + h1), (0, 255, 0), 1)
                    rect = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                    rect_contour, hierarchy = cv2.findContours(rect, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(ori_img, rect_contour[0], -1, (255, 0, 0), 5)
                    if debug == 'print':
                        print_image(ori_img, (str(device) + '_roi.png'))
                    elif debug == 'plot':
                        plot_image(ori_img)
                    return device, rect_contour, hierarchy
                elif shape == 'circle':
                    x, y, w, h = cv2.boundingRect(cnt)
                    x1 = x + x_adj
                    y1 = y + y_adj
                    w1 = w + w_adj
                    h1 = h + h_adj
                    center = (int((w + x1) / 2), int((h + y1) / 2))
                    if h > w:
                        radius = int(w1 / 2)
                        cv2.circle(background, center, radius, (255, 255, 255), -1)
                        circle = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                        circle_contour, hierarchy = cv2.findContours(circle, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                        cv2.drawContours(ori_img, circle_contour[0], -1, (255, 0, 0), 5)
                        if debug == 'print':
                            print_image(ori_img, (str(device) + '_roi.png'))
                        elif debug == 'plot':
                            plot_image(ori_img)
                        return device, circle_contour, hierarchy
                    else:
                        radius = int(h1 / 2)
                        cv2.circle(background, center, radius, (255, 255, 255), -1)
                        circle = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                        circle_contour, hierarchy = cv2.findContours(circle, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                        cv2.drawContours(ori_img, circle_contour[0], -1, (255, 0, 0), 5)
                        if debug == 'print':
                            print_image(ori_img, (str(device) + '_roi.png'))
                        elif debug == 'plot':
                            plot_image(ori_img)
                        return device, circle_contour, hierarchy
                elif shape == 'ellipse':
                    x, y, w, h = cv2.boundingRect(cnt)
                    x1 = x + x_adj
                    y1 = y + y_adj
                    w1 = w + w_adj
                    h1 = h + h_adj
                    center = (int((w + x1) / 2), int((h + y1) / 2))
                    if w > h:
                        cv2.ellipse(background, center, (w1 / 2, h1 / 2), 0, 0, 360, (0, 255, 0), 2)
                        ellipse = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                        ellipse_contour, hierarchy = cv2.findContours(ellipse, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                        cv2.drawContours(ori_img, ellipse_contour[0], -1, (255, 0, 0), 5)
                        if debug == 'print':
                            print_image(ori_img, (str(device) + '_roi.png'))
                        elif debug == 'plot':
                            plot_image(ori_img)
                        return device, ellipse_contour, hierarchy
                    else:
                        cv2.ellipse(background, center, (h1 / 2, w1 / 2), 0, 0, 360, (0, 255, 0), 2)
                        ellipse = cv2.cvtColor(background, cv2.COLOR_RGB2GRAY)
                        ellipse_contour, hierarchy = cv2.findContours(ellipse, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                        cv2.drawContours(ori_img, ellipse_contour[0], -1, (255, 0, 0), 5)
                        if debug == 'print':
                            print_image(ori_img, (str(device) + '_roi.png'))
                        elif debug == 'plot':
                            plot_image(ori_img)
                        return device, ellipse_contour, hierarchy
                else:
                    fatal_error('Shape' + str(shape) + ' is not "rectangle", "circle", or "ellipse"!')
# Join images (XOR)

import cv2
from . import print_image
from . import plot_image


def logical_xor(img1, img2, device, debug=None):
    """Join two images using the bitwise XOR operator.

    Inputs:
    img1   = image object1, grayscale
    img2   = image object2, grayscale
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    merged = joined image

    :param img1: numpy array
    :param img2: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return merged: numpy array
    """

    device += 1
    merged = cv2.bitwise_xor(img1, img2)
    if debug == 'print':
        print_image(merged, (str(device) + '_xor_joined.png'))
    elif debug == 'plot':
        plot_image(merged, cmap='gray')
    return device, merged
# Binary image auto threshold

from __future__ import division, print_function
import cv2
import math
import numpy as np
from . import print_image
from . import plot_image


def _detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising', kpsh=False, valley=False, show=False, ax=None):
    """Marcos Duarte, https://github.com/demotu/BMC; version 1.0.4; license MIT

    Detect peaks in data based on their amplitude and other features.

    Parameters
    ----------
    x : 1D array_like
        data.
    mph : {None, number}, optional (default = None)
        detect peaks that are greater than minimum peak height.
    mpd : positive integer, optional (default = 1)
        detect peaks that are at least separated by minimum peak distance (in
        number of data).
    threshold : positive number, optional (default = 0)
        detect peaks (valleys) that are greater (smaller) than `threshold`
        in relation to their immediate neighbors.
    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')
        for a flat peak, keep only the rising edge ('rising'), only the
        falling edge ('falling'), both edges ('both'), or don't detect a
        flat peak (None).
    kpsh : bool, optional (default = False)
        keep peaks with same height even if they are closer than `mpd`.
    valley : bool, optional (default = False)
        if True (1), detect valleys (local minima) instead of peaks.
    show : bool, optional (default = False)
        if True (1), plot data in matplotlib figure.
    ax : a matplotlib.axes.Axes instance, optional (default = None).

    Returns
    -------
    ind : 1D array_like
        indices of the peaks in `x`.

    Notes
    -----
    The detection of valleys instead of peaks is performed internally by simply
    negating the data: `ind_valleys = detect_peaks(-x)`

    The function can handle NaN's

    See this IPython Notebook [1]_.

    References
    ----------
    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb

    Examples
    --------
    from detect_peaks import detect_peaks
    x = np.random.randn(100)
    x[60:81] = np.nan
    # detect all peaks and plot data
    ind = detect_peaks(x, show=True)
    print(ind)

    x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5
    # set minimum peak height = 0 and minimum peak distance = 20
    detect_peaks(x, mph=0, mpd=20, show=True)

    x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]
    # set minimum peak distance = 2
    detect_peaks(x, mpd=2, show=True)

    x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5
    # detection of valleys instead of peaks
    detect_peaks(x, mph=0, mpd=20, valley=True, show=True)

    x = [0, 1, 1, 0, 1, 1, 0]
    # detect both edges
    detect_peaks(x, edge='both', show=True)

    x = [-2, 1, -2, 2, 1, 1, 3, 0]
    # set threshold = 2
    detect_peaks(x, threshold = 2, show=True)
    """

    x = np.atleast_1d(x).astype('float64')
    if x.size < 3:
        return np.array([], dtype=int)
    if valley:
        x = -x
    # find indices of all peaks
    dx = x[1:] - x[:-1]
    # handle NaN's
    indnan = np.where(np.isnan(x))[0]
    if indnan.size:
        x[indnan] = np.inf
        dx[np.where(np.isnan(dx))[0]] = np.inf
    ine, ire, ife = np.array([[], [], []], dtype=int)
    if not edge:
        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]
    else:
        if edge.lower() in ['rising', 'both']:
            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]
        if edge.lower() in ['falling', 'both']:
            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]
    ind = np.unique(np.hstack((ine, ire, ife)))
    # handle NaN's
    if ind.size and indnan.size:
        # NaN's and values close to NaN's cannot be peaks
        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan - 1, indnan + 1))), invert=True)]
    # first and last values of x cannot be peaks
    if ind.size and ind[0] == 0:
        ind = ind[1:]
    if ind.size and ind[-1] == x.size - 1:
        ind = ind[:-1]
    # remove peaks < minimum peak height
    if ind.size and mph is not None:
        ind = ind[x[ind] >= mph]
    # remove peaks - neighbors < threshold
    if ind.size and threshold > 0:
        dx = np.min(np.vstack([x[ind] - x[ind - 1], x[ind] - x[ind + 1]]), axis=0)
        ind = np.delete(ind, np.where(dx < threshold)[0])
    # detect small peaks closer than minimum peak distance
    if ind.size and mpd > 1:
        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height
        idel = np.zeros(ind.size, dtype=bool)
        for i in range(ind.size):
            if not idel[i]:
                # keep peaks with the same height if kpsh is True
                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \
                              & (x[ind[i]] > x[ind] if kpsh else True)
                idel[i] = 0  # Keep current peak
        # remove the small peaks and sort back the indices by their occurrence
        ind = np.sort(ind[~idel])

    if show:
        if indnan.size:
            x[indnan] = np.nan
        if valley:
            x = -x
        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)

    return ind


def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):
    """Plot results of the detect_peaks function, see its help."""
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print('matplotlib is not available.')
    else:
        if ax is None:
            _, ax = plt.subplots(1, 1, figsize=(8, 4))

        ax.plot(x, 'b', lw=1)
        if ind.size:
            label = 'valley' if valley else 'peak'
            label = label + 's' if ind.size > 1 else label
            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,
                    label='%d %s' % (ind.size, label))
            ax.legend(loc='best', framealpha=.5, numpoints=1)
        ax.set_xlim(-.02 * x.size, x.size * 1.02 - 1)
        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()
        yrange = ymax - ymin if ymax > ymin else 1
        ax.set_ylim(ymin - 0.1 * yrange, ymax + 0.1 * yrange)
        ax.set_xlabel('Data #', fontsize=14)
        ax.set_ylabel('Amplitude', fontsize=14)
        mode = 'Valley detection' if valley else 'Peak detection'
        ax.set_title("%s (mph=%s, mpd=%d, threshold=%s, edge='%s')"
                     % (mode, str(mph), mpd, str(threshold), edge))
        # plt.grid()
        plt.show()


def triangle_auto_threshold(device, img, maxvalue, object_type, xstep=1, debug=None):
    """Creates a binary image from a grayscaled image using Zack et al.'s (1977) thresholding.

    Inputs:
    device      = device number. Used to count steps in the pipeline
    img         = img object, grayscale
    maxvalue    = value to apply above threshold (usually 255 = white)
    object_type = light or dark
                  - If object is light then standard thresholding is done
                  - If object is dark then inverse thresholding is done
    xstep       = value to move along x-axis to determine the points from which to calculate distance
                    recommended to start at 1 and change if needed)
    debug       = True/False. If True, print image

    Returns:
    device      = device number
    t_img       = the thresholded image


    :param img: numpy array
    :param maxvalue: int
    :param object_type: str
    :param device: int
    :param debug: bool
    :param xstep: optional int
    :return device: int
    :return t_img: numpy array
    """
    device += 1

    # Calculate automatic threshold value based on triangle algorithm
    hist = cv2.calcHist([img], [0], None, [256], [0, 255])

    # Make histogram one array
    newhist = []
    for item in hist:
        newhist.extend(item)

    # Detect peaks
    show = False
    if debug == "plot":
        show = True
    ind = _detect_peaks(newhist, mph=None, mpd=1, show=show)

    # Find point corresponding to highest peak
    # Find intensity value (y) of highest peak
    max_peak_int = max(list(newhist[i] for i in ind))
    # Find value (x) of highest peak
    max_peak = [i for i, x in enumerate(newhist) if x == max(newhist)]
    # Combine x,y
    max_peak_xy = [max_peak[0], max_peak_int]

    # Find final point at end of long tail
    end_x = len(newhist) - 1
    end_y = newhist[end_x]
    end_xy = [end_x, end_y]

    # Define the known points
    points = [max_peak_xy, end_xy]
    x_coords, y_coords = zip(*points)

    # Get threshold value
    peaks = []
    dists = []

    for i in range(x_coords[0], x_coords[1], xstep):
        distance = (((x_coords[1] - x_coords[0]) * (y_coords[0] - hist[i])) -
                    ((x_coords[0] - i) * (y_coords[1] - y_coords[0]))) / math.sqrt(
            (float(x_coords[1]) - float(x_coords[0])) *
            (float(x_coords[1]) - float(x_coords[0])) +
            ((float(y_coords[1]) - float(y_coords[0])) *
             (float(y_coords[1]) - float(y_coords[0]))))
        peaks.append(i)
        dists.append(distance)
    autothresh = [peaks[x] for x in [i for i, x in enumerate(list(dists)) if x == max(list(dists))]]
    autothreshval = autothresh[0]

    # check whether to inverse the image or not and make an ending extension
    obj = 0
    extension = ''
    if object_type == 'light':
        extension = '.png'
        obj = cv2.THRESH_BINARY
    elif object_type == 'dark':
        extension = '_inv.png'
        obj = cv2.THRESH_BINARY_INV

    # threshold the image based on the object type using triangle binarization
    t_val, t_img = cv2.threshold(img, autothreshval, maxvalue, obj)

    if debug is not None:
        import matplotlib
        matplotlib.use('Agg')
        from matplotlib import pyplot as plt

    if debug == 'print':
        name = str(device) + '_triangle_thresh_img_' + str(t_val) + str(extension)
        print_image(t_img, name)
        plt.clf()

        plt.plot(hist)
        plt.title('Threshold value = {t}'.format(t=autothreshval))
        plt.axis([0, 256, 0, max(hist)])
        plt.grid('on')
        fig_name_hist = str(device) + '_triangle_thresh_hist_' + str(t_val) + str(extension)
        # write the figure to current directory
        plt.savefig(fig_name_hist)
        # close pyplot plotting window
        plt.clf()

    elif debug == 'plot':
        print('Threshold value = {t}'.format(t=autothreshval))
        plot_image(t_img, cmap="gray")

        plt.plot(hist)
        plt.axis([0, 256, 0, max(hist)])
        plt.grid('on')
        plt.show()

    return device, t_img
# Analyze Color of Object

import os
import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error
from . import plot_colorbar


def _pseudocolored_image(device, histogram, bins, img, mask, background, channel, filename, resolution,
                         analysis_images, debug):
    """Pseudocolor image.

    Inputs:
    histogram       = a normalized histogram of color values from one color channel
    bins            = number of color bins the channel is divided into
    img             = input image
    mask            = binary mask image
    background      = what background image?: channel image (img) or white
    channel         = color channel name
    filename        = input image filename
    resolution      = output image resolution
    analysis_images = list of analysis image filenames
    debug           = print or plot. Print = save to file, Plot = print to screen.

    Returns:
    analysis_images = list of analysis image filenames

    :param histogram: list
    :param bins: int
    :param img: numpy array
    :param mask: numpy array
    :param background: str
    :param channel: str
    :param filename: str
    :param resolution: int
    :param analysis_images: list
    :return analysis_images: list
    """
    mask_inv = cv2.bitwise_not(mask)

    cplant = cv2.applyColorMap(histogram, colormap=2)
    cplant1 = cv2.bitwise_and(cplant, cplant, mask=mask)

    output_imgs = {"pseudo_on_img": {"background": "img", "img": None},
                   "pseudo_on_white": {"background": "white", "img": None}}

    if background == 'img' or background == 'both':
        # mask the background and color the plant with color scheme 'jet'
        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        img_back = cv2.bitwise_and(img_gray, img_gray, mask=mask_inv)
        img_back3 = np.dstack((img_back, img_back, img_back))

        output_imgs["pseudo_on_img"]["img"] = cv2.add(cplant1, img_back3)

    if background == 'white' or background == 'both':
        # Get the image size
        if np.shape(img)[2] == 3:
            ix, iy, iz = np.shape(img)
        else:
            ix, iy = np.shape(img)
        size = ix, iy
        back = np.zeros(size, dtype=np.uint8)
        w_back = back + 255
        w_back3 = np.dstack((w_back, w_back, w_back))
        img_back3 = cv2.bitwise_and(w_back3, w_back3, mask=mask_inv)
        output_imgs["pseudo_on_white"]["img"] = cv2.add(cplant1, img_back3)

    if filename:
        for key in output_imgs:
            if output_imgs[key]["img"] is not None:
                fig_name_pseudo = str(filename[0:-4]) + '_' + str(channel) + '_pseudo_on_' + \
                                  output_imgs[key]["background"] + '.jpg'
                path = os.path.dirname(filename)
                print_image(output_imgs[key]["img"], fig_name_pseudo)
                analysis_images.append(['IMAGE', 'pseudo', fig_name_pseudo])
    else:
        path = "."
        
    if debug is not None:
        if debug == 'print':
            for key in output_imgs:
                if output_imgs[key]["img"] is not None:
                    print_image(output_imgs[key]["img"], (str(device) + "_" + output_imgs[key]["background"] +
                                                          '_pseudocolor.jpg'))
            fig_name = 'VIS_pseudocolor_colorbar_' + str(channel) + '_channel.svg'
            if not os.path.isfile(os.path.join(path, fig_name)):
                plot_colorbar(path, fig_name, bins)
        elif debug == 'plot':
            for key in output_imgs:
                if output_imgs[key]["img"] is not None:
                    plot_image(output_imgs[key]["img"])

    return analysis_images


def analyze_color(img, imgname, mask, bins, device, debug=None, hist_plot_type=None, pseudo_channel='v',
                  pseudo_bkg='img', resolution=300, filename=False):
    """Analyze the color properties of an image object

    Inputs:
    img              = image
    imgname          = name of input image
    mask             = mask made from selected contours
    device           = device number. Used to count steps in the pipeline
    debug            = None, print, or plot. Print = save to file, Plot = print to screen.
    hist_plot_type   = 'None', 'all', 'rgb','lab' or 'hsv'
    color_slice_type = 'None', 'rgb', 'hsv' or 'lab'
    pseudo_channel   = 'None', 'l', 'm' (green-magenta), 'y' (blue-yellow), h','s', or 'v', creates pseduocolored image
                       based on the specified channel
    pseudo_bkg       = 'img' => channel image, 'white' => white background image, 'both' => both img and white options
    filename         = False or image name. If defined print image

    Returns:
    device           = device number
    hist_header      = color histogram data table headers
    hist_data        = color histogram data table values
    analysis_images  = list of output images

    :param img: numpy array
    :param imgname: str
    :param mask: numpy array
    :param bins: int
    :param device: int
    :param debug: str
    :param hist_plot_type: str
    :param pseudo_channel: str
    :param pseudo_bkg: str
    :param resolution: int
    :param filename: str
    :return device: int
    :return hist_header: list
    :return hist_data: list
    :return analysis_images: list
    """
    device += 1

    masked = cv2.bitwise_and(img, img, mask=mask)
    b, g, r = cv2.split(masked)
    lab = cv2.cvtColor(masked, cv2.COLOR_BGR2LAB)
    l, m, y = cv2.split(lab)
    hsv = cv2.cvtColor(masked, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)

    # Color channel dictionary
    norm_channels = {"b": b / (256 / bins),
                     "g": g / (256 / bins),
                     "r": r / (256 / bins),
                     "l": l / (256 / bins),
                     "m": m / (256 / bins),
                     "y": y / (256 / bins),
                     "h": h / (256 / bins),
                     "s": s / (256 / bins),
                     "v": v / (256 / bins)
                     }
    # Histogram plot types
    hist_types = {"all": ("b", "g", "r", "l", "m", "y", "h", "s", "v"),
                  "rgb": ("b", "g", "r"),
                  "lab": ("l", "m", "y"),
                  "hsv": ("h", "s", "v")}

    # If the user-input pseudo_channel is not None and is not found in the list of accepted channels, exit
    if pseudo_channel is not None and pseudo_channel not in norm_channels:
        fatal_error("Pseudocolor channel was " + str(pseudo_channel) +
                    ', but can only be one of the following: None, "l", "m", "y", "h", "s" or "v"!')
    # If the user-input pseudocolored image background is not in the accepted input list, exit
    if pseudo_bkg not in ["white", "img", "both"]:
        fatal_error("The pseudocolored image background was " + str(pseudo_bkg) +
                    ', but can only be one of the following: "white", "img", or "both"!')
    # If the user-input histogram color-channel plot type is not in the list of accepted channels, exit
    if hist_plot_type is not None and hist_plot_type not in hist_types:
        fatal_error("The histogram plot type was " + str(hist_plot_type) +
                    ', but can only be one of the following: None, "all", "rgb", "lab", or "hsv"!')

    histograms = {
        "b": {"label": "blue", "graph_color": "blue",
              "hist": cv2.calcHist([norm_channels["b"]], [0], mask, [bins], [0, (bins - 1)])},
        "g": {"label": "green", "graph_color": "forestgreen",
              "hist": cv2.calcHist([norm_channels["g"]], [0], mask, [bins], [0, (bins - 1)])},
        "r": {"label": "red", "graph_color": "red",
              "hist": cv2.calcHist([norm_channels["r"]], [0], mask, [bins], [0, (bins - 1)])},
        "l": {"label": "lightness", "graph_color": "dimgray",
              "hist": cv2.calcHist([norm_channels["l"]], [0], mask, [bins], [0, (bins - 1)])},
        "m": {"label": "green-magenta", "graph_color": "magenta",
              "hist": cv2.calcHist([norm_channels["m"]], [0], mask, [bins], [0, (bins - 1)])},
        "y": {"label": "blue-yellow", "graph_color": "yellow",
              "hist": cv2.calcHist([norm_channels["y"]], [0], mask, [bins], [0, (bins - 1)])},
        "h": {"label": "hue", "graph_color": "blueviolet",
              "hist": cv2.calcHist([norm_channels["h"]], [0], mask, [bins], [0, (bins - 1)])},
        "s": {"label": "saturation", "graph_color": "cyan",
              "hist": cv2.calcHist([norm_channels["s"]], [0], mask, [bins], [0, (bins - 1)])},
        "v": {"label": "value", "graph_color": "orange",
              "hist": cv2.calcHist([norm_channels["v"]], [0], mask, [bins], [0, (bins - 1)])}
    }

    hist_data_b = [l[0] for l in histograms["b"]["hist"]]
    hist_data_g = [l[0] for l in histograms["g"]["hist"]]
    hist_data_r = [l[0] for l in histograms["r"]["hist"]]
    hist_data_l = [l[0] for l in histograms["l"]["hist"]]
    hist_data_m = [l[0] for l in histograms["m"]["hist"]]
    hist_data_y = [l[0] for l in histograms["y"]["hist"]]
    hist_data_h = [l[0] for l in histograms["h"]["hist"]]
    hist_data_s = [l[0] for l in histograms["s"]["hist"]]
    hist_data_v = [l[0] for l in histograms["v"]["hist"]]

    binval = np.arange(0, bins)
    bin_values = [l for l in binval]

    # Store Color Histogram Data
    hist_header = [
        'HEADER_HISTOGRAM',
        'bin-number',
        'bin-values',
        'blue',
        'green',
        'red',
        'lightness',
        'green-magenta',
        'blue-yellow',
        'hue',
        'saturation',
        'value'
    ]

    hist_data = [
        'HISTOGRAM_DATA',
        bins,
        bin_values,
        hist_data_b,
        hist_data_g,
        hist_data_r,
        hist_data_l,
        hist_data_m,
        hist_data_y,
        hist_data_h,
        hist_data_s,
        hist_data_v
    ]

    analysis_images = []

    if pseudo_channel is not None:
        analysis_images = _pseudocolored_image(device, norm_channels[pseudo_channel], bins, img, mask, pseudo_bkg,
                                               pseudo_channel, filename, resolution, analysis_images, debug)

    if hist_plot_type is not None and filename:
        import matplotlib
        matplotlib.use('Agg')
        from matplotlib import pyplot as plt

        # Create Histogram Plot
        for channel in hist_types[hist_plot_type]:
            plt.plot(histograms[channel]["hist"], color=histograms[channel]["graph_color"],
                     label=histograms[channel]["label"])
            plt.xlim([0, bins - 1])
            plt.legend()

        # Print plot
        fig_name = (str(filename[0:-4]) + '_' + str(hist_plot_type) + '_hist.svg')
        plt.savefig(fig_name)
        analysis_images.append(['IMAGE', 'hist', fig_name])
        if debug == 'print':
            fig_name = (str(device) + '_' + str(hist_plot_type) + '_hist.svg')
            plt.savefig(fig_name)
        plt.clf()

    return device, hist_header, hist_data, analysis_images
# Find Objects

import cv2
import numpy as np
from . import print_image
from . import plot_image


def find_objects(img, mask, device, debug=None):
    """Find all objects and color them blue.

    Inputs:
    img       = image that the objects will be overlayed
    mask      = what is used for object detection
    device    = device number.  Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    objects   = list of contours
    hierarchy = contour hierarchy list

    :param img: numpy array
    :param mask: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return objects: list
    :return hierarchy: list
    """

    device += 1
    mask1 = np.copy(mask)
    ori_img = np.copy(img)
    objects, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    for i, cnt in enumerate(objects):
        cv2.drawContours(ori_img, objects, i, (255, 102, 255), -1, lineType=8, hierarchy=hierarchy)
    if debug == 'print':
        print_image(ori_img, (str(device) + '_id_objects.png'))
    elif debug == 'plot':
        plot_image(ori_img)

    return device, objects, hierarchy
# Error handling


def fatal_error(error):
    """Print out the error message that gets passed, then quit the program.

    Inputs:
    error = error message text

    :param error: str
    :return:
    """

    raise RuntimeError(error)
# Apply White or Black Background Mask

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def apply_mask(img, mask, mask_color, device, debug=None):
    """Apply white image mask to image, with bitwise AND operator bitwise NOT operator and ADD operator.

    Inputs:
    img        = image object, color(RGB)
    mask       = image object, binary (black background with white object)
    mask_color = white or black
    device     = device number. Used to count steps in the pipeline
    debug      = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device     = device number
    masked_img = masked image

    :param img: numpy array
    :param mask: numpy array
    :param mask_color: str
    :param device: int
    :param debug: str
    :return device: int
    :return masked_img: numpy array
    """

    device += 1
    if mask_color == 'white':
        # Mask image
        masked_img = cv2.bitwise_and(img, img, mask=mask)
        # Create inverted mask for background
        mask_inv = cv2.bitwise_not(mask)
        # Invert the background so that it is white, but apply mask_inv so you don't white out the plant
        white_mask = cv2.bitwise_not(masked_img, mask=mask_inv)
        # Add masked image to white background (can't just use mask_inv because that is a binary)
        white_masked = cv2.add(masked_img, white_mask)
        if debug == 'print':
            print_image(white_masked, (str(device) + '_wmasked.png'))
        elif debug == 'plot':
            plot_image(white_masked)
        return device, white_masked
    elif mask_color == 'black':
        masked_img = cv2.bitwise_and(img, img, mask=mask)
        if debug == 'print':
            print_image(masked_img, (str(device) + '_bmasked.png'))
        elif debug == 'plot':
            plot_image(masked_img)
        return device, masked_img
    else:
        fatal_error('Mask Color' + str(mask_color) + ' is not "white" or "black"!')
# User-Input Boundary Line

import cv2
import numpy as np
from . import print_image
from . import plot_image


def analyze_bound(img, imgname, obj, mask, line_position, device, debug=None, filename=False):
    """User-input boundary line tool

    Inputs:
    img             = image
    imgname         = name of input image
    obj             = single or grouped contour object
    mask            = mask made from selected contours
    shape_header    = pass shape header data to function
    shape_data      = pass shape data so that analyze_bound data can be appended to it
    line_position   = position of boundry line (a value of 0 would draw the line through the bottom of the image)
    device          = device number. Used to count steps in the pipeline
    debug           = None, print, or plot. Print = save to file, Plot = print to screen.
    filename        = False or image name. If defined print image.

    Returns:
    device          = device number
    bound_header    = data table column headers
    bound_data      = boundary data table
    analysis_images = output image filenames

    :param img: numpy array
    :param imgname: str
    :param obj: list
    :param mask: numpy array
    :param line_position: int
    :param device: int
    :param debug: str
    :param filename: str
    :return device: int
    :return bound_header: tuple
    :return bound_data: tuple
    :return analysis_images: list
    """

    device += 1
    ori_img = np.copy(img)

    # Draw line horizontal line through bottom of image, that is adjusted to user input height
    if len(np.shape(ori_img)) == 3:
        iy, ix, iz = np.shape(ori_img)
    else:
        iy, ix = np.shape(ori_img)
    size = (iy, ix)
    size1 = (iy, ix, 3)
    background = np.zeros(size, dtype=np.uint8)
    wback = (np.zeros(size1, dtype=np.uint8)) + 255
    x_coor = int(ix)
    y_coor = int(iy) - int(line_position)
    rec_corner = int(iy - 2)
    rec_point1 = (1, rec_corner)
    rec_point2 = (x_coor - 2, y_coor - 2)
    cv2.rectangle(background, rec_point1, rec_point2, (255), 1)
    below_contour, below_hierarchy = cv2.findContours(background, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

    x, y, width, height = cv2.boundingRect(obj)

    if y_coor - y <= 0:
        height_above_bound = 0
        height_below_bound = height
    elif y_coor - y > 0:
        height_1 = y_coor - y
        if height - height_1 <= 0:
            height_above_bound = height
            height_below_bound = 0
        else:
            height_above_bound = y_coor - y
            height_below_bound = height - height_above_bound

    below = []
    above = []
    mask_nonzerox, mask_nonzeroy = np.nonzero(mask)
    obj_points = np.vstack((mask_nonzeroy, mask_nonzerox))
    obj_points1 = np.transpose(obj_points)

    for i, c in enumerate(obj_points1):
        xy = tuple(c)
        pptest = cv2.pointPolygonTest(below_contour[0], xy, measureDist=False)
        if pptest == 1:
            below.append(xy)
            cv2.circle(ori_img, xy, 1, (0, 0, 255))
            cv2.circle(wback, xy, 1, (0, 0, 255))
        else:
            above.append(xy)
            cv2.circle(ori_img, xy, 1, (0, 255, 0))
            cv2.circle(wback, xy, 1, (0, 255, 0))
    above_bound_area = len(above)
    below_bound_area = len(below)
    percent_bound_area_above = ((float(above_bound_area)) / (float(above_bound_area + below_bound_area))) * 100
    percent_bound_area_below = ((float(below_bound_area)) / (float(above_bound_area + below_bound_area))) * 100

    bound_header = [
        'HEADER_BOUNDARY' + str(line_position),
        'height_above_bound',
        'height_below_bound',
        'above_bound_area',
        'percent_above_bound_area',
        'below_bound_area',
        'percent_below_bound_area'
    ]

    bound_data = [
        'BOUNDARY_DATA',
        height_above_bound,
        height_below_bound,
        above_bound_area,
        percent_bound_area_above,
        below_bound_area,
        percent_bound_area_below
    ]

    analysis_images = []

    if above_bound_area or below_bound_area:
        point3 = (0, y_coor - 4)
        point4 = (x_coor, y_coor - 4)
        cv2.line(ori_img, point3, point4, (255, 0, 255), 5)
        cv2.line(wback, point3, point4, (255, 0, 255), 5)
        m = cv2.moments(mask, binaryImage=True)
        cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
        if y_coor - y <= 0:
            cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (0, 255, 0), 3)
            cv2.line(wback, (int(cmx), y), (int(cmx), y + height), (0, 255, 0), 3)
        elif y_coor - y > 0:
            height_1 = y_coor - y
            if height - height_1 <= 0:
                cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (255, 0, 0), 3)
                cv2.line(wback, (int(cmx), y), (int(cmx), y + height), (255, 0, 0), 3)
            else:
                cv2.line(ori_img, (int(cmx), y_coor - 2), (int(cmx), y_coor - height_above_bound), (255, 0, 0), 3)
                cv2.line(ori_img, (int(cmx), y_coor - 2), (int(cmx), y_coor + height_below_bound), (0, 255, 0), 3)
                cv2.line(wback, (int(cmx), y_coor - 2), (int(cmx), y_coor - height_above_bound), (255, 0, 0), 3)
                cv2.line(wback, (int(cmx), y_coor - 2), (int(cmx), y_coor + height_below_bound), (0, 255, 0), 3)
        if filename:
            # Output images with boundary line, above/below bound area
            extention = filename.split('.')[-1]
            # out_file = str(filename[0:-4]) + '_boundary' + str(line_position) + '.' + extention
            out_file = str(filename[0:-4]) + '_boundary' + str(line_position) + '.jpg'
            print_image(ori_img, out_file)
            analysis_images = ['IMAGE', 'boundary', out_file]

    if debug is not None:
        point3 = (0, y_coor - 4)
        point4 = (x_coor, y_coor - 4)
        cv2.line(ori_img, point3, point4, (255, 0, 255), 5)
        cv2.line(wback, point3, point4, (255, 0, 255), 5)
        m = cv2.moments(mask, binaryImage=True)
        cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
        if y_coor - y <= 0:
            cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (0, 255, 0), 3)
            cv2.line(wback, (int(cmx), y), (int(cmx), y + height), (0, 255, 0), 3)
        elif y_coor - y > 0:
            height_1 = y_coor - y
            if height - height_1 <= 0:
                cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (255, 0, 0), 3)
                cv2.line(wback, (int(cmx), y), (int(cmx), y + height), (255, 0, 0), 3)
            else:
                cv2.line(ori_img, (int(cmx), y_coor - 2), (int(cmx), y_coor - height_above_bound), (255, 0, 0), 3)
                cv2.line(ori_img, (int(cmx), y_coor - 2), (int(cmx), y_coor + height_below_bound), (0, 255, 0), 3)
                cv2.line(wback, (int(cmx), y_coor - 2), (int(cmx), y_coor - height_above_bound), (255, 0, 0), 3)
                cv2.line(wback, (int(cmx), y_coor - 2), (int(cmx), y_coor + height_below_bound), (0, 255, 0), 3)
        if debug == 'print':
            print_image(wback, (str(device) + '_boundary_on_white.jpg'))
            print_image(ori_img, (str(device) + '_boundary_on_img.jpg'))
        if debug == 'plot':
            plot_image(wback)
            plot_image(ori_img)

    return device, bound_header, bound_data, analysis_images
# Crop position mask

import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def shift_img(img, device, number, side="right", debug=None):
    """this function allows you to shift an image over without changing dimensions

    Inputs:
    img     = image to mask
    number  = number of rows or columns to add
    side   = "top", "bottom", "right", "left" where to add the rows or columns to
    device  = device counter
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device  = device number
    newmask = image mask

    :param img: numpy array
    :param device: int
    :param number: int
    :param side: str
    :param debug: str
    :return newmask: numpy array
    """
    device += 1

    number = number - 1

    if number < 0:
        fatal_error("x and y cannot be negative numbers or non-integers")

    # get the sizes of the images
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
        ori_img = np.copy(img)
    else:
        ix, iy = np.shape(img)
        ori_img = np.dstack((img, img, img))

    if side == "top":
        top = np.zeros((number, iy, 3), dtype=np.uint8)
        adjust = ix - number
        adjusted_img = np.vstack((top, ori_img[0:adjust, 0:]))

    if side == 'bottom':
        bottom = np.zeros((number, iy, 3), dtype=np.uint8)
        adjusted_img = np.vstack((ori_img[number:, 0:], bottom))

    if side == 'right':
        right = np.zeros((ix, number, 3), dtype=np.uint8)
        adjusted_img = np.hstack((ori_img[0:, number:], right))
    if side == 'left':
        left = np.zeros((ix, number, 3), dtype=np.uint8)
        adjust = iy - number
        adjusted_img = np.hstack((left, ori_img[0:, 0:adjust]))


    if len(np.shape(img)) == 2:
        adjusted_img, channel2, channel3 = np.dsplit(adjusted_img, 3)
    if debug == 'print':
        print_image(adjusted_img, (str(device) + "_shifted_img.png"))
    elif debug == 'plot':
        if len(np.shape(adjusted_img)) == 3:
            plot_image(adjusted_img)
        else:
            plot_image(adjusted_img, cmap='gray')

    return device, adjusted_img
# Dilation filter

import cv2
import numpy as np
from . import print_image
from . import plot_image


def dilate(img, kernel, i, device, debug=None):
    """Performs morphological 'dilation' filtering. Adds pixel to center of kernel if conditions set in kernel are true.

    Inputs:
    img     = input image
    kernel  = filtering window, you'll need to make your own using as such:
              kernal = np.zeros((x,y), dtype=np.uint8), then fill the kernal with appropriate values
    i       = interations, i.e. number of consecutive filtering passes
    device  = device number. Used to count steps in the pipeline
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device  = device number
    dil_img = dilated image

    :param img: numpy array
    :param kernel: numpy array
    :param i: int
    :param device: int
    :param debug: str
    :return device: int
    :return dil_img: numpy array
    """

    kernel1 = int(kernel)
    kernel2 = np.ones((kernel1, kernel1), np.uint8)
    dil_img = cv2.dilate(src=img, kernel=kernel2, iterations=i)
    device += 1
    if debug == 'print':
        print_image(dil_img, str(device) + '_dil_image_' + 'itr_' + str(i) + '.png')
    elif debug == 'plot':
        plot_image(dil_img, cmap='gray')
    return device, dil_img
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract NIR signal data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
  parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Main pipeline
def main():
  # Get options
  args = options()
  
  # Does the database exist?
  if not os.path.exists(args.database):
    pcv.fatal_error("The database file " + str(args.database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(args.database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Open output file
  try:
    out = open(args.outfile, 'w')
  except IOError:
    print("IO error")
    
  # Replace the row_factory result constructor with a dictionary constructor
  #connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str
  
  # Database handler
  db = connect.cursor()
  
  # Retrieve snapshots and process data
  db.execute('SELECT * FROM `snapshots` INNER JOIN `nir_signal` ON `snapshots`.`image_id` = `nir_signal`.`image_id`')
  names = list(map(lambda x: x[0], db.description))
  bins = 0
  names[-1] = 's0'
  for row in db.fetchall():
    if bins == 0:
      bins = row[9]
      for s in range(1,bins):
        names.append('s' + str(s))
      out.write(','.join(map(str,names)) + '\n')
    out.write(','.join(map(str, row)) + '\n')
  

if __name__ == '__main__':
  main()
#!/usr/bin/env python

from __future__ import print_function
import os
import sys
import argparse
import datetime
import plantcv.learn


# Parse command-line arguments
###########################################
def options():
    """Parse command line options.

    :return args: object -- parsed arguments
    :raises: IOError, KeyError
    """

    # Job start time
    start_time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')
    print("Starting run " + start_time + '\n', file=sys.stderr)

    # Create an argument parser
    parser = argparse.ArgumentParser(description="PlantCV machine learning training script.",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # Create subcommand parsers
    subparsers = parser.add_subparsers()

    # Create the Naive Bayes subcommand
    nb_cmd = subparsers.add_parser("naive_bayes", help="Run the naive Bayes two-class training method.")
    nb_cmd.add_argument("-i", "--imgdir", help="Input directory containing images.", required=True)
    nb_cmd.add_argument("-b", "--maskdir", help="Input directory containing black/white masks.", required=True)
    nb_cmd.add_argument("-o", "--outfile", help="Trained classifier output filename.", required=True)
    nb_cmd.add_argument("-p", "--plots", help="Make output plots.", default=False, action="store_true")
    nb_cmd.set_defaults(func=run_naive_bayes)

    # Create the Naive Bayes Multiclass subcommand
    nbm_cmd = subparsers.add_parser("naive_bayes_multiclass",
                                    help="Run the naive Bayes two or more class training method.")
    nbm_cmd.add_argument("-f", "--file",
                         help="Input file containing a table of pixel RGB values sampled for each input class.",
                         required=True)
    nbm_cmd.add_argument("-o", "--outfile", help="Trained classifier output filename.", required=True)
    nbm_cmd.add_argument("-p", "--plots", help="Make output plots.", default=False, action="store_true")
    nbm_cmd.set_defaults(func=run_naive_bayes_multiclass)

    # Parse command-line options
    args = parser.parse_args()
    # Execute the selected training method
    args.func(args)
###########################################


# Run the naive Bayes method
###########################################
def run_naive_bayes(args):
    if not os.path.exists(args.imgdir):
        raise IOError("Directory does not exist: {0}".format(args.imgdir))
    if not os.path.exists(args.maskdir):
        raise IOError("Directory does not exist: {0}".format(args.maskdir))
    print("Running the naive Bayes two-class training method...")
    plantcv.learn.naive_bayes(imgdir=args.imgdir, maskdir=args.maskdir, outfile=args.outfile, mkplots=args.plots)
###########################################


# Run the naive Bayes multiclass method
###########################################
def run_naive_bayes_multiclass(args):
    if not os.path.exists(args.file):
        raise IOError("File does not exist: {0}".format(args.file))
    print("Running the naive Bayes multiclass training method...")
    plantcv.learn.naive_bayes_multiclass(samples_file=args.file, outfile=args.outfile, mkplots=args.plots)
###########################################


# Main
###########################################
def main():
    """Main program.
    """
    # Parse command-line options and run training method
    options()
###########################################


if __name__ == '__main__':
    main()
# Identify landmark positions within a contour for morphometric analysis

import numpy as np
import math
import cv2


def acute(obj, win, thresh, mask, device, debug=None):
    """acute: identify landmark positions within a contour for morphometric analysis

    Inputs:
    obj         = An opencv contour array of interest to be scanned for landmarks
    win         = maximum cumulative pixel distance window for calculating angle
                  score; 1 cm in pixels often works well
    thresh      = angle score threshold to be applied for mapping out landmark
                  coordinate clusters within each contour
    mask        = binary mask used to generate contour array (necessary for ptvals)
    device      = device number. Used to count steps in the pipeline
    debug       = None, print, or plot. Print = save to file, Plot = print to screen.

    Outputs:
    homolog_pts = pseudo-landmarks selected from each landmark cluster
    start_pts   = pseudo-landmark island starting position; useful in parsing homolog_pts in downstream analyses
    stop_pts    = pseudo-landmark island end position ; useful in parsing homolog_pts in downstream analyses
    ptvals      = average values of pixel intensity from the mask used to generate cont;
                  useful in parsing homolog_pts in downstream analyses
    chain       = raw angle scores for entire contour, used to visualize landmark
                  clusters
    verbose_out = supplemental file which stores coordinates, distance from
                  landmark cluster edges, and angle score for entire contour.  Used
                  in troubleshooting.

    :param obj: ndarray
    :param win: int
    :param thresh: int
    :param mask: ndarray
    :param device: int
    :param debug: str
    :return homolog_pts:
    """

    device += 1
    chain = []                                         # Create empty chain to store angle scores
    for k in list(range(len(obj))):                    # Coordinate-by-coordinate 3-point assignments
        vert = obj[k]
        dist_1 = 0
        for r in range(len(obj)):                      # Reverse can to obtain point A
            rev = k - r
            pos = obj[rev]
            dist_2 = np.sqrt(np.square(pos[0][0]-vert[0][0])+np.square(pos[0][1]-vert[0][1]))   
            if r >= 2:
                if (dist_2 > dist_1) & (dist_2 <= win):  # Further from vertex than current pt A while within window?
                    dist_1 = dist_2
                    ptA = pos                              # Load best fit within window as point A
                elif dist_2 > win:
                    break
            else:
                ptA = pos          
        dist_1 = 0
        for f in range(len(obj)):                      # Forward scan to obtain point B
            fwd = k + f
            if fwd >= len(obj):
                fwd -= len(obj)
            pos = obj[fwd]
            dist_2 = np.sqrt(np.square(pos[0][0]-vert[0][0])+np.square(pos[0][1]-vert[0][1]))   
            if f >= 2:
                if (dist_2 > dist_1) & (dist_2 <= win):  # Further from vertex than current pt B while within window?
                    dist_1 = dist_2
                    ptB = pos                              # Load best fit within window as point B
                elif dist_2 > win:
                    break
            else:
                ptB = pos

        # Angle in radians derived from Law of Cosines, converted to degrees
        P12 = np.sqrt((vert[0][0]-ptA[0][0])*(vert[0][0]-ptA[0][0])+(vert[0][1]-ptA[0][1])*(vert[0][1]-ptA[0][1]))
        P13 = np.sqrt((vert[0][0]-ptB[0][0])*(vert[0][0]-ptB[0][0])+(vert[0][1]-ptB[0][1])*(vert[0][1]-ptB[0][1]))
        P23 = np.sqrt((ptA[0][0]-ptB[0][0])*(ptA[0][0]-ptB[0][0])+(ptA[0][1]-ptB[0][1])*(ptA[0][1]-ptB[0][1]))
        dot = (P12*P12 + P13*P13 - P23*P23)/(2*P12*P13)

        if dot > 1:              # If float excedes 1 prevent arcos error and force to equal 1
            dot = 1
        elif dot < -1:           # If float excedes -1 prevent arcos error and force to equal -1
            dot = -1      
        ang = math.degrees(math.acos(dot))
        chain.append(ang)

    index = []                      # Index chain to find clusters below angle threshold

    for c in range(len(chain)):     # Identify links in chain with acute angles
        if float(chain[c]) <= thresh:
            index.append(c)         # Append positions of acute links to index

    acute_pos = obj[[index]]            # Extract all island points blindly

    float(len(acute_pos)) / float(len(obj))  # Proportion of informative positions

    if len(index) != 0:
        
        isle = []
        island = []

        for c in range(len(index)):           # Scan for iterative links within index
            if not island:
                island.append(index[c])       # Initiate new link island
            elif island[-1]+1 == index[c]:
                island.append(index[c])       # Append successful iteration to island
            elif island[-1]+1 != index[c]:
                ptA = obj[index[c]]
                ptB = obj[island[-1]+1]
                dist = np.sqrt(np.square(ptA[0][0]-ptB[0][0])+np.square(ptA[0][1]-ptB[0][1]))
                if win/2 > dist:
                    island.append(index[c])
                else:
                    isle.append(island)
                    island = [index[c]]

        isle.append(island)

        if len(isle) > 1:
            if (isle[0][0] == 0) & (isle[-1][-1] == (len(chain)-1)):
                print('Fusing contour edges')
                island = range(-(len(chain)-isle[-1][0]), 0)+isle[0]  # Fuse overlapping ends of contour
                # Delete islands to be spliced if start-end fusion required
                del isle[0]
                del isle[-1]
                isle.insert(0, island)      # Prepend island to isle
        else:
            print('Microcontour...')

        # Homologous point maximum distance method
        pt = []
        vals = []
        maxpts = []
        SSpts = []
        TSpts = []
        ptvals = []
        max_dist = [['cont_pos', 'max_dist', 'angle']]
        for x in range(len(isle)):

            # Identify if contour is concavity/convexity using image mask
            pix_x, pix_y, w, h = cv2.boundingRect(obj[isle[x]])  # Obtain local window around island

            for c in range(w):
                for r in range(h):
                    # Identify pixels in local window internal to the island hull
                    pos = cv2.pointPolygonTest(obj[isle[x]], (pix_x+c, pix_y+r), 0)
                    if 0 < pos:
                        vals.append(mask[pix_y+r][pix_x+c])  # Store pixel value if internal
            if len(vals) > 0:
                ptvals.append(sum(vals)/len(vals))
                vals = []
            else:
                ptvals.append('NaN')        # If no values can be retrieved (small/collapsed contours)
                vals = [] 

            # Identify pixel coordinate to use as pseudolandmark for island
            if len(isle[x]) == 1:           # If landmark is a single point (store position)
                # print 'route A'
                pt = isle[x][0]
                max_dist.append([isle[x][0], '-', chain[isle[x][0]]])
                # print pt
            elif len(isle[x]) == 2:         # If landmark is a pair of points (store more acute position)
                # print 'route B'
                ptA = chain[isle[x][0]]
                ptB = chain[isle[x][1]]
                if ptA < ptB:
                    pt = isle[x][0]             # Store point A if more acute
                    max_dist.append([isle[x][0], '-', chain[isle[x][0]]])  
                elif ptA > ptB:
                    pt = isle[x][1]             # Store point B if more acute
                    max_dist.append([isle[x][1], '-', chain[isle[x][1]]])
                # print pt
            else:                           # If landmark is multiple points (distance scan for position)
                # print 'route C'
                SS = obj[[isle[x]]][0]          # Store isle "x" start site
                TS = obj[[isle[x]]][-1]         # Store isle "x" termination site
                dist_1 = 0
                for d in range(len(isle[x])):   # Scan from SS to TS within isle "x"
                    site = obj[[isle[x][d]]]
                    SSd = np.sqrt(np.square(SS[0][0]-site[0][0][0])+np.square(SS[0][1]-site[0][0][1]))
                    TSd = np.sqrt(np.square(TS[0][0]-site[0][0][0])+np.square(TS[0][1]-site[0][0][1]))
                    # Current mean distance of 'd' to 'SS' & 'TS'
                    dist_2 = np.mean([np.abs(SSd), np.abs(TSd)])
                    max_dist.append([isle[x][d], dist_2, chain[isle[x][d]]])
                    if dist_2 > dist_1:                           # Current mean distance better fit that previous best?
                        pt = isle[x][d]
                        dist_1 = dist_2                           # Current mean becomes new best mean
                # print pt
            maxpts.append(pt)           # Empty 'pts' prior to next mean distance scan
            SSpts.append(isle[x][0])
            TSpts.append(isle[x][-1])

        homolog_pts = obj[maxpts]
        start_pts = obj[SSpts]
        stop_pts = obj[TSpts]

        if debug is not None:
            return device, homolog_pts, start_pts, stop_pts, ptvals, chain, max_dist
        else:
            return device, homolog_pts
    else: 
        if debug is not None:
            return device, [], [], [], [], [], []
        else:
            return device, []
# Histogram equalization

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def hist_equalization(img, device, debug=None):
    """Histogram equalization is a method to normalize the distribution of intensity values. If the image has low
       contrast it will make it easier to threshold.

    Inputs:
    img    = input image
    device = device number. Used to count steps in the pipeline
    debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device = device number
    img_eh = normalized image

    :param img: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return img_eh: numpy array
    """

    if len(np.shape(img)) == 3:
        fatal_error("Input image must be gray")

    img_eh = cv2.equalizeHist(img)
    device += 1
    if debug == 'print':
        print_image(img_eh, str(device) + '_hist_equal_img.png')
    elif debug == 'plot':
        plot_image(img_eh, cmap='gray')

    return device, img_eh
# Script to generate an image with metrics displayed on image

import numpy as np
import math
import numbers


def landmark_reference_pt_dist(points_r, centroid_r, bline_r, device, debug=None):
    """landmark_reference_pt_dist

    For each point in contour, get a point before (pre) and after (post) the point of interest.
    The win argument specifies the pre and post point distances.

    Inputs:
    points_r   = a set of rescaled points (basically the output of the acute_vertex fxn after the scale_features fxn)
    centroid_r = a tuple that contains the rescaled centroid coordinates
    bline_r    = a tuple that contains the rescaled boundary line - centroid coordinates
    device     = a count variable
    debug      = no output supported currently

    :param points_r: ndarray
    :param centroid_r: tuple
    :param bline_r: tuple
    :param device: int
    :param debug: str
    :return device: int
    :return vert_ave_c: float
    :return hori_ave_c: float
    :return euc_ave_c: float
    :return ang_ave_c: float
    :return vert_ave_b: float
    :return hori_ave_b: float
    :return euc_ave_b: float
    :return ang_ave_b: float
    """

    # scaled_img = np.zeros((1500,1500,3), np.uint8)
    # plotter = np.array(points_r)
    # plotter = plotter * 1000
    # for i in plotter:
    #  x,y = i.ravel()
    #  cv2.circle(scaled_img,(int(x) + 250, int(y) + 250),15,(255,255,255),-1)
    # cv2.circle(scaled_img,(int(cmx_scaled * 1000) + 250, int(cmy_scaled * 1000) + 250),25,(0,0,255), -1)
    # cv2.circle(scaled_img,(int(blx_scaled * 1000) + 250, int(bly_scaled * 1000) + 250),25,(0,255,0), -1)
    device += 1
    vert_dist_c = []
    hori_dist_c = []
    euc_dist_c = []
    angles_c = []
    cx, cy = centroid_r
    # Check to see if points are numerical or NA
    if not isinstance(cy, numbers.Number):
        return device, ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'), \
               ('NA', 'NA'), ('NA', 'NA')
    # Do this for centroid
    for pt in points_r:
        # Get coordinates from point
        x, y = pt
        # Get vertical distance and append to list
        v = y - cy
        # print "Here is the centroid vertical distance: " + str(v)
        vert_dist_c.append(v)
        # cv2.line(scaled_img, (int((x*1000)+250), int((cy*1000)+250)), (int((x*1000)+250), int((y*1000)+250)),
        #          (0,0,255), 5)
        # Get horizontal distance and append to list
        h = abs(x - cx)
        # print "Here is the centroid horizotnal distance: " + str(h)
        hori_dist_c.append(h)
        e = np.sqrt((cx - x) * (cx - x) + (cy - y) * (cy - y))
        # print "Here is the centroid euclidian distance: " + str(h)
        euc_dist_c.append(e)
        # cv2.line(scaled_img, (int((cx*1000)+250), int((cy*1000)+250)), (int((x*1000)+250),
        #                                                                 int((y*1000)+250)), (0,165,255), 5)
        # a = (h*h + v*v - e*e)/(2*h*v)
        a = (h * h + e * e - v * v) / (2 * h * e)
        if a > 1:              # If float excedes 1 prevent arcos error and force to equal 1
            a = 1
        elif a < -1:           # If float excedes -1 prevent arcos error and force to equal -1
            a = -1
        ang = abs(math.degrees(math.acos(a)))
        if v < 0:
            ang = ang * -1
        # print "Here is the centroid angle: " + str(ang)
        angles_c.append(ang)

    vert_ave_c = np.mean(vert_dist_c)
    hori_ave_c = np.mean(hori_dist_c)
    euc_ave_c = np.mean(euc_dist_c)
    ang_ave_c = np.mean(angles_c)
  
    vert_dist_b = []
    hori_dist_b = []
    euc_dist_b = []
    angles_b = []
    bx, by = bline_r
    # Do this for baseline
    for pt in points_r:
        # Get coordinates from point
        x, y = pt
        # Get vertical distance and append to list
        v = y - by
        # print "Here is the baseline vertical distance: " + str(v)
        vert_dist_b.append(v)
        # cv2.line(scaled_img, (int((x*1000)+250), int((by*1000)+250)), (int((x*1000)+250),
        #                                                                int((y*1000)+250)), (255,255,102), 5)
        # Get horizontal distance and append to list
        h = abs(x - bx)
        # print "Here is the baseline horizotnal distance: " + str(h)
        hori_dist_b.append(h)
        e = np.sqrt((bx - x) * (bx - x) + (by - y) * (by - y))
        # print "Here is the baseline euclidian distance: " + str(h)
        euc_dist_b.append(e)
        # cv2.line(scaled_img, (int((bx*1000)+250), int((by*1000)+250)), (int((x*1000)+250),
        #                                                                 int((y*1000)+250)), (255,178,102), 5)
        # a = (h*h + v*v - e*e)/(2*h*v)
        a = (h * h + e * e - v * v) / (2 * h * e)
        if a > 1:              # If float excedes 1 prevent arcos error and force to equal 1
            a = 1
        elif a < -1:           # If float excedes -1 prevent arcos error and force to equal -1
            a = -1
        ang = abs(math.degrees(math.acos(a)))
        if v < 0:
            ang = ang * -1
        # print "Here is the baseline angle: " + str(ang)
        angles_b.append(ang)

    vert_ave_b = np.mean(vert_dist_b)
    hori_ave_b = np.mean(hori_dist_b)
    euc_ave_b = np.mean(euc_dist_b)
    ang_ave_b = np.mean(angles_b)
    # cv2.line(scaled_img, (int(2), int((cy*1000)+250)), (int(1498), int((cy*1000)+250)), (0,215,255), 5)
    # cv2.line(scaled_img, (int(2), int((by*1000)+250)), (int(1498), int((by*1000)+250)), (255,0,0), 5)
    # cv2.circle(scaled_img,(int(cx * 1000) + 250, int(cy * 1000) + 250),25,(0,215,255), -1)
    # cv2.circle(scaled_img,(int(bx * 1000) + 250, int(by * 1000) + 250),25,(255,0,0), -1)
    # flipped_scaled = cv2.flip(scaled_img, 0)
    # cv2.imwrite('centroid_dist.png', flipped_scaled)
    return device, vert_ave_c, hori_ave_c, euc_ave_c, ang_ave_c, vert_ave_b, hori_ave_b, euc_ave_b, ang_ave_b
# Sobel filtering

import cv2
from . import print_image
from . import plot_image


def sobel_filter(img, dx, dy, k, device, debug=None):
    """This is a filtering method used to identify and highlight gradient edges/features using the 1st derivative.
       Typically used to identify gradients along the x-axis (dx = 1, dy = 0) and y-axis (dx = 0, dy = 1) independently.
       Performance is quite similar to Scharr filter. Used to detect edges / changes in pixel intensity. ddepth = -1
       specifies that the dimensions of output image will be the same as the input image.

    Inputs:
    # img    = image
    # dx     = derivative of x to analyze (1-3)
    # dy     = derivative of x to analyze (1-3)
    # k      = specifies the size of the kernel (must be an odd integer: 1,3,5...)
    # device = device number. Used to count steps in the pipeline
    # debug  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    sb_img   = Sobel filtered image

    :param img: numpy array
    :param dx: int
    :param dy: int
    :param k: int
    :param scale: int
    :param device: int
    :param debug: str
    :return device: int
    :return sb_img: numpy array
    """
    device += 1
    sb_img = cv2.Sobel(src=img, ddepth=-1, dx=dx, dy=dy, ksize=k)

    if debug == 'print':
        print_image(sb_img, str(device) + '_sb_img_dx' + str(dx) + '_dy' + str(dy) + '_k' + str(k) + '.png')
    elif debug == 'plot':
        plot_image(sb_img, cmap='gray')
    return device, sb_img
# Find NIR image

import os
import numpy as np
from . import print_image
from . import plot_image


def output_mask(device, img, mask, filename, outdir=None, mask_only=False, debug=None):
    """Prints ori image and mask to directories.

    Inputs:
    device   = pipeline step counter
    img = original image, read in with plantcv function read_image
    mask  = binary mask image (single chanel)
    filename = vis image file name (output of plantcv read_image function)
    outdir = output directory
    debug    = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device   = device number
    imgpath = path to image
    maskpath path to mask

    :param img: array
    :param mask: array
    :param filename: str
    :param outdir: str
    :param device: int
    :param debug: str
    :return device: int
    :return imgpath: str
    :return maskpath: str

    """

    device += 1
    analysis_images = []

    if outdir == None:
        directory = os.getcwd()
    else:
        directory = outdir

    if mask_only == False:
        path = os.path.join(str(directory), "ori-images")

        if os.path.exists(path) == True:
            imgpath = os.path.join(str(path), str(filename))
            print_image(img, imgpath)
            analysis_images.append(['IMAGE', 'ori-img', imgpath])

        else:
            os.mkdir(path)
            imgpath = os.path.join(str(path), str(filename))
            print_image(img, imgpath)
            analysis_images.append(['IMAGE', 'ori-img', imgpath])

        path1 = os.path.join(str(directory), "mask-images")

        if os. path.exists(path1) == True:
            maskpath = os.path.join(str(path1), str(filename))
            print_image(mask, maskpath)
            analysis_images.append(['IMAGE', 'mask', maskpath])
        else:
            os.mkdir(path1)
            maskpath = os.path.join(str(path1), str(filename))
            print_image(mask, maskpath)
            analysis_images.append(['IMAGE', 'mask', maskpath])

        if debug == 'print':
            print_image(img, (str(device) + '_ori-img.png'))
            print_image(mask, (str(device) + '_mask-img.png'))

        elif debug == 'plot':
            if len(np.shape(img)) == 3:
                plot_image(img)
                plot_image(mask, cmap='gray')
            else:
                plot_image(img, cmap='gray')
                plot_image(mask, cmap='gray')

        return device, imgpath, maskpath, analysis_images

    else:
        path1 = os.path.join(str(directory), "mask-images")

        if os.path.exists(path1) == True:
            maskpath = os.path.join(str(path1), str(filename))
            print_image(mask, maskpath)
            analysis_images.append(['IMAGE', 'mask', maskpath])
        else:
            os.mkdir(path1)
            maskpath = os.path.join(str(path1), str(filename))
            print_image(mask, maskpath)
            analysis_images.append(['IMAGE', 'mask', maskpath])

        if debug == 'print':
            print_image(mask, (str(device) + '_mask-img.png'))
        elif debug == 'plot':
                plot_image(mask, cmap='gray')

        return device, maskpath, analysis_images
# Analyzes an object and outputs numeric properties

import cv2
import numpy as np
from . import print_image
from . import plot_image


def analyze_object(img, imgname, obj, mask, device, debug=None, filename=False):
    """Outputs numeric properties for an input object (contour or grouped contours).

    Inputs:
    img             = image object (most likely the original), color(RGB)
    imgname         = name of image file.  (No longer used; kept for compatibility)
    obj             = single or grouped contour object
    mask            = binary image to use as mask for moments analysis
    device          = device number. Used to count steps in the pipeline
    debug           = None, print, or plot. Print = save to file, Plot = print to screen.
    filename        = False or image name. If defined print image

    Returns:
    device          = device number
    shape_header    = shape data table headers
    shape_data      = shape data table values
    analysis_images = list of output images

    :param img: numpy array
    :param imgname: str
    :param obj: list
    :param mask: numpy array
    :param device: int
    :param debug: str
    :param filename: str
    :return:
    """

    device += 1

    # Valid objects can only be analyzed if they have >= 5 vertices
    if len(obj) < 5:
        return device, None, None, None

    ori_img = np.copy(img)
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
    else:
        ix, iy = np.shape(img)
    size = ix, iy, 3
    size1 = ix, iy
    background = np.zeros(size, dtype=np.uint8)
    background1 = np.zeros(size1, dtype=np.uint8)
    background2 = np.zeros(size1, dtype=np.uint8)

    # Check is object is touching image boundaries (QC)
    frame_background = np.zeros(size1, dtype=np.uint8)
    frame = frame_background + 1
    frame_contour, frame_heirarchy = cv2.findContours(frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    ptest = []
    vobj = np.vstack(obj)
    for i, c in enumerate(vobj):
        xy = tuple(c)
        pptest = cv2.pointPolygonTest(frame_contour[0], xy, measureDist=False)
        ptest.append(pptest)
    in_bounds = all(c == 1 for c in ptest)

    # Convex Hull
    hull = cv2.convexHull(obj)
    hull_vertices = len(hull)
    # Moments
    #  m = cv2.moments(obj)
    m = cv2.moments(mask, binaryImage=True)
    # Properties
    # Area
    area = m['m00']

    if area:
        # Convex Hull area
        hull_area = cv2.contourArea(hull)
        # Solidity
        solidity = 1
        if int(hull_area) != 0:
            solidity = area / hull_area
        # Perimeter
        perimeter = cv2.arcLength(obj, closed=True)
        # x and y position (bottom left?) and extent x (width) and extent y (height)
        x, y, width, height = cv2.boundingRect(obj)
        # Centroid (center of mass x, center of mass y)
        cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
        # Ellipse
        center, axes, angle = cv2.fitEllipse(obj)
        major_axis = np.argmax(axes)
        minor_axis = 1 - major_axis
        major_axis_length = axes[major_axis]
        minor_axis_length = axes[minor_axis]
        eccentricity = np.sqrt(1 - (axes[minor_axis] / axes[major_axis]) ** 2)

        # Longest Axis: line through center of mass and point on the convex hull that is furthest away
        cv2.circle(background, (int(cmx), int(cmy)), 4, (255, 255, 255), -1)
        center_p = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)
        ret, centerp_binary = cv2.threshold(center_p, 0, 255, cv2.THRESH_BINARY)
        centerpoint, cpoint_h = cv2.findContours(centerp_binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

        dist = []
        vhull = np.vstack(hull)

        for i, c in enumerate(vhull):
            xy = tuple(c)
            pptest = cv2.pointPolygonTest(centerpoint[0], xy, measureDist=True)
            dist.append(pptest)

        abs_dist = np.absolute(dist)
        max_i = np.argmax(abs_dist)

        caliper_max_x, caliper_max_y = list(tuple(vhull[max_i]))
        caliper_mid_x, caliper_mid_y = [int(cmx), int(cmy)]

        xdiff = float(caliper_max_x - caliper_mid_x)
        ydiff = float(caliper_max_y - caliper_mid_y)

        if xdiff != 0:
            slope = (float(ydiff / xdiff))
        if xdiff == 0:
            slope = 1
        b_line = caliper_mid_y - (slope * caliper_mid_x)

        if slope == 0:
            xintercept = 0
            xintercept1 = 0
            yintercept = 'none'
            yintercept1 = 'none'
            cv2.line(background1, (iy, caliper_mid_y), (0, caliper_mid_y), (255), 1)
        else:
            xintercept = int(-b_line / slope)
            xintercept1 = int((ix - b_line) / slope)
            yintercept = 'none'
            yintercept1 = 'none'
            if 0 <= xintercept <= iy and 0 <= xintercept1 <= iy:
                cv2.line(background1, (xintercept1, ix), (xintercept, 0), (255), 1)
            elif xintercept < 0 or xintercept > iy or xintercept1 < 0 or xintercept1 > iy:
                if xintercept < 0 and 0 <= xintercept1 <= iy:
                    yintercept = int(b_line)
                    cv2.line(background1, (0, yintercept), (xintercept1, ix), (255), 1)
                elif xintercept > iy and 0 <= xintercept1 <= iy:
                    yintercept1 = int((slope * iy) + b_line)
                    cv2.line(background1, (iy, yintercept1), (xintercept1, ix), (255), 1)
                elif 0 <= xintercept <= iy and xintercept1 < 0:
                    yintercept = int(b_line)
                    cv2.line(background1, (0, yintercept), (xintercept, 0), (255), 1)
                elif 0 <= xintercept <= iy and xintercept1 > iy:
                    yintercept1 = int((slope * iy) + b_line)
                    cv2.line(background1, (iy, yintercept1), (xintercept, 0), (255), 1)
                else:
                    yintercept = int(b_line)
                    yintercept1 = int((slope * iy) + b_line)
                    cv2.line(background1, (0, yintercept), (iy, yintercept1), (255), 1)

        ret1, line_binary = cv2.threshold(background1, 0, 255, cv2.THRESH_BINARY)
        # print_image(line_binary,(str(device)+'_caliperfit.png'))

        cv2.drawContours(background2, [hull], -1, (255), -1)
        ret2, hullp_binary = cv2.threshold(background2, 0, 255, cv2.THRESH_BINARY)
        # print_image(hullp_binary,(str(device)+'_hull.png'))

        caliper = cv2.multiply(line_binary, hullp_binary)
        # print_image(caliper,(str(device)+'_caliperlength.png'))

        caliper_y, caliper_x = np.array(caliper.nonzero())
        caliper_matrix = np.vstack((caliper_x, caliper_y))
        caliper_transpose = np.transpose(caliper_matrix)
        caliper_length = len(caliper_transpose)

        caliper_transpose1 = np.lexsort((caliper_y, caliper_x))
        caliper_transpose2 = [(caliper_x[i], caliper_y[i]) for i in caliper_transpose1]
        caliper_transpose = np.array(caliper_transpose2)

    # else:
    #  hull_area, solidity, perimeter, width, height, cmx, cmy = 'ND', 'ND', 'ND', 'ND', 'ND', 'ND', 'ND'

    # Store Shape Data
    shape_header = [
        'HEADER_SHAPES',
        'area',
        'hull-area',
        'solidity',
        'perimeter',
        'width',
        'height',
        'longest_axis',
        'center-of-mass-x',
        'center-of-mass-y',
        'hull_vertices',
        'in_bounds',
        'ellipse_center_x',
        'ellipse_center_y',
        'ellipse_major_axis',
        'ellipse_minor_axis',
        'ellipse_angle',
        'ellipse_eccentricity'
    ]

    shape_data = [
        'SHAPES_DATA',
        area,
        hull_area,
        solidity,
        perimeter,
        width,
        height,
        caliper_length,
        cmx,
        cmy,
        hull_vertices,
        in_bounds,
        center[0],
        center[1],
        major_axis_length,
        minor_axis_length,
        angle,
        eccentricity
    ]

    analysis_images = []

    # Draw properties
    if area and filename:
        cv2.drawContours(ori_img, obj, -1, (255, 0, 0), 3)
        cv2.drawContours(ori_img, [hull], -1, (0, 0, 255), 3)
        cv2.line(ori_img, (x, y), (x + width, y), (0, 0, 255), 3)
        cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (0, 0, 255), 3)
        cv2.line(ori_img, (tuple(caliper_transpose[caliper_length - 1])), (tuple(caliper_transpose[0])), (0, 0, 255), 3)
        cv2.circle(ori_img, (int(cmx), int(cmy)), 10, (0, 0, 255), 3)
        # Output images with convex hull, extent x and y
        extention = filename.split('.')[-1]
        # out_file = str(filename[0:-4]) + '_shapes.' + extention
        out_file = str(filename[0:-4]) + '_shapes.jpg'
        out_file1 = str(filename[0:-4]) + '_mask.jpg'

        print_image(ori_img, out_file)
        analysis_images.append(['IMAGE', 'shapes', out_file])

        print_image(mask, out_file1)
        analysis_images.append(['IMAGE', 'mask', out_file1])

    else:
        pass

    if debug is not None:
        cv2.drawContours(ori_img, obj, -1, (255, 0, 0), 3)
        cv2.drawContours(ori_img, [hull], -1, (0, 0, 255), 3)
        cv2.line(ori_img, (x, y), (x + width, y), (0, 0, 255), 3)
        cv2.line(ori_img, (int(cmx), y), (int(cmx), y + height), (0, 0, 255), 3)
        cv2.circle(ori_img, (int(cmx), int(cmy)), 10, (0, 0, 255), 3)
        cv2.line(ori_img, (tuple(caliper_transpose[caliper_length - 1])), (tuple(caliper_transpose[0])), (0, 0, 255), 3)
        if debug == 'print':
            print_image(ori_img, (str(device) + '_shapes.jpg'))
        elif debug == 'plot':
            if len(np.shape(img)) == 3:
                plot_image(ori_img)
            else:
                plot_image(ori_img, cmap='gray')

    return device, shape_header, shape_data, analysis_images
__all__ = ['fatal_error', 'print_image', 'plot_image', 'color_palette', 'plot_colorbar', 'apply_mask', 'readimage',
           'laplace_filter', 'sobel_filter', 'scharr_filter', 'hist_equalization', 'plot_hist', 'image_add',
           'image_subtract', 'erode', 'dilate', 'watershed', 'rectangle_mask', 'rgb2gray_hsv', 'rgb2gray_lab',
           'rgb2gray', 'binary_threshold', 'median_blur', 'fill', 'invert', 'logical_and', 'logical_or', 'logical_xor',
           'find_objects', 'define_roi', 'roi_objects', 'object_composition', 'analyze_object', 'analyze_bound',
           'analyze_color', 'analyze_NIR_intensity', 'fluor_fvfm', 'print_results', 'resize', 'flip',
           'crop_position_mask', 'get_nir', 'adaptive_threshold', 'otsu_auto_threshold', 'report_size_marker_area',
           'white_balance', 'triangle_auto_threshold', 'acute_vertex', 'scale_features', 'landmark_reference_pt_dist',
           'x_axis_pseudolandmarks', 'y_axis_pseudolandmarks', 'gaussian_blur', 'cluster_contours',
           'cluster_contour_splitimg', 'rotate_img', 'shift_img', 'output_mask', 'auto_crop',
           'background_subtraction', 'naive_bayes_classifier', 'acute']

from fatal_error import fatal_error
from print_image import print_image
from plot_image import plot_image
from color_palette import color_palette
from plot_colorbar import plot_colorbar
from apply_mask import apply_mask
from readimage import readimage
from laplace_filter import laplace_filter
from sobel_filter import sobel_filter
from scharr_filter import scharr_filter
from hist_equalization import hist_equalization
from plot_hist import plot_hist
from image_add import image_add
from image_subtract import image_subtract
from erode import erode
from dilate import dilate
from watershed import watershed_segmentation
from rectangle_mask import rectangle_mask
from rgb2gray_hsv import rgb2gray_hsv
from rgb2gray_lab import rgb2gray_lab
from rgb2gray import rgb2gray
from binary_threshold import binary_threshold
from median_blur import median_blur
from fill import fill
from invert import invert
from logical_and import logical_and
from logical_or import logical_or
from logical_xor import logical_xor
from find_objects import find_objects
from define_roi import define_roi
from roi_objects import roi_objects
from object_composition import object_composition
from analyze_object import analyze_object
from analyze_bound import analyze_bound
from analyze_color import analyze_color
from analyze_NIR_intensity import analyze_NIR_intensity
from fluor_fvfm import fluor_fvfm
from print_results import print_results
from resize import resize
from flip import flip
from crop_position_mask import crop_position_mask
from get_nir import get_nir
from adaptive_threshold import adaptive_threshold
from otsu_auto_threshold import otsu_auto_threshold
from report_size_marker_area import report_size_marker_area
from white_balance import white_balance
from triangle_auto_threshold import triangle_auto_threshold
from acute_vertex import acute_vertex
from scale_features import scale_features
from landmark_reference_pt_dist import landmark_reference_pt_dist
from x_axis_pseudolandmarks import x_axis_pseudolandmarks
from y_axis_pseudolandmarks import y_axis_pseudolandmarks
from gaussian_blur import gaussian_blur
from cluster_contours import cluster_contours
from cluster_contour_splitimg import cluster_contour_splitimg
from rotate_img import rotate_img
from shift_img import shift_img
from output_mask_ori_img import output_mask
from auto_crop import auto_crop
from background_subtraction import background_subtraction
from naive_bayes_classifier import naive_bayes_classifier
from acute import acute

# add new functions to end of lists
#!/usr/bin/env python

import argparse
import sys, os
import re
import shutil

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get file names to run FASTQC over")
  parser.add_argument("-d", "--directory", help="directory to run script over.")
  parser.add_argument("-o", "--outdir", help="out directory to move files to")
  args = parser.parse_args()
  return args

def append_img_name(directory,outdir):
    dirs=os.listdir(directory)
    
    dirname=str(directory)
    dirsplit=dirname.split('/')
    experiment=(dirsplit[-1])
    
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    for x in dirs:
        path=str(directory)+"/"+str(x)
        #print(path)
        if os.path.isdir(path) == True:
            files=os.listdir(path)
            outdirpath=str(outdir)+"/"+str(experiment)+"/"+str(x)
            os.makedirs(outdirpath)
            for i in files:
                filepath=str(path)+"/"+str(i)
                outfilepath=str(outdirpath)+"/"+str(experiment)+"_"+str(x)+"_"+str(i)
                shutil.copyfile(filepath,outfilepath)
                print(filepath)
                print(outfilepath)
            
    
### Main pipeline
def main():
  # Get options
  args = options()
  
  append_img_name(args.directory,args.outdir)
  

if __name__ == '__main__':
  main()
# Watershed Se detection function
# This function is based on code contributed by Suxing Liu, Arkansas State University.
# For more information see https://github.com/lsx1980/Leaf_count

import cv2
import numpy as np
from scipy import ndimage as ndi
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from . import print_image
from . import plot_image
from . import apply_mask
from . import color_palette


def watershed_segmentation(device, img, mask, distance=10, filename=False, debug=None):
    """Uses the watershed algorithm to detect boundary of objects. Needs a marker file which specifies area which is
       object (white), background (grey), unknown area (black).

    Inputs:
    device              = device number. Used to count steps in the pipeline
    img                 = image to perform watershed on needs to be 3D (i.e. np.shape = x,y,z not np.shape = x,y)
    mask                = binary image, single channel, object in white and background black
    distance            = min_distance of local maximum
    filename            = if user wants to output analysis images change filenames from false
    debug               = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device              = device number
    watershed_header    = shape data table headers
    watershed_data      = shape data table values
    analysis_images     = list of output images

    :param device: int
    :param img: numpy array
    :param mask: numpy array
    :param distance: int
    :param filename: str
    :param debug: str
    :return device: int
    :return watershed_header: list
    :return watershed_data: list
    :return analysis_images: list
    """

    dist_transform = cv2.distanceTransform(mask, cv2.cv.CV_DIST_L2, maskSize=0)

    localMax = peak_local_max(dist_transform, indices=False, min_distance=distance, labels=mask)

    markers = ndi.label(localMax, structure=np.ones((3, 3)))[0]
    dist_transform1 = -dist_transform
    labels = watershed(dist_transform1, markers, mask=mask)

    img1 = np.copy(img)

    for x in np.unique(labels):
        rand_color = color_palette(len(np.unique(labels)))
        img1[labels == x] = rand_color[x]

    device, img2 = apply_mask(img1, mask, 'black', device, debug=None)

    joined = np.concatenate((img2, img), axis=1)

    estimated_object_count = len(np.unique(markers)) - 1

    analysis_images = []
    if filename != False:
        out_file = str(filename[0:-4]) + '_watershed.jpg'
        print_image(joined, out_file)
        analysis_images.append(['IMAGE', 'watershed', out_file])

    watershed_header = (
        'HEADER_WATERSHED',
        'estimated_object_count'
    )

    watershed_data = (
        'WATERSHED_DATA',
        estimated_object_count
    )

    if debug == 'print':
        print_image(dist_transform, str(device) + '_watershed_dist_img.png')
        print_image(joined, str(device) + '_watershed_img.png')
    elif debug == 'plot':
        plot_image(dist_transform, cmap='gray')
        plot_image(joined)

    return device, watershed_header, watershed_data, analysis_images
#!/usr/bin/env python
import argparse
import sys, os
import sqlite3 as sq
import plantcv as pcv

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Extract FLU Fv/Fm data from an SQLite database")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.", required=True)
  parser.add_argument("-o", "--outfile", help="Output text file.", required=True)
  args = parser.parse_args()
  return args

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Main pipeline
def main():
  # Get options
  args = options()
  
  # Does the database exist?
  if not os.path.exists(args.database):
    pcv.fatal_error("The database file " + str(args.database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(args.database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Open output file
  try:
    out = open(args.outfile, 'w')
  except IOError:
    print("IO error")
    
  # Replace the row_factory result constructor with a dictionary constructor
  #connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str
  
  # Database handler
  db = connect.cursor()
  
  # Retrieve snapshots and process data
  db.execute('SELECT snapshots.image_id,run_id,plant_id,datetime,camera,frame,zoom,lifter,image_path,bins,peak_bin,median,fdark_qc,fvfm FROM `snapshots` INNER JOIN `flu_fvfm` ON `snapshots`.`image_id` = `flu_fvfm`.`image_id`')
  names = list(map(lambda x: x[0], db.description))
  bins = 0
  names[-1] = 's0'
  for row in db.fetchall():
    if bins == 0:
      bins = row[9]
      for s in range(1,bins):
        names.append('s' + str(s))
      out.write(','.join(map(str,names)) + '\n')
    out.write(','.join(map(str, row)) + '\n')
  

if __name__ == '__main__':
  main()
# RGB -> HSV -> Gray

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def rgb2gray_hsv(img, channel, device, debug=None):
    """Convert an RGB color image to HSV colorspace and return a gray image (one channel).

    Inputs:
    img     = image object, RGB colorspace
    channel = color subchannel (h = hue, s = saturation, v = value/intensity/brightness)
    device  = device number. Used to count steps in the pipeline
    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device    = device number
    h | s | v = image from single HSV channel

    :param img: numpy array
    :param channel: str
    :param device: int
    :param debug: str
    :return device: int
    :return channel: numpy array
    """
    # Auto-increment the device counter
    device += 1

    # The allowable channel inputs are h, s or v
    names = {"h": "hue", "s": "saturation", "v": "value"}
    if channel not in names:
        fatal_error("Channel " + str(channel) + " is not h, s or v!")

    # Convert the input BGR image to HSV colorspace
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    # Split HSV channels
    h, s, v = cv2.split(hsv)
    # Create a channel dictionaries for lookups by a channel name index
    channels = {"h": h, "s": s, "v": v}

    if debug == "print":
        print_image(channels[channel], str(device) + "_hsv_" + names[channel] + ".png")
    elif debug == "plot":
        plot_image(channels[channel], cmap="gray")

    return device, channels[channel]
# Function to return feature scaled points

import cv2
import numpy as np
from . import plot_image


def scale_features(obj, mask, points, boundary_line, device, debug=None):
    """scale_features: returns feature scaled points

    This is a function to transform the coordinates of landmark points onto a common scale (0 - 1.0).

    Inputs:
    obj           = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask          = this is a binary image. The object should be white and the background should be black
    points        = the points to scale
    boundary_line = A vertical coordinate that denotes the height of the plant pot, the coordinates of this reference
                    point is also rescaled
    device        = a counter variable
    debug         = True/False. If True, print image

    :param obj: ndarray
    :param mask: ndarray
    :param points: ndarray
    :param boundary_line: int
    :param device: int
    :param debug: str
    :return:
    """
    device += 1
    # Get the dimensions of the image from the binary thresholded object (mask)
    if not np.any(mask) or not np.any(obj):
        rescaled = ('NA', 'NA')
        centroid_scaled = ('NA', 'NA')
        boundary_line_scaled = ('NA', 'NA')
        return device, rescaled, centroid_scaled, boundary_line_scaled
    iy, ix = np.shape(mask)
    x, y, width, height = cv2.boundingRect(obj)
    m = cv2.moments(mask, binaryImage=True)
    cmx, cmy = (m['m10'] / m['m00'], m['m01'] / m['m00'])
    # Convert the boundary line position (top of the pot) into a coordinate on the image
    if boundary_line != 'NA':
        line_position = int(iy) - int(boundary_line)
        bly = line_position
    else:
        bly = cmy
    blx = cmx
    # Maximum and minimum values of the object
    Ymax = y
    Ymin = y + height
    Xmin = x
    Xmax = x + width
    # Scale the coordinates of each of the feature locations
    # Feature scaling X' = (X - Xmin) / (Xmax - Xmin)
    # Feature scaling Y' = (Y - Ymin) / (Ymax - Ymin)
    rescaled = []
    for p in points:
        xval = float(p[0, 0] - Xmin) / float(Xmax - Xmin)
        yval = float(p[0, 1] - Ymin) / float(Ymax - Ymin)
        scaled_point = (xval, yval)
        rescaled.append(scaled_point)
    # Lets rescale the centroid
    cmx_scaled = float(cmx - Xmin) / float(Xmax - Xmin)
    cmy_scaled = float(cmy - Ymin) / float(Ymax - Ymin)
    centroid_scaled = (cmx_scaled, cmy_scaled)
    # Lets rescale the boundary_line
    blx_scaled = float(blx - Xmin) / float(Xmax - Xmin)
    bly_scaled = float(bly - Ymin) / float(Ymax - Ymin)
    boundary_line_scaled = (blx_scaled, bly_scaled)
    # If debug is 'True' plot an image of the scaled points on a black background
    if debug == 'print':
        # Make a decent size blank image
        scaled_img = np.zeros((1500, 1500, 3), np.uint8)
        plotter = np.array(rescaled)
        # Multiple the values between 0 - 1.0 by 1000 so you can plot on the black image
        plotter = plotter * 1000
        # For each of the coordinates plot a circle where the point is
        # (+250 helps center the object in the middle of the blank image)
        for i in plotter:
            x, y = i.ravel()
            cv2.circle(scaled_img, (int(x) + 250, int(y) + 250), 15, (255, 255, 255), -1)
        cv2.circle(scaled_img, (int(cmx_scaled * 1000) + 250, int(cmy_scaled * 1000) + 250), 25, (0, 0, 255), -1)
        cv2.circle(scaled_img, (int(blx_scaled * 1000) + 250, int(bly_scaled * 1000) + 250), 25, (0, 255, 0), -1)
        # Because the coordinates increase as you go down and to the right on the
        # image you need to flip the object around the x-axis
        flipped_scaled = cv2.flip(scaled_img, 0)
        cv2.imwrite((str(device) + '_feature_scaled.png'), flipped_scaled)
    # Return the transformed points
    if debug == 'plot':
        # Make a decent size blank image
        scaled_img = np.zeros((1500, 1500, 3), np.uint8)
        plotter = np.array(rescaled)
        # Multiple the values between 0 - 1.0 by 1000 so you can plot on the black image
        plotter = plotter * 1000
        # For each of the coordinates plot a circle where the point is (+250 helps center
        # the object in the middle of the blank image)
        for i in plotter:
            x, y = i.ravel()
            cv2.circle(scaled_img, (int(x) + 250, int(y) + 250), 15, (255, 255, 255), -1)
        cv2.circle(scaled_img, (int(cmx_scaled * 1000) + 250, int(cmy_scaled * 1000) + 250), 25, (0, 0, 255), -1)
        cv2.circle(scaled_img, (int(blx_scaled * 1000) + 250, int(bly_scaled * 1000) + 250), 25, (0, 255, 0), -1)
        # Because the coordinates increase as you go down and to the right on the
        # image you need to flip the object around the x-axis
        flipped_scaled = cv2.flip(scaled_img, 0)
        plot_image(flipped_scaled)

    # Return the transformed points
    return device, rescaled, centroid_scaled, boundary_line_scaled
# Make masking rectangle

import cv2
import numpy as np
from . import print_image
from . import plot_image


def rectangle_mask(img, p1, p2, device, debug=None, color="black"):
    """Takes an input image and returns a binary image masked by a rectangular area denoted by p1 and p2. Note that
       p1 = (0,0) is the top left hand corner bottom right hand corner is p2 = (max-value(x), max-value(y)).

    Inputs:
    img       = image object
    p1        = point 1
    p2        = point 2
    device    = device number. Used to count steps in the pipeline
    debug     = None, print, or plot. Print = save to file, Plot = print to screen.
    color     = black,white, or gray

    Returns:
    device    = device number
    masked      = original image with masked image
    bnk       = binary image
    contour   = object contour vertices
    hierarchy = contour hierarchy list

    :param img: numpy array
    :param p1: tuple
    :param p2: tuple
    :param device: int
    :param debug: str
    :param color: str
    :return device: int
    :return masked:numpy array
    :return bnk: numpy array
    :return contour: list
    :return hierarchy: list
    """

    device += 1
    # get the dimensions of the input image
    if len(np.shape(img)) == 3:
        ix, iy, iz = np.shape(img)
    else:
        ix, iy = np.shape(img)
    size = ix, iy
    # create a blank image of same size
    bnk = np.zeros(size, dtype=np.uint8)
    img1 = np.copy(img)
    # draw a rectangle denoted by pt1 and pt2 on the blank image

    cv2.rectangle(img=bnk, pt1=p1, pt2=p2, color=(255, 255, 255), thickness=-1)
    ret, bnk = cv2.threshold(bnk, 127, 255, 0)
    contour, hierarchy = cv2.findContours(bnk, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    # make sure entire rectangle is within (visable within) plotting region or else it will not fill with
    # thickness = -1. Note that you should only print the first contour (contour[0]) if you want to fill with
    # thickness = -1. otherwise two rectangles will be drawn and the space between them will get filled

    if color == "white":
        cv2.drawContours(bnk, contour, 0, (255, 255, 255), -1)
        cv2.drawContours(img1, contour, 0, (255, 255, 255), -1)
    if color == "black":
        bnk = bnk + 255
        cv2.drawContours(bnk, contour, 0, (0, 0, 0), -1)
        cv2.drawContours(img1, contour, 0, (0, 0, 0), -1)
    if color == "gray":
        cv2.drawContours(bnk, contour, 0, (192, 192, 192), -1)
        cv2.drawContours(img1, contour, 0, (192, 192, 192), -1)
    if debug == 'print':
        print_image(bnk, (str(device) + '_roi.png'))

    elif debug == 'plot':
        if len(np.shape(bnk))==3:
            plot_image(bnk)
            plot_image(img1)
        else:
            plot_image(bnk, cmap="gray")
            plot_image(img1, cmap="gray")
    return device, img1, bnk, contour, hierarchy
# Binary image threshold device

import cv2
from . import print_image
from . import plot_image
from . import fatal_error


def binary_threshold(img, threshold, maxValue, object_type, device, debug=None):
    """Creates a binary image from a gray image based on the threshold value.

    Inputs:
    img         = img object, grayscale
    threshold   = threshold value (0-255)
    maxValue    = value to apply above threshold (usually 255 = white)
    object_type = light or dark
                  - If object is light then standard thresholding is done
                  - If object is dark then inverse thresholding is done
    device      = device number. Used to count steps in the pipeline
    debug       = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device      = device number
    t_img       = thresholded image

    :param img: numpy array
    :param threshold: int
    :param maxValue: int
    :param object_type: str
    :param device: int
    :param debug: str
    :return device: int
    :return t_img: numpy array
    """

    device += 1
    if object_type == 'light':
        ret, t_img = cv2.threshold(img, threshold, maxValue, cv2.THRESH_BINARY)
        if debug == 'print':
            print_image(t_img, (str(device) + '_binary_threshold' + str(threshold) + '.png'))
        elif debug == 'plot':
            plot_image(t_img, cmap='gray')
        return device, t_img
    elif object_type == 'dark':
        ret, t_img = cv2.threshold(img, threshold, maxValue, cv2.THRESH_BINARY_INV)
        if debug == 'print':
            print_image(t_img, (str(device) + '_binary_threshold' + str(threshold) + '_inv.png'))
        elif debug == 'plot':
            plot_image(t_img, cmap='gray')
        return device, t_img
    else:
        fatal_error('Object type ' + str(object_type) + ' is not "light" or "dark"!')
# Background Subtraction:
# Subtracts a background image from a foreground image once.

import cv2
import numpy as np
from . import print_image
from . import plot_image
from . import fatal_error


def background_subtraction(background_image, foreground_image, device, debug=None):
    """Creates a binary image from a background subtraction of the foreground using cv2.BackgroundSubtractorMOG().
    The binary image returned is a mask that should contain mostly foreground pixels.
    The background image should be the same background as the foreground image except not containing the object
    of interest.

    Images must be of the same size and type.
    If not, larger image will be taken and downsampled to smaller image size.
    If they are of different types, an error will occur.

    Inputs:
    background_image       = img object, RGB or binary/grayscale/single-channel
    foreground_image       = img object, RGB or binary/grayscale/single-channel
    device                 = device number. Used to count steps in the pipeline
    debug                  = None, print, or plot. Print = save to file, Plot = print to screen.

    Returns:
    device                 = device number
    fgmask                 = background subtracted foreground image (mask)

    :param background_image: numpy array
    :param foreground_image: numpy array
    :param device: int
    :param debug: str
    :return device: int
    :return fgmask: numpy array
    """

    device += 1
    # Copying images to make sure not alter originals
    bg_img = np.copy(background_image)
    fg_img = np.copy(foreground_image)
    # Checking if images need to be resized or error raised
    if bg_img.shape != fg_img.shape:
        # If both images are not 3 channel or single channel then raise error.
        if len(bg_img.shape) != len(fg_img.shape):
            fatal_error("Images must both be single-channel/grayscale/binary or RGB")
        # Forcibly resizing largest image to smallest image
        print("WARNING: Images are not of same size.\nResizing")
        if bg_img.shape > fg_img.shape:
            width, height = fg_img.shape[1], fg_img.shape[0]
            bg_img = cv2.resize(bg_img, (width, height), interpolation=cv2.INTER_AREA)
        else:
            width, height = bg_img.shape[1], bg_img.shape[0]
            fg_img = cv2.resize(fg_img, (width, height), interpolation=cv2.INTER_AREA)

    # Instantiating the background subtractor, for a single history no default parameters need to be changed.
    bgsub = cv2.BackgroundSubtractorMOG()
    # Applying the background image to the background subtractor first.
    # Anything added after is subtracted from the previous iterations.
    fgmask = bgsub.apply(bg_img)
    # Applying the foreground image to the background subtractor (therefore removing the background)
    fgmask = bgsub.apply(fg_img)

    # Debug options
    if debug == "print":
        print_image(fgmask, "{0}_background_subtraction.png".format(device))
    elif debug == "plot":
        plot_image(fgmask, cmap="gray")
    
    return device, fgmask
#!/usr/bin/env python

import argparse
import numpy as np
import sys, os
import sqlite3 as sq
import plantcv as pcv
from shutil import copy
import datetime
import re

### Parse command-line arguments
def options():
  parser = argparse.ArgumentParser(description="Get images from an SQLite database and some input information")
  parser.add_argument("-d", "--database", help="SQLite database file from plantcv.")
  parser.add_argument("-f", "--file", help="text file, tab seperated, containing plant IDs and other information.", required=True)
  parser.add_argument("-o", "--outdir", help="Output directory.", required=False)
  parser.add_argument("--vis", help="Images are class VIS.", action='store_true')
  parser.add_argument("--nir", help="Images are class NIR.", action='store_true')
  parser.add_argument("--flu", help="Images are class FLU.", action='store_true')
  parser.add_argument("-t", "--type", help="Image format type.", required=True)
  args = parser.parse_args()
  return args


### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d
  
  
### Get images with more information
def dict_plant_info(infile):  
  table=np.genfromtxt(infile, dtype='str', delimiter='\t')
  table1=np.asarray(table)
  
  query=[]
  
  header=table[0].tolist()
  tablenohead=table[1:]
  y,x=tablenohead.shape
  columncount=list(range(0,x))
  split_table=np.vsplit(tablenohead,y)
  split_table= [l[0] for l in split_table]
  
  for row in split_table:
    where=[]
    col=np.hsplit(row,x)
    col=[l[0] for l in col]
    for i,h in enumerate(columncount):
      where.append(str(header[i])+'='+"'"+str(col[i]+"'"))
    where_and=' and '.join(map(str,where))
    query.append(where_and)
  return query

### Dictionary factory for SQLite query results
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

### Database image lookup method
def db_lookup(database, outdir, query, type, vis=False, nir=False, flu=False):
  # Does the database exist?
  if not os.path.exists(database):
    pcv.fatal_error("The database file " + str(database) + " does not exist");
  
  # Open a connection
  try:
    connect=sq.connect(database)
  except sq.Error, e:
    print("Error %s:" % e.args[0])
  
  # Replace the row_factory result constructor with a dictionary constructor
  connect.row_factory = dict_factory
  # Change the text output format from unicode to UTF-8
  connect.text_factory=str

   # Database handler
  db = connect.cursor()
  
  for row in query:
    print row
    
    query1='select * from snapshots where ' + str(row)
    print query1
    for row in (db.execute(query1)):
      dt = datetime.datetime.fromtimestamp(row['datetime']).strftime('%Y-%m-%d %H:%M:%S')
      if (vis):
        if (row['camera'] == 'vis_sv' or row['camera'] == 'vis_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
          #print(args.outdir + '/' + row['plant_id'])
      if (nir):
        if (row['camera'] == 'nir_sv' or row['camera'] == 'nir_tv'):
          img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + dt + '.' + type
          copy(row['image_path'], img_name)
      if (flu):
        if (row['camera'] == 'flu_tv'):
          images = row['image_path'].split(',')
          for i in enumerate(images):
            img_name = outdir + '/' + row['plant_id'] + '_' + row['camera'] + '_' + str(row['frame']) + '_z' + str(row['zoom']) + '_h' + str(row['lifter']) + '_' + str(i) + '_' + dt + '.' + type
            copy(images[i], outdir)

### Main pipeline
def main():
  # Get options
  args = options()
  
  query=dict_plant_info(args.file)
  db_lookup(args.database, args.outdir, query, args.type, args.vis, args.nir, args.flu)
  

if __name__ == '__main__':
  main()"""
This file is part of Image Harvest.

Image Harvest is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Image Harvest is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Image Harvest.  If not, see <http://www.gnu.org/licenses/>.
"""
import math
import os
import sqlite3
import conf
import datetime
import numpy as np
import pandas
import ih.imgproc
import cv2
import json
import traceback
import shutil
import subprocess
import itertools
from scipy import stats

class Stats:

    def __init__(self, db):
        self.db = db
        self.conn = self._openConnection()
        return

    def _openConnection(self, db = None):
        db = self.db if not db else db
        if os.path.isfile(db):
            conn = sqlite3.connect(db, check_same_thread = False)
            conn.row_factory = sqlite3.Row
            result = conn.execute("select pegasusid from images limit 0,1")
            if not result.fetchone():
                raise Exception("Database '%s' has no entries!" % (db,))
            # Have to enable foreign keys EVERY time.
            conn.execute("PRAGMA foreign_keys = ON")
            conn.commit()
        else:
            raise Exception("Database file '%s' does not exist." % (db,))
        return conn

    def _closeConnection(self, conn = None):
        conn = self.conn if not conn else conn
        conn.close()
        return

    def _tableExists(self, tablename, conn = None):
        conn = self.conn if not conn else conn
        result = conn.execute("select name from sqlite_master where type='table' and name=?", (tablename,))
        if not result.fetchone():
            return False
        else:
            return True

    def _validate(self, funcname, intable, outtable, overwrite, conn = None):
        conn = self.conn if not conn else conn
        if funcname in conf.statsColumns:
            if self._tableExists(intable):
                cols = [x[1] for x in conn.execute("PRAGMA table_info('" + intable + "')")]
                if set(conf.statsColumns[funcname]["required"]) <= set(cols):
                    if self._tableExists(outtable, conn) and not overwrite:
                        raise Exception("Output table '" + outtable + "' already exists.")
                else:
                    raise Exception("Table '" + intable + "' doesn't have all the required columns: '" + str(conf.statsColumns[funcname]["required"]) + "'.")
            else:
                raise Exception("Table '" + intable + "' doesn't exist.")
        else:
            raise Exception("Function '" + funcname + "' doesn't exist.")
        return

    def _massage(self, table, headers, imtypes = None, conn = None):
        conn = self.conn if not conn else conn
        if not imtypes:
            imtypes = self._getImageTypes(table, conn)
        elif not isinstance(imtypes, list):
            imtypes = [imtypes]

        for header in headers:
            self.conn.execute("update " + table + " set " + header + "=? where " + header + " is null and (" + "or".join(["imtype=?" for x in imtypes]) + ")", (0,) + tuple(imtypes))
        self.conn.commit()
        return

    def _columnExists(self, column, tablename, conn = None):
        conn = self.conn if not conn else conn
        if column in [row["name"] for row in self.conn.execute("PRAGMA table_info('" + tablename + "')")]:
            return True
        else:
            return False

    def _addColumn(self, column, tablename = "images", conn = None):
        conn = self.conn if not conn else conn
        if not self._columnExists(column, tablename, conn):
            conn.execute("alter table '" + tablename + "' add column " + column + ";")
            conn.commit()
        return

    def _addKeyColumn(self, parent, parentCol, child, childCol, conn = None):
        conn = self.conn if not conn else conn
        if parentCol not in [row["name"] for row in conn.execute("PRAGMA table_info('" + parent + "');")] and childCol in [row["name"] for row in conn.execute("PRAGMA table_info('" + child + "');")]:
            conn.execute("alter table '" + parent + "' add column '" + parentCol + "' REFERENCES '" + child + "'('" + childCol + "');")
            conn.commit()
        return

    def _createTable(self, funcname, intable, outtable, overwrite, conn = None):
        conn = self.conn if not conn else conn
        tablestr = "(pegasusid INTEGER PRIMARY KEY"
        cols = [x[1] for x in conn.execute("pragma table_info('" + intable + "')")]
        if "all" not in conf.statsColumns[funcname]["exclude"]:
            for col in cols:
                if col not in conf.statsColumns[funcname]["exclude"] and col[:3] != "ref":
                    tablestr += "," + str(col)
        for col in conf.statsColumns[funcname]["add"]:
            tablestr += "," + str(col)
        tablestr += ")"
        if overwrite:
            if conf.statsColumns[funcname]["ref"]:
                if self._columnExists("ref_" + outtable, intable, conn):
                    conn.execute("update " + intable + " set ref_" + outtable + "=? where ref_" + outtable + " is not null", (None,))
                    conn.commit()
            conn.execute("DROP TABLE IF EXISTS " + outtable)
            conn.commit()
        conn.execute("CREATE TABLE " + outtable + " " + tablestr)
        conn.commit()
        if conf.statsColumns[funcname]["ref"]:
            self._addKeyColumn(intable, "ref_" + outtable, outtable, "pegasusid", conn)
        return

    def _getHeaders(self, table, conn = None):
        conn = self.conn if not conn else conn
        return [str(x[1]) for x in conn.execute("pragma table_info(" + table + ")") if x[1] != "pegasusid"]

    def _getIds(self, table, conn = None):
        conn = self.conn if not conn else conn
        return [str(x[0]) for x in conn.execute("select distinct id from " + table)]

    def _getImageTypes(self, table, conn = None):
        conn = self.conn if not conn else conn
        return [str(x[0]) for x in conn.execute("select distinct imtype from " + table)]

    def _getDates(self, table, conn = None):
        conn = self.conn if not conn else conn
        return [str(x[0]) for x in conn.execute("select distinct date from " + table)]

    def _checkNull(self, value):
        return True if value is None or str(value).lower() == "none" else False

    def _listToSql(self, l):
        return str(l).replace("[", "(").replace("]", ")")

    def _loadLemnaData(self, dataFile, dataHeaders):
        """
        :param dataFile: The input dataFile
        :type dataFile: str
        :param dataHeaders: The input data headers
        :type dataHeaders: dict

        dataHeaders should be:
        {
            "id": "Snapshot ID Tag",
            "date": "Snapshot Time Stamp",
            "dateFormat": "%m/%d/%y",
            "metric": "Projected Shoot Area [pixels]"
        }
        """
        if os.path.isfile(dataFile):
            if all(key in dataHeaders for key in ["id", "date", "dateFormat", "metric"]):
                with open(dataFile) as rh:
                    lines = rh.readlines()
                    firstline = lines[0].strip().split(",")
                    if dataHeaders["id"] in firstline:
                        if dataHeaders["date"] in firstline:
                            if dataHeaders["metric"] in firstline:
                                idIndex = firstline.index(dataHeaders["id"])
                                dateIndex = firstline.index(dataHeaders["date"])
                                metricIndex = firstline.index(dataHeaders["metric"])
                                lemnaData = {}
                                for line in lines[1:]:
                                    info = line.strip().split(",")
                                    if (info[metricIndex] != "NA"):
                                        id = info[idIndex][-8:]
                                        date = datetime.datetime.strptime(info[dateIndex].split()[0], dataHeaders["dateFormat"]).strftime("%Y-%m-%d")
                                        try:
                                            metric = float(info[metricIndex])
                                            if id not in lemnaData:
                                                lemnaData[id] = {}
                                            if date not in lemnaData[id]:
                                                lemnaData[id][date] = {}
                                            lemnaData[id][date]["metric"] = metric
                                        except:
                                            pass
                                return lemnaData
                            else:
                                raise Exception("Metric Column: ",dataHeaders["metric"]," does not exist.")
                        else:
                            raise Exception("Date Column: ",dataHeaders["date"]," does not exist.")
                    else:
                        raise Exception("Id Column: ",dataHeaders["id"]," does not exist.")
            else:
                raise Exception("Data Dictionary does not have all required keys ['id', 'date', 'dateFormat', 'metric']")
        else:
            raise Exception("Data File: " + dataFile + " does not exist!")
        return

    def loadSql(self, dblist):
        for i,f in enumerate(dblist):
            if os.path.isfile(f):
                conn = self._openConnection(f)
                headers = self._getHeaders("images", conn)
                for cols in (headers[pos:pos + 500] for pos in xrange(0, len(headers), 500)):
                    for col in cols:
                        self._addColumn(col, "images")
                    result = conn.execute("select pegasusid," + ",".join(cols) + " from images")
                    query = "update images set " + ",".join([col + "=?" for col in cols]) + " where pegasusid=?"
                    values = [tuple([row[col] for col in cols] + [row["pegasusid"]]) for row in result]
                    self.conn.executemany(query, values)
                    self.conn.commit()
                self._closeConnection(conn)
            else:
                print "DB File: '%s' does not exist." % (f,)
        return

    def logErrors(self, logfile, table = "images"):
        """
        :param logfile: The logfile to write errors to
        :type logfile: str
        :param table: The table to load errors from
        :type table: str

        This function writes all errors from a given table to a log file,
        usually used at the end of image processing to write all images that
        did not process correctly.
        """
        with open(logfile, "w") as wh:
            wh.write("Image processing error log\n")
            wh.write("==========================\n")
            result = self.conn.execute("select pegasusid,path,error from '" + table + "' where error is not null")
            firstrow = result.fetchone()
            if firstrow:
                wh.write("Image with id '%s' loaded from input path '%s' was not processed successfully.\n" % (firstrow["pegasusid"], firstrow["path"]))
                for row in result:
                    wh.write("Image with id '%s' loaded from input path '%s' was not processed successfully.\n" % (row["pegasusid"], row["path"]))
            else:
                wh.write("All images processed successfully!  Nice job!\n")
        return

    def makePublic(self, targetFolder, process = False):
        imtypes = ["rgbsv", "rgbtv", "fluosv"]
        query = "select path from images where imtype in " + self._listToSql(imtypes)
        print query
        result = self.conn.execute(query)
        for row in result:
            target = "/iplant/home/shared/walia_rice_salt/" + targetFolder + "/" + "/".join(row["path"].split("/")[5:-1]) + "/0_0.png"
            print target
            if process:
                for utype in ["anonymous", "public"]:
                    subprocess.call(["ichmod","read", utype, target])
        return

    def dataToPlantcv(self, folder, ids = None, imtypes = None, dates = None):
        if not os.path.isdir(folder):
            ids = ids if ids else self._getIds("images")
            imtypes = imtypes if imtypes else self._getImageTypes("images")
            dates = dates if dates else self._getDates("images")
            query = "select id,date,imgname,imtype,path from images where id in " + self._listToSql(ids) + " and imtype in " + self._listToSql(imtypes) + " and date in " + self._listToSql(dates)
            print query
            result = self.conn.execute(query)
            os.makedirs(folder)
            for row in result:
                id = row["id"].split("-")[0] if "-" in row["id"] else row["id"]
                date_time = row["date"] + " 000"
                imtype = {
                    "rgbsv": "vis_sv_z700",
                    "rgbtv": "vis_tv_z300"
                }[row["imtype"]]
                fname = "%s-%s-%s-%s.png" % (id, date_time, row["imgname"], imtype)
                shutil.copyfile(row["path"], folder + "/" + fname)
        else:
            print "Folder already exists."
        return

    def dataToIAP(self, folder, ids = None, imtypes = None, dates = None):
        if not os.path.isdir(folder):
            ids = ids if ids else self._getIds("images")
            imtypes = imtypes if imtypes else self._getImageTypes("images")
            dates = dates if dates else self._getDates("images")
            query = "select id,date,imgname,imtype,path from images where id in " + self._listToSql(ids) + " and imtype in " + self._listToSql(imtypes) + " and date in " + self._listToSql(dates)
            print query
            result = self.conn.execute(query)
            os.makedirs(folder)
            for row in result:
                if not os.path.isdir(folder + "/" + row["imtype"][:-2]):
                    os.makedirs(folder + "/" + row["imtype"][:-2])
                writeim = "side" if "sv" in row["imtype"] else "top"
                dirname = folder + "/" + row["imtype"][:-2] + "/" + writeim
                if not os.path.isdir(dirname):
                    os.makedirs(dirname)
                fname = "%s_%s_%s_%s.png" % (row["imgname"], writeim, row["id"], row["date"])
                shutil.copyfile(row["path"], dirname + "/" + fname)
        else:
            print "Folder already exists."
        return

    def shootArea(self, intable, outtable, grouping, overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param grouping: Which imtypes to group
        :type grouping: list

        This function adds the numeric values of multiple image types together.
        In general, it is used to combine side view + top view images of the same spectrum.
        If you plan on correlating your data to lemnaTec's data, lemnaTec uses SV1 + SV2 + TV,
        so you should aggregate your data likewise.
        """
        self._validate("shootArea", intable, outtable, overwrite)
        self._createTable("shootArea", intable, outtable, overwrite)
        headers = self._getHeaders(outtable)
        dataHeaders = [header for header in headers if header not in conf.allHeaders and "ref" not in header]
        start = len(headers) - len(dataHeaders)
        """
        map = {
            "genotype": {
                "date": {
                    "treatment": {
                        "writeback": [],
                        "values": ()
                    }
                }
            }
        }
        """
        dataMap = {}
        result = self.conn.execute("select pegasusid," + ",".join(headers) + " from " + intable + " where " + " or ".join(["imtype=?" for x in grouping]), tuple([x for x in grouping]))
        for row in result:
            if row["genotype"] not in dataMap:
                dataMap[row["genotype"]] = {}
            if row["date"] not in dataMap[row["genotype"]]:
                dataMap[row["genotype"]][row["date"]] = {}
            if row["treatment"] not in dataMap[row["genotype"]][row["date"]]:
                dataMap[row["genotype"]][row["date"]][row["treatment"]] = {"writeback": [row["pegasusid"]], "values": tuple([row[x] for x in headers])}
            else:
                dataMap[row["genotype"]][row["date"]][row["treatment"]]["writeback"].append(row["pegasusid"])
                values = []
                for i,(x,y) in enumerate(zip(dataMap[row["genotype"]][row["date"]][row["treatment"]]["values"], tuple([row[x] for x in headers]))):
                    if i < start:
                        values.append(x)
                    else:
                        try:
                            values.append(x + y)
                        except:
                            values.append(None)
                dataMap[row["genotype"]][row["date"]][row["treatment"]]["values"] = tuple(values)

        #Loop through dataMap, insert values and writeback accordingly
        for genotype in dataMap:
            for date in dataMap[genotype]:
                for treatment in dataMap[genotype][date]:
                    lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", dataMap[genotype][date][treatment]["values"]).lastrowid
                    self.conn.executemany("update " + intable + " set ref_" + outtable + "=? where pegasusid=?", tuple([(lastid, x) for x in dataMap[row["genotype"]][row["date"]][row["treatment"]]["writeback"]]))
        self.conn.commit()
        return

    def normalize(self, intable, outtable, column = "pixels", overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param column: The column to normalize information to
        :type column: str

        Normalizes all numerical information to the specific column.  This
        function is usually used with 'pixels' as the specified column, which
        expresses all numeric information as a percentage of the total pixels
        in the image.
        """
        self._validate("normalize", intable, outtable, overwrite)
        if self._columnExists(column, intable):
            conf.statsColumns["normalize"]["exclude"] += [column]
            self._createTable("normalize", intable, outtable, overwrite)
            headers = self._getHeaders(outtable)
            dataHeaders = [header for header in headers if header not in conf.allHeaders and "ref" not in header]
            result = self.conn.execute("select pegasusid," + column + "," + ",".join(headers) + " from " + intable)
            vals = ()
            for row in result:
                vals = []
                for x in headers:
                    if x in conf.allHeaders:
                        vals.append(row[x])
                    else:
                        if not self._checkNull(row[x]):
                            if not self._checkNull(row[column]) and row[column]:
                                vals.append(float(row[x]) / float(row[column]))
                            else:
                                vals.append(None)
                        else:
                            vals.append(None)
                lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", tuple(vals)).lastrowid
                self.conn.execute("update " + intable + " set ref_" + outtable + "=? where pegasusid=?", (lastid, row["pegasusid"]))
            self.conn.commit()
        return

    def extractAll(self, options):
        basepath = os.path.dirname(os.path.dirname(os.path.abspath(self.db) + "/"))
        if "workflows" in options:
            for type in options["workflows"]:
                self._openConnection()
                tmp = self.conn.execute("select pegasusid,experiment,id,date,imtype,imgname from images where imtype=?", (type,))
                result = tmp.fetchall()
                for row in result:
                    finalpath = row["experiment"].replace(" ","") + "/" + row["id"].replace(" ","") + "/" + row["date"].replace(" ","") + "/" + row["imtype"].replace(" ","") + "/" + row["imgname"].replace(" ","") + "/" + row["pegasusid"] + "_" + options["workflows"][type]["inputs"][0] + ".png"
                    #finalpath = row["pegasusid"] + "_" + options["workflows"][type]["inputs"][0] + ".png"
                    self._closeConnection()
                    if os.path.isfile(finalpath):
                        try:
                            plant = ih.imgproc.Image(finalpath, db = self.db, dbid = row["pegasusid"])
                            plant.extractFinalPath()
                            if "--dimensions" in options["workflows"][type]["arguments"]:
                                plant.extractDimensions()
                            if "--pixels" in options["workflows"][type]["arguments"]:
                                plant.extractPixels()
                            if "--moments" in options["workflows"][type]["arguments"]:
                                plant.extractMoments()
                            if "--colors" in options["workflows"][type]["arguments"]:
                                plant.extractColorData()
                            if "--channels" in options["workflows"][type]["arguments"]:
                                plant.extractColorChannels()
                            if "--bins" in options["workflows"][type]["arguments"]:
                                plant.extractBins(options["workflows"][type]["arguments"]["--bins"])
                        except Exception as e:
                            print traceback.format_exc()
                    else:
                        print "PATH DOES NOT EXIST: %s." % (finalpath,)
        if "histogram-bin" in options:
            self._openConnection()
            self.histogramBinning("images", "histogramBins", dict((type,options["workflows"][type]["inputs"][0]) for type in options["workflows"]), options["histogram-bin"]["--group"], options["histogram-bin"]["--chunks"], options["histogram-bin"]["--channels"], True)
        return

    def histogramBinning(self, intable, outtable, grouping, chunks, channels, jsonwrite = False, overwrite = False):
        color_vector = {}
        for name in grouping:
            self._validate("histogramBins", intable, name + "_" + outtable, overwrite)
        for name in grouping:
            self._createTable("histogramBins", intable, name + "_" + outtable, overwrite)
            color_vector[name] = [0, 0, 0]
        map = dict((type, name) for name in grouping for type in grouping[name])
        basequery = "select "
        for x,c in enumerate(["bhist", "ghist", "rhist"]):
            for i in range(0, 256):
                if i == 0 and x == 0:
                    basequery += "SUM(" + c + str(i) + ")"
                else:
                    basequery += ",SUM(" + c + str(i) + ")"
        basequery += " from images where "
        for name in grouping:
            query = basequery + " or ".join(["imtype=?" for x in grouping[name]])
            result = self.conn.execute(query, tuple(grouping[name]))
            data = list(result.fetchone())
            for i in range(0, 3):
                color_vector[name][i] = data[i*256:(i + 1)*256]
        bins = {}
        for name in grouping:
            bins[name] = self._generateBins(self._splitHist(color_vector[name], chunks[name], channels[name]), name)
            self.conn.executemany("insert into " + name + "_" + outtable + " (name, minr, ming, minb, maxr, maxg, maxb) values (?, ?, ?, ?, ?, ?, ?)", [(bin["name"], int(bin["min"][2]), int(bin["min"][1]), int(bin["min"][0]), int(bin["max"][2]), int(bin["max"][1]), int(bin["max"][0])) for bin in bins[name]])
#             for bin in bins[name]:
#                 self.conn.execute("insert into " + name + "_" + outtable + " (name, minr, ming, minb, maxr, maxg, maxb) values (?, ?, ?, ?, ?, ?, ?)", (name + bin["name"], int(bin["min"][2]), int(bin["min"][1]), int(bin["min"][0]), int(bin["max"][2]), int(bin["max"][1]), int(bin["max"][0])))
#                 self.conn.commit()
            self.conn.commit()
        if jsonwrite:
            for name in grouping:
                with open(name + "_hist_bins.json", "w") as wh:
                    json.dump(bins[name], wh)
        return


    def _splitHist(self, hist, chunks, channels):
        returnlist = [ [1], [1], [1] ]
        s = [sum(x) for x in hist]
        #s = [int(x) for x in np.sum(hist, axis = 1)]
        for i in channels:
            tmp = 0
            integral = s[i] / chunks[i]
            for j,item in enumerate(hist[i]):
                tmp += item
                if (tmp > integral):
                    returnlist[i].append(j)
                    tmp = 0
        returnlist[0].append(255)
        returnlist[1].append(255)
        returnlist[2].append(255)
        return returnlist

    def _generateBins(self, vector, type):
        bins = []
        num = 1
        for i in range (0, len(vector[0]) - 1):
            for j in range(0, len(vector[1]) - 1):
                for k in range(0, len(vector[2]) - 1):
                    bins.append({
                        "name": type + "_bin" + str(num),
                        "min": np.array([vector[0][i],vector[1][j],vector[2][k]], dtype=np.uint8).tolist(),
                        "max": np.array([vector[0][i+1],vector[1][j+1],vector[2][k+1]], dtype=np.uint8).tolist()
                    })
                    num += 1
        return bins


    def correlation(self, intable, outtable, dataFile, dataHeaders, overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param dataFile: The csv data file to load
        :type dataFile: str
        :param dataHeaders: Column names for relevant table headers.
        :type dataHeaders: str

        This function correlates all numeric values with values in the given file.
        The input data file is assumed to be in csv format.  In dataHeaders, you must
        specify four keys, 'id', 'date', 'dateFormat', and 'metric'.  Each of these should
        be column names.  Id corresponds to the lemnaTec style identifier ex: '023535-S'.
        All dates are converted into Y-m-d form, you must provide a valid format in 'dateFormat',
        that way the dates can be converted correctly.  The 'metric' column is the actual value you
        want to correlate to.
        """
        self._validate("correlation", intable, outtable, overwrite)
        lemnaData = self._loadLemnaData(dataFile, dataHeaders)
        self._createTable("correlation", intable, outtable, overwrite)
        headers = self._getHeaders(outtable)
        numeric = [h for h in headers if h not in conf.allHeaders]
        for id in lemnaData:
            for imtype in self._getImageTypes(intable):
                x = {}
                y = []
                corr = {}
                result = self.conn.execute("select pegasusid," + ",".join(headers + ["date"]) + " from " + intable + " where id=? and imtype=? order by date", (id,imtype))
                if result.fetchone():
                    for row in result:
                        if row["date"] in lemnaData[id]:
                            y.append(lemnaData[id][row["date"]]["metric"])
                            for header in numeric:
                                if header not in x:
                                    x[header] = []
                                try:
                                    x[header].append(float(row[header]))
                                except:
                                    pass
                    if any([x[header] for header in numeric]):
                        for header in numeric:
                            if len(x[header]) == len(y):
                                corr[header] = np.corrcoef(x[header], y)[0][1]
                            else:
                                corr[header] = None
                        lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", tuple([row[header] if header in conf.allHeaders else corr[header] for header in headers])).lastrowid
                        self.conn.execute("update " + intable + " set ref_" + outtable + "=? where id=? and imtype=?", (lastid, id, imtype))
        self.conn.commit()
        return

    def anova(self, intable, outtable, grouping, overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param grouping: The list of image types to group by.
        :type grouping: list

        Computes the analysis of variation of all numeric information based on 3 factors,
        treatment, date, and the interaction between the two.  Analysis of variation is
        different than the rest of the stats functions, in that a lot of information is
        lost after running it.  The results themselves correspond to columns (pixels, rmed, binx...)
        instead of actual images.
        """
        self._validate("anova", intable, outtable, overwrite)
        headers = self._getHeaders(intable)
        numeric = [h for h in headers if h not in conf.allHeaders]
        self._createTable("anova", intable, outtable, overwrite)
        outHeaders = self._getHeaders(outtable)
        result = self.conn.execute("select * from images")
        rowdata = []
        for row in result:
            rowdata.append(dict(row))
        data = pandas.DataFrame(rowdata).dropna()
        anova = {}
        for col in numeric:
            anova[col] = {}
            dateT = []
            treatmentT = []
            date_treatmentT = []
            for date in self._getDates(intable):
                dateT.append(data[data["date"] == date][col])
                for treatment in ["Control", "Stress"]:
                    treatmentT.append(data[data["treatment"] == treatment][col])
                    date_treatmentT.append(data[np.where(np.logical_and(data["treatment"] == treatment,data["date"] == date), True, False)][col])
            f_val, d_pval = stats.f_oneway(*dateT)
            f_val, t_pval = stats.f_oneway(*treatmentT)
            f_val, dt_pval = stats.f_oneway(*date_treatmentT)
            anova[col]["date"] = d_pval
            anova[col]["treatment"] = t_pval
            anova[col]["date_treatment"] = dt_pval
        for type in ["date", "treatment", "date_treatment"]:
            vals = [group] + [anova[x][type] for x in numeric] + [type]
            self.conn.execute("insert into " + outtable + " (" + ",".join(outHeaders) + ") values (" + ",".join(["?"] * len(outHeaders)) + ")", tuple(vals))
        self.conn.commit()
        return

    def tTest(self, intable, outtable, comp = "imtype", overwrite = False):
        """
        :param intable: The input table to perform the T test on.
        :type intable: str
        :param outtable: The output table to write the results to.
        :type outtable: str
        :param comp: Whether to compare image types or image names.
        :type comp: str
        :param overwrite: Whether or not to overwrite the output database
        :type overwrite: bool

        This function computes a ttest of the input table for all numeric headers.
        The comparison is either done based on image types or image names.  Default
        functionality is a comparison between image types, specify comp = 'imgname'
        to compare image names.
        """

        self._validate("ttest", intable, outtable, overwrite)
        self._createTable("ttest", intable, outtable, overwrite)
        comp = "imtype" if comp == "imtype" else "imgname"
        headers = self._getHeaders(outtable)
        numeric = [h for h in headers if h not in conf.allHeaders]
        meta = [h for h in headers if h not in numeric]
        data = {}
        result = self.conn.execute("select pegasusid,treatment," + ",".join(headers) + " from " + intable)
        for row in result:
            if row[comp] not in data:
                data[row[comp]] = {}
            if row["date"] not in data[row[comp]]:
                data[row[comp]][row["date"]] = {"meta": dict(row), "id": [], "values": {}}
            data[row[comp]][row["date"]]["id"].append(row["pegasusid"])
            if row["treatment"] not in data[row[comp]][row["date"]]["values"]:
                data[row[comp]][row["date"]]["values"][row["treatment"]] = [[row[h]] for h in numeric]
            else:
                for i,h in enumerate(numeric):
                    data[row[comp]][row["date"]]["values"][row["treatment"]][i].append(row[h])
        for comp in data:
            for date in data[comp]:
                vals = [data[comp][date]["meta"][x] for x in meta]
                for a,b in zip(data[comp][date]["values"]["Control"], data[comp][date]["values"]["Stress"]):
                    try:
                        res = stats.ttest_ind(a, b)[1]
                    except:
                        res = None
                    vals += [res]
                lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", vals).lastrowid
                updateVals = (lastid,) + tuple(data[comp][date]["id"])
                self.conn.execute("update " + intable + " set ref_" + outtable + "=? where id in (" + ",".join(["?"] * len(data[comp][date]["id"])) + ")", updateVals)
        self.conn.commit()
        return

    def treatmentComp(self, intable, outtable, type = "ratio", direction = "Control", comp = "imtype", overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param type: The type of calculation to do between treatments.  Should be ratio or difference.
        :type type: str
        :param direction: Whether to compute C ~ S, or S ~ C, default is Control first.
        :type direction: str
        :param comp: Whether to compare by imtype or imgname
        :type comp: str

        This function compares information between treatments -- It finds plants that are identical
        except for treatment, and computes either a ratio or difference between them.  Direction
        is specified as the column you want first, so direction = "Control" will compute C ~ S, and
        direction = "Stress" will compute S ~ C.  If you have already normalized your table, difference
        will provide better information than ratio.
        """
        t1 = "Stress" if direction == "Stress" else "Control"
        t2 = "Stress" if direction == "Control" else "Control"
        if type == "ratio":
            op = lambda left, right: (left / right)
        else:
            op = lambda left, right: (left - right)
        comp = "imtype" if comp == "imtype" else "imgname"
        self._validate("treatmentComp", intable, outtable, overwrite)
        self._createTable("treatmentComp", intable, outtable, overwrite)
        headers = self._getHeaders(outtable)
        numeric = [h for h in headers if h not in conf.allHeaders]
        meta = [h for h in headers if h not in numeric]
        data = {}
        result = self.conn.execute("select pegasusid," + ",".join(headers) + " from " + intable)
        for row in result:
            if row["genotype"] not in data:
                data[row["genotype"]] = {"id": [], "values": {}, "meta": {}}
            data[row["genotype"]]["meta"][row[comp]] = dict(row)
            data[row["genotype"]]["id"].append(row["pegasusid"])
            if row[comp] not in data[row["genotype"]]["values"]:
                data[row["genotype"]]["values"][row[comp]] = {}
            if row["date"] not in data[row["genotype"]]["values"][row[comp]]:
                data[row["genotype"]]["values"][row[comp]][row["date"]] = {}
            if row["treatment"] not in data[row["genotype"]]["values"][row[comp]][row["date"]]:
                data[row["genotype"]]["values"][row[comp]][row["date"]][row["treatment"]] = [row[h] for h in numeric]
        for genotype in data:
            for comp in data[genotype]["values"]:
                for date in data[genotype]["values"][comp]:
                    vals = [data[genotype]["meta"][comp][x] for x in meta]
                    for a,b in zip(data[genotype]["values"][comp][date][t1], data[genotype]["values"][comp][date][t2]):
                        try:
                            res = op(float(a), float(b))
                        except:
                            res = None
                        vals.append(res)
                    lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", vals).lastrowid
                    updateVals = (lastid,) + tuple(data[genotype]["id"])
                    self.conn.execute("update " + intable + " set ref_" + outtable + "=? where id in (" + ",".join(["?"] * len(data[genotype]["id"])) + ")", updateVals)
        self.conn.commit()
        return

    def threshold(self, intable, outtable, thresh = 0.01, overwrite = False):
        """
        :param intable: The input table to load information from
        :type intable: str
        :param outtable: The output table to write information to.
        :type outtable: str
        :param thresh: The threshold value.
        :type thresh: float

        This function thresholds the table based on the given value.  All values
        that fall below the threhsold are 'updated' in place with 'null'.  Normalize
        is often called before thresholding.
        """
        self._validate("threshold", intable, outtable, overwrite)
        self._createTable("threshold", intable, outtable, overwrite)
        headers = self._getHeaders(outtable)
        numeric = [h for h in headers if h not in conf.allHeaders]
        result = self.conn.execute("select pegasusid," + ",".join(headers) + " from " + intable)
        for row in result:
            vals = []
            for x in headers:
                if x in conf.allHeaders:
                    vals.append(row[x])
                else:
                    if not self._checkNull(row[x]):
                        if float(row[x]) > thresh:
                            vals.append(row[x])
                        else:
                            vals.append(None)
                    else:
                        vals.append(None)
            lastid = self.conn.execute("insert into " + outtable + " (" + ",".join(headers) + ") values (" + ",".join(["?"] * len(headers)) + ")", tuple(vals)).lastrowid
            self.conn.execute("update " + intable + " set ref_" + outtable + "=? where pegasusid=?", (lastid, row["pegasusid"]))
        self.conn.commit()
        return

    def export(self, table, processed = True, group = None, fname = None):
        """
        :param table: The table to write to csv
        :type table: str
        :param processed: Whether or not to extract processed data
        :type processed: bool
        :param group: Which image types to extract from the database.
        :type group: list
        :param fname: The file name to write to.
        :type fname: str

        This function simply extracts data from a database and writes it to csv format.
        Default functionality is to extract only data that has been processed.  This is
        checked by finding if an outputPath has been set.  Additionally,
        you can specify a list of image types to extract, if not, the default list contains
        all rgb and fluo images.  Finally, if no file name is specified, the name
        of the table is used as the filename.
        """
        group = group if isinstance(group, list) else self._getImageTypes(table)
        fname = fname if fname else table + ".csv"
        query = "select * from " + table
        values = tuple()
        if processed:
            query += " where outputPath is not null"
            if group:
                query += " and (" + " or ".join(["imtype=?" for x in group]) + ")"
                values = tuple([x for x in group])
        elif group:
            query += " where " + " or ".join(["imtype=?" for x in group])
            values = tuple([x for x in group])
        table = pandas.io.sql.read_sql(sql = query, params = values, con = self.conn)
        table.to_csv(fname)
        return
"""
This file is part of Image Harvest.

Image Harvest is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Image Harvest is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Image Harvest.  If not, see <http://www.gnu.org/licenses/>.

.. module:: imgproc
    :platform: Mac, Linux
    :synopsis: Image processing function wrappers to opencv.

.. moduleauthor:: Avi Knecht <avi@kurtknecht.com>

"""

import os
import cv2
import numpy as np
import math
import conf
import sqlite3
import pymeanshift as pms
import traceback
import json
import random

class ColorFilter(object):

    """
    Color Filtration logic container.
    """
    def __init__(self, filter):
        self.tokens = {
            "True": True,
            "False": False,
            "and": lambda left, right: np.logical_and(left, right),
            "or": lambda left, right: np.logical_or(left, right),
            ">": lambda left, right: left > right,
            "<": lambda left, right: left < right,
            ">=": lambda left, right: left >= right,
            "<=": lambda left, right: left <= right,
            "=": lambda left, right: left == right,
            "+": lambda left, right: left + right,
            "-": lambda left, right: left - right,
            ".": lambda left, right: left * right,
            "/": lambda left, right: left / right,
            "max": lambda left, right: np.maximum(left, right),
            "min": lambda left, right: np.minimum(left, right),
            "not": lambda left, right: np.invert(left),
            "(": "(",
            ")": ")",
        }
        self.filterString = filter
        self.emptyRes = True
        return

    def _createArgList(self):
        s = self.filterString.replace("(", " ( ")
        s = s.replace(")", " ) ")
        self.filter = []
        for item in s.split():
            if item in self.tokens:
                self.filter.append(self.tokens[item])
            else:
                try:
                    val = int(item)
                    self.filter.append(val)
                except:
                    raise Exception("Invalid logic string.")
        #self.filter = [self.tokens[x] if x in self.tokens else int(x) for x in s.split()]
        return

    def _find(self, what, start = 0):
        return [i for i,x in enumerate(self.filter) if x == what and i >= start]

    def _parens(self):
        left_1st = self._find("(")
        if not left_1st:
            return False, -1, -1
        left = left_1st[-1]
        right = self._find(")", left + 2)[0]
        return True, left, right

    def _eval(self, filter):
        return filter[1](filter[0], filter[2])

    def _formattedEval(self, filter):
        if not filter:
            return self.emptyRes
        if len(filter) == 1:
            return filter[0]

        has_parens, l_paren, r_paren = self._parens()

        if not has_parens:
            return self._eval(filter)

        filter[l_paren:r_paren + 1] = [self._eval(filter[l_paren+1:r_paren])]
        self.emptyRes = self._eval
        return self._formattedEval(filter)

    def apply(self, image, roi):
        self.tokens["r"] = image[:,:,2].astype(float)
        self.tokens["g"] = image[:,:,1].astype(float)
        self.tokens["b"] = image[:,:,0].astype(float)
        self.tokens["i"] = np.add(np.add(self.tokens["r"], self.tokens["g"]), self.tokens["b"])
        self.tokens["high"] = np.maximum(np.maximum(self.tokens["r"], self.tokens["g"]), self.tokens["b"])
        self.tokens["low"] = np.minimum(np.minimum(self.tokens["r"], self.tokens["g"]), self.tokens["b"])
        self._createArgList()
        result = cv2.cvtColor(np.where(self._formattedEval(self.filter), 255, 0).astype(np.uint8), cv2.COLOR_GRAY2BGR)
        image[roi[0]:roi[1], roi[2]:roi[3]] = cv2.bitwise_and(image[roi[0]:roi[1], roi[2]:roi[3]], result[roi[0]:roi[1], roi[2]:roi[3]])
        return image



class Image(object):

    """
    An individual image.  Each image is loaded in as its own instance of the Image class for processing.
    """
    def __init__(self, input, outputdir = ".", writename = None, dev = False, db = None, dbid = None):
        """
        :param input: The input resource, either a path to an image or a raw numpy array.
        :type resource: numpy.ndarray or str
        :param outputdir: The directory to write output files
        :type outputdir: str
        :param writename: The name to write the output file as, should include extension.
        :type writename: str
        :param dev: Dev mode will do something...
        :type dev: bool
        """
        if os.path.isdir(outputdir):
            self.states = {}
            self._loadDb(db, dbid)
            self.input = input
            self.fname, self.image = self._loadResource(input)
            self.y = self.image.shape[0]
            self.x = self.image.shape[1]
            self.outputdir = os.path.abspath(outputdir)
            if writename:
                self.writename = writename
            elif isinstance(input, np.ndarray):
                self.writename = "out.png"
            else:
                self.writename = os.path.basename(input)
            self.dev = dev
            self.window = 1
        else:
            raise Exception("Invalid output!")
        return

    def _closeDb(self):
        if self.conn:
            self.conn.close()
        return

    def _loadDb(self, db, dbid):
        if db:
            if os.path.isfile(db):
                if dbid:
                    self.dbid = dbid
                    self.conn = sqlite3.connect(db, check_same_thread = False)
                    self.conn.row_factory = sqlite3.Row
                    result = self.conn.execute("select pegasusid from images where pegasusid=?", (self.dbid,))
                    if not result.fetchone():
                        raise Exception("Invalid pegasusid given!")
                    self._addColumn("error")
                else:
                    raise Exception("A database id must be provided if you give a database.")
            else:
                raise Exception("Invalid database given!")
        else:
            self.conn = False
        return

    def _addColumn(self, column, tablename = "images"):
        if self.conn:
            if column not in [row["name"] for row in self.conn.execute("PRAGMA table_info(" + tablename + ");")]:
                self.conn.execute("alter table " + tablename + " add column " + column + ";")
                self.conn.commit()
        return


    def _loadROIArg(self, arg, i):
        vals = {
            0: 0,
            1: self.y,
            2: 0,
            3: self.x
        }
        arg = str(arg)
        if arg == "-1":
            return vals[i]
        arg = arg.replace("x", str(self.x)).replace("y", str(self.y)).replace(" ","")
        if "-" in arg:
            try:
                left, right = arg.split("-")
                return int(left) - int(right)
            except:
                raise Exception("Could not load roi fragment '%s'" % (arg,))
        elif "+" in arg:
            try:
                left, right = arg.split("+")
                return int(left) + int(right)
            except:
                raise Exception("Could not load roi fragment '%s'" % (arg,))
        elif "/" in arg:
            try:
                left, right = arg.split("/")
                return int(left) / int(right)
            except:
                raise Exception("Could not load roi fragment '%s'" % (arg,))
        elif "*" in arg:
            try:
                left, right = arg.split("*")
                return int(left) / int(right)
            except:
                raise Exception("Could not load roi fragment '%s'" % (arg,))
        else:
            return int(arg)


    def _loadROI(self, roi):
        """
        :param roi: The region of interest to load.
        :type roi: list or file
        :return: The region of interest of form [ystart, yend, xstart, xend]
        :rtype: list
        :raises OSError: if the input path does not exist.

        Loads a region of interest, either a path to an roi or a raw list.
        """
        if not roi:
            roi = [-1, -1, -1, -1]
        if isinstance(roi, list):
            return [self._loadROIArg(z, i) for i,z in enumerate(roi)]
        else:
            if os.path.isfile(roi):
                try:
                    with open(roi, "r") as rh:
                        r = json.load(rh)
                        if all(x in r for x in ["xstart", "xend", "ystart", "yend"]):
                            return [self._loadROIArg(r["ystart"], 0), self._loadROIArg(r["yend"], 1), self._loadROIArg(r["xstart"], 2), self._loadROIArg(r["xend"], 3)]
                        else:
                            raise Exception("Input roi file '%s' needs xstart, xend, ystart, and yend definitions!" % (roi,))
                except Exception as e:
                    print traceback.format_exc()
                    raise Exception("Input roi file '%s' is not valid json!" % (roi,))
            else:
                raise Exception("Input path to roi file '%s' does not exist!" % (roi,))
        return

    def _writeROI(self, roi, output):
        """
        :param roi: The region of interest to write.
        :type roi: list
        """
        try:
            with open(self.outputdir + "/" + output, "w") as wh:
                wh.write(json.dumps({"ystart": roi[0], "yend": roi[1], "xstart": roi[2], "xend": roi[3]}, indent = 4))
        except:
            raise Exception("Could not write roi.")
        return

    def _loadBins(self, binlist):
        """
        :param bin: The bin to load
        :type bin: list or file
        """

        if isinstance(binlist, list):
            return binlist
        else:
            if os.path.isfile(binlist):
                try:
                    with open(binlist, "r") as rh:
                        return json.load(rh)
                except:
                    raise Exception("Bin list not valid json!")
            else:
                raise Exception("Bin file does not exist.")
        return

    def _loadResource(self, resource):
        """
        :param resource: The resource to load.
        :type resource: numpy.ndarray or file
        :return: The image
        :rtype: numpy.ndarray
        :raises OSError: if the input path does not exist.

        Loads a resource, either a path to an image or a raw numpy array.
        """
        if isinstance(resource, np.ndarray):
            return ("unknown", resource.copy())
        else:
            if resource in self.states:
                return (resource, self.states[resource].copy())
            elif os.path.isfile(resource):
                image = cv2.imread(resource)
                if image is not None:
                    return (os.path.basename(resource), image)
                else:
                    if self.conn:
                        self.conn.execute("update images set error =? where pegasusid = ?", ("Load Error, input is not an image.", self.dbid))
                        self.conn.commit()
                    raise Exception("Input is not an image.")
            else:
                if self.conn:
                    self.conn.execute("update images set error = ? where pegasusid = ?", ("Load Error, input path doesn't exist or specified resource is not a saved state.", self.dbid))
                    self.conn.commit()
                raise Exception("Input path to resource does not exist.")
        return

    def _getMergedContour(self):
        """
        Assumes that image is already binary.
        """
        if self._isColor():
            binary = cv2.inRange(self.image.copy(), np.array([0, 0, 1], np.uint8), np.array([255, 255, 255], np.uint8))
        else:
            binary = self.image.copy()
        contours,hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, 2)
        merged = []
        for cnt in contours:
            for point in cnt:
                merged.append([point[0][0], point[0][1]])
        return np.array(merged, dtype=np.int32)

    def drawContours(self):
        """
        A helper function that draws all detected contours in the image onto the image.
        """
        if self._isColor():
            binary = cv2.inRange(self.image.copy(), np.array([0, 0, 1], np.uint8), np.array([255, 255, 255], np.uint8))
        else:
            binary = self.image.copy()
        contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, 2)
        for cnt in contours:
            cv2.drawContours(self.image, [cnt], 0, (random.randint(127, 255), random.randint(127, 255), random.randint(127, 255)), -1)
        return

    def _colorHistogram(self):
        """
        :return: A list of histograms, corresponding to R, G, B.
        :rtype: List of numpy arrays.

        Calculates a normalized colorHistogram of the current image.
        The intensity is normalized between 0 and 255.
        """
        returnhist = []

        for ch in range(0, 3):
            hist_item = cv2.calcHist([self.image], [ch], None, [256], [1,255])
            cv2.normalize(hist_item, hist_item, 0, 255, cv2.NORM_MINMAX)
            hist = np.int32(np.around(hist_item))
            returnhist.append(hist)
        return returnhist

    def _isColor(self, image = None):
        image = self.image if image is None else image
        return len(image.shape) == 3

    def save(self, name):
        """
        :param name: The name to save the image under.
        :type name: str OR any hashable type.

        This function saves the current image in the 'states' variable under
        the specified name.  It can then be reloaded using the :py:meth:`~ih.imgproc.Image.restore`
        method.
        """
        self.states[name] = self.image.copy()
        return

    def restore(self, name):
        """
        :param name: The name the image is saved under.
        :type name: str OR any hashable type.

        Reloads a previously saved image from the 'states' variable.
        """
        if name in self.states:
            self.image = self.states[name].copy()
            self.y = self.image.shape[0]
            self.x = self.image.shape[1]
        else:
            print "Invalid state specified."
        return

    def list(self):
        """
        Lists all saved states.
        """
        for state in self.states:
            print state
        return

    def destroy(self):
        """
        Destroys all currently open windows.
        """
        cv2.destroyAllWindows()
        return

    def wait(self):
        """
        Waits until a key is pressed, then destroys all windows and
        continues program execution.
        """
        cv2.waitKey(0)
        self.destroy()
        return

    def split(self, channel):
        """
        :param channel: The channel to select from the image.
        :type channel: int

        This function is a wrapper to the OpenCV function
        `split <http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#split>`_.
        Splits an image into individually channels, and selects a single channel
        to be the resulting image (Remember, color images have channel order BGR).
        No validation is done on channel number, so it is possible to provide a
        channel number that does not exist.  For example, calling split on an
        bgr image with channel = 2 will extract the red channel from the image.
        """
        self.image = cv2.split(self.image)[channel]
        return

    def equalizeHist(self):
        """
        This function is a wrapper to the OpenCV function
        `equalizeHist <http://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html#equalizehist>`_.
        This function equalizes the histogram of a grayscale image by stretching
        the minimum and maximum values to 0 and 255 respectively.
        If this is run on a color image it will be converted to gray scale first.
        """
        if self._isColor():
            self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        self.image = cv2.equalizeHist(self.image)
        return

    def equalizeColor(self):
        """
        This function calls the :py:meth:`~ih.imgproc.Image.equalizeHist` function
        on each individual channel of a color image, and then returns the merged
        result.
        """
        if self._isColor():
            b, g, r = cv2.split(self.image)
            b = cv2.equalizeHist(b)
            g = cv2.equalizeHist(g)
            r = cv2.equalizeHist(r)
            self.image = cv2.merge([b, g, r])
        return

    def show(self, title = None, state = None):
        """
        :param title: The title to give the display window, if left blank one will be created.
        :type title: str
        :return: None

        Displays the image in a window.  Utilizes the :py:meth:`~ih.imgproc.Image.resize` function.
        If a title is not specified, the window will be named 'windowX' where X is the number of times
        show has been called.
        """
        cv2.imshow(title if title else "window " + str(self.window), self.resize(state))
        self.window += 1
        return

    def resizeSelf(self, scale = None, width = None, height = None):
        """
        :param scale: Value to scale image by.
        :type scale: float
        :param width: Target width of image.
        :type width: int
        :param height: Target height of image.
        :type height: int

        Resizes the current image.  If scale is set, it simply resizes the
        width and height of the image based on the scale.  If only one of width
        or height is set, it scales the other accordingly.  If both width
        and height are set, it scales the image to the exact size specified.
        """
        if scale or width or height:
            if scale:
                self.image = cv2.resize(self.image, (int(self.x * scale), int(self.y * scale)))
            elif width and height:
                self.image = cv2.resize(self.image, height, width)
            elif width:
                scale = float(width) / float(self.x)
                self.image = cv2.resize(self.image, (int(self.x * scale), int(self.y * scale)))
            elif height:
                scale = float(height) / float(self.y)
                self.image = cv2.resize(self.image, (int(self.x * scale), int(self.y * scale)))
            self.y = self.image.shape[0]
            self.x = self.image.shape[1]
        return

    def addWeighted(self, image, weight1, weight2):
        """
        :param image: The image to add.
        :type image: str of np.ndarray
        :param weight1: The weight to apply to the current image.
        :type weight1: float
        :param weight2: The weight to apply to the additional image.
        :type weight2: float

        This function is a wrapper to the OpenCV function
        `addWeighted <http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#addweighted>`_.
        This function adds/blends an additional image to the current based on the provided
        weights.  Both positive and negative weights can be used.
        """
        self.image = cv2.addWeighted(self.image, weight1, self._loadResource(image)[1], weight2, 0)
        return

    def resize(self, state = None):
        """
        If the image is larger than conf.maxArea, resize its total area down to conf.maxArea.
        This function is primarily used for viewing purposes, and as such, it does not resize
        the base image, but creates a copy to resize instead.
        """
        if (state):
          im = self.states[state].copy()
          if (im.shape[0] * im.shape[1] > conf.maxArea):
              scale = 1 / math.sqrt((im.shape[1] * im.shape[0]) / conf.maxArea)
              return cv2.resize(im, (int(im.shape[1] * scale), int(im.shape[0] * scale)))
        elif (self.image.shape[0] * self.image.shape[1] > conf.maxArea):
            scale = 1 / math.sqrt((self.image.shape[1] * self.image.shape[0]) / conf.maxArea)
            return cv2.resize(self.image.copy(), (int(self.x*scale), int(self.y*scale)))
        else:
            return self.image

    def write(self, name = None):
        """
        Writes the current image to the given output directory, with the given name.
        """
        writename = self.writename if not name else name
        cv2.imwrite(self.outputdir + "/" + writename, self.image)
        return

    def convertColor(self, intype, outtype):
        """
        :param intype: The input image type
        :type intype: str
        :param outtype: The output image type
        :type outtype: str
        :return: The converted image.
        :rtype: numpy.ndarray
        :raises: KeyError

        Converts the given image between color spaces, based on the given types.
        Supported types are: bgr, gray, hsv, lab, and ycrcb.  Note, you cannot
        restore color to a gray image with this function, for that you must use
        bitwise_and with an appropriate mask + image.
        """
        if intype in conf.colors:
            if outtype in conf.colors[intype]:
                for code in conf.colors[intype][outtype]:
                    self.image = cv2.cvtColor(self.image, code)
            else:
                raise KeyError(outtype + " is not a valid output type for the input type: " + intype)
        else:
            raise KeyError(intype + " is not a valid image type.")
        return

    def threshold(self, thresh, max = 255, type = "binary"):
        """
        :param thresh: Threshold value.
        :type thresh: int
        :param max: Write value for binary threshold.
        :type max: int
        :param type: Threhsold type.
        :type type: str
        :return: The thresholded image.
        :rtype: numpy.ndarray
        :raises KeyError: if the specified threshold type doesn't exist.

        Thresholds the image based on the given type.  The image must be
        grayscale to be thresholded.  If the image is of type 'bgr' it is
        automatically converted to grayscale before thresholding.
        Supported types are: binary, inverse, truncate, tozero, and otsu.
        """
        if self._isColor():
            self.convertColor("bgr", "gray")
        if type in conf.thresholds:
            self.image = cv2.threshold(self.image, thresh, max, conf.thresholds[type])[1]
        else:
            raise KeyError(type + " is not a valid threshold type.")
        return

    def rotateColor(self, color):
        """
        :param color: Color shift to perform.  Should be [b, g, r].
        :type color: list

        Shifts the entire color of the image based on the values in
        the color list.
        """
        b, g, r = cv2.split(self.image.astype(np.uint16))
        np.clip(np.add(b, color[0]), 0, 255, out = b)
        np.clip(np.add(g, color[1]), 0, 255, out = g)
        np.clip(np.add(r, color[2]), 0, 255, out = r)
        self.image = cv2.merge([b, g, r]).astype(np.uint8)
        return

    def knn(self, k, labels, remove = []):
        """
        :param k: Number of nearest neighbors to use
        :type k: int
        :param labels: Path to label file.  More info below
        :type labels: file
        :param remove: Labels to remove from final image.
        :type remove: list

        This function is a wrapper to the OpenCV function `KNearest <http://docs.opencv.org/modules/ml/doc/k_nearest_neighbors.html>`_.
        The label file should contain training data in json format, using the label name of keys, and all
        the colors matching that label as an array value.  Each color should be a list of 3 values, in BGR order.
        That is:

        .. code-block:: python

            {
            "plant": [
                [234, 125, 100],
                [100, 100, 100]
            ],
            "pot": [
                ...
            }

        When creating your label file, make sure to use helpful names.  Calling each set of colors "label1", "label2" e.t.c
        provides no meaningful information.  The remove list is the list of matched labels to remove from the final image.
        The names to remove should match the names in your label file exactly. For example, let's say you have the labels
        "plant", "pot", "track", and "background" defined, and you only want to keep pixels that match the "plant" label.
        Your remove list should be specified as ["pot", "track", "background"].
        """
        if (os.path.isfile(labels)):
            with open(labels, "r") as rh:
                data = json.load(rh)
                labelMap = []
                trainData = []
                response = []
                for index,key in enumerate(data.keys()):
                     labelMap.append(key)
                     for color in data[key]:
                         trainData.append(color)
                         response.append(index)
                trainData = np.array(trainData, dtype = np.float32)
                response = np.array(response)
                knn = cv2.KNearest()
                knn.train(trainData, response)
                fim = self.image.copy().reshape((-1, 3)).astype(np.float32)
                ret, results, neighbors, dist = knn.find_nearest(fim, k)
                ires = np.in1d(results.ravel(), [i for i,x in enumerate(labelMap) if x not in remove])
                final = cv2.cvtColor(np.where(ires, 255, 0).astype(np.uint8).reshape((self.y, self.x)).astype(np.uint8), cv2.COLOR_GRAY2BGR)
                self.bitwise_and(final)
        else:
            print "Cannot find label file."
        return


    def kmeans(self, k, criteria, maxiter = 10, accuracy = 1.0, attempts = 10, flags = "random", labels = None):
        """
        :param k: Number of colors in final image.
        :type k: int
        :param criteria: Determination of how the algorithm stops execution.  Should be one of 'accuracy', 'iteration', or 'either'.
        :type criteria: str
        :param maxiter: Maximum number of iterations of the algorithm.
        :type maxiter: int
        :param accuracy: Minimum accuracy before algorithm finishes executing.
        :type accuracy: float
        :param attempts: Number of times the algorithm is executed using different initial guesses.
        :type attempts: int
        :param flags: How to determine initial centers should be either 'random' or 'pp'.
        :type flags: str

        This function is a wrapper to the OpenCV function `kmeans <http://docs.opencv.org/modules/core/doc/clustering.html>`_
        Adjusts the colors in the image to find the most compact 'central' colors.  The amount of colors
        in the resulting image is the specified value 'k'.  The colors are chosen based upon the minimum
        amount of adjustment in the image necessary.  The criteria parameter determines when the algorithm
        stops.  If 'accuracy' is specified, the algorithm runs until the specified accuracy is reached.  If 'iteration'
        is specified, the algorithm runs the specified number of iterations.  If 'either' is specified, the algorithm
        runs until one of the conditions is satisfied.  The flags parameter determines the initial central colors,
        and should be either 'random' -- to generate a random initial guess -- or 'pp' to use center initialization by Arthur and Vassilvitskii.
        """
        if flags in conf.centers:
            if criteria in conf.ktermination:
                reshaped = self.image.reshape((-1,3))
                reshaped = np.float32(reshaped)
                ret, label, center = cv2.kmeans(reshaped, k, (conf.ktermination[criteria], maxiter, accuracy), attempts, conf.centers[flags], bestLabels = labels)
                center = np.uint8(center)
                res = center[label.flatten()]
                self.image = res.reshape((self.image.shape))
            else:
                raise KeyError(criteria + " is not a valid termination type.  Should be one of 'accuracy', 'iteration', or 'either'")
        else:
            raise KeyError(flags + " is not a valid center type.  Should be either 'random' or 'pp'.")
        return


    def meanshift(self, spatial_radius, range_radius, min_density):
        """
        :param spatial_radius: Spatial Radius
        :type spatial_radius: int
        :param range_radius: Range Radius.
        :type range_radius: int
        :param min_density: Minimum Density.
        :type min_density: int
        :return: The mean-shifted image.
        :rtype: numpy.ndarray

        Segments the image into clusters based on nearest neighbors.  This function
        is a wrapper to the `pymeanshift <https://code.google.com/p/pymeanshift/>`_
        module.  For details on the algorithm itself: `Mean shift: A robust approach toward feature space analysis <http://dx.doi.org/10.1109/34.1000236>`_.
        """
        (self.image, labels_image, number_regions) = pms.segment(self.image, spatial_radius = spatial_radius, range_radius = range_radius, min_density = min_density)
        return

    def adaptiveThreshold(self, value, adaptiveType, thresholdType, blockSize, C):
        """
        :param value: Intensity value for the pixels based on the thresholding conditions.
        :type param: int
        :param adaptiveType: Adaptive algorithm to use, should be either 'mean' or 'gaussian'.
        :type adaptiveType: str
        :param thresholdType: Threshold type, should be either 'binary' or 'inverse'.
        :type thresholdType: str
        :param blockSize: The window size to consider while thresholding, should only be an odd number.
        :type blockSize: int
        :param C: A constant subtracted from the calculated mean in each window.
        :type C: int

        Thresholds an image by considering the image in several different windows instead of the image
        as a whole.  This function is a wrapper to the OpenCV function `adaptiveThreshold <http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html#adaptivethreshold>`_.
        Specifying 'mean' for adaptiveType calculates a simple mean of the area, wheras specifying 'gaussian' calculates a weighted sum
        based upon a `Gaussian Kernel <http://docs.opencv.org/modules/imgproc/doc/filtering.html#Mat getGaussianKernel(int ksize, double sigma, int ktype)>`_.
        Specifying 'binary' for thresholdType means that a particular intensity value must beat the threshold to be kept, whereas
        specifying 'inverse' means that a particular intensity value must lose to the threshold to be kept.
        Similar to a normal thresholding function, the image must be converted to grayscale first.  This can be done using the
        :meth:`~ih.imgproc.Image.convertColor` function, however, if your image is of type 'bgr', this is handled automatically.
        """
        if self._isColor():
            self.convertColor("bgr", "gray")
        if adaptiveType in conf.adaptives:
            if thresholdType == "binary" or thresholdType == "inverse":
                self.image = cv2.adaptiveThreshold(self.image, value, conf.adaptives[adaptiveType], conf.thresholds[thresholdType], blockSize, C)
            else:
                raise Exception("Threshold type: " + thresholdType + " must be either binary or inverse.")
        else:
            raise Exception("Adaptive type: " + adaptiveType + " is not a valid adaptive threshold type, should be either 'mean' or 'gaussian'")
        return

    def blur(self, ksize, anchor = (-1, -1), borderType = "default"):
        """
        :param ksize: The size of the kernel represented by a tuple (width, height).  Both numbers should be odd and positive.
        :type ksize: tuple
        :param anchor: The anchor point for filtering.  Default is (-1, -1) which is the center of the kernel.
        :type anchor: tuple
        :param borderType: The type of border mode used to extrapolate pixels outside the image.
        :type borderType: str

        Smoothes an image using the normalized box filter.  This function is a wrapper to the OpenCV function
        `blur <http://docs.opencv.org/modules/imgproc/doc/filtering.html#blur>`_.  Increasing the kernel size increase
        the window considered when applying a blur.  The anchor by default is the center of the kernel,
        however you can alter the anchor to consider different areas of the kernel.  When blurring on the edge
        of the image, values for pixels that would be outside of the image are extrapolated.  The method
        of extrapolation depends on the specified 'borderType', and can be one of 'default', 'constant',
        'reflect', or 'replicate'.
        """
        if borderType in conf.borders:
            self.image = cv2.blur(self.image, ksize, anchor = anchor, borderType = conf.borders[borderType])
        else:
            raise Exception("Invalid border type, should be one of: " + ",".join(conf.borders.keys()) + ".")
        return

    def medianBlur(self, ksize):
        """
        :param ksize: The size of the kernel (ksize x ksize).  Should be odd and positive.
        :type ksize: int

        This function smoothes an image using the median filter.  The kernel is set to size (ksize, ksize).
        The anchor position is assumed to be the center.  This function is a wrapper to the opencv function
        `medianBlur <http://docs.opencv.org/modules/imgproc/doc/filtering.html#medianBlur>`_.
        """
        self.image = cv2.medianBlur(self.image, ksize)
        return

    def gaussianBlur(self, ksize, sigmaX = 0, sigmaY = 0, borderType = "default", roi = None):
        """
        :param ksize: The size of the kernel represented by a tuple (width, height).  Both numers should be odd and positive.
        :type ksize: tuple
        :param sigmaX: The standard deviation in the x direction.  If 0, the value is calculated based on the kernel size.
        :type sigmaX: float
        :param sigmaY: The standard deviation in the y direction.  If 0, the value is equal to sigmaX.
        :type sigmaY: float
        :param borderType: The type of border mode used to extrapolate pixels outside the image.
        :type borderType: str

        This function blurs an image based on a Gaussian kernel.  When blurring on the edge
        of the image, values for pixels that would be outside of the image are extrapolated.  The method
        of extrapolation depends on the specified 'borderType', and can be one of 'default', 'constant',
        'reflect', or 'replicate'.  This function is a wrapper to the OpenCV function `GaussianBlur <http://docs.opencv.org/modules/imgproc/doc/filtering.html#gaussianblur>`_.
        """
        if borderType in conf.borders:
            sigmaY = sigmaX if sigmaY == 0 else sigmaY
            roi = self._loadROI(roi)
            ystart, yend, xstart, xend = roi
            self.image[ystart:yend, xstart:xend] = cv2.GaussianBlur(self.image[ystart:yend, xstart:xend], ksize, sigmaX, sigmaY, borderType = conf.borders[borderType])
        else:
            raise Exception("Invalid border type, should be one of: " + ",".join(conf.borders.keys()) + ".")
        return

    def normalizeByIntensity(self):
        """
        Normalizes each channel of the pixel by its intensity.  For each pixel, the intensity is defined as
        :math:`I = R + G + B`, where :math:`R,G,B` are the color values for that pixel.  We calculate new color values by
        multiplying the original number by 255, and dividing by the intensity, that is, :math:`r = \\frac{255 \\cdot R}{I}
        , g = \\frac{255 \\cdot G}{I}, b = \\frac{255 \\cdot B}{I}`.
        """
        f = self.image.astype(float)
        combined = np.add(np.add(f[:,:,0], f[:,:,1]), f[:,:,2])
        scaled = np.multiply(f, [255])
        self.image = np.divide(scaled, combined[:,:,None]).astype(np.uint8)
        return

    def morphology(self, morphType, ktype, ksize, anchor = (-1, -1), iterations = 1, borderType = "default"):
        """
        :param morphType: The type of morphology to perform.  Should be dilate, erode, open, close, gradient, tophat, or blackhat.
        :type morphType: str
        :param ktype: the type of the kernel, should be rect, ellipse, or cross.
        :type ktype: str
        :param ksize: The size of the kernel represented by a tuple (width, height).  Both numbers should be odd and positive.
        :type ksize: tuple
        :param anchor: The anchor point for filtering.  Default is (-1, -1) which is the center of the kernel.
        :type anchor: tuple
        :param iterations: The number of times to perform the specified morphology.
        :type iterations: int
        :param borderType: The type of border mode used to extrapolate pixels outside the image.
        :type borderType: str

        This function performs morphological operations based on the inputted values. This function is
        a wrapper to the OpenCv function `morphologyEx <http://docs.opencv.org/modules/imgproc/doc/filtering.html#morphologyex>`_. When performing the morphology on the edges
        of the image, values for pixels that would be outside of the image are extrapolated.  The method
        of extrapolation depends on the specified 'borderType', and can be one of 'default', 'constant',
        'reflect', or 'replicate'.
        """
        if morphType in conf.morph:
            if ktype in conf.kernels:
                if borderType in conf.borders:
                    kernel = cv2.getStructuringElement(conf.kernels[ktype], ksize, anchor)
                    self.image = cv2.morphologyEx(self.image, conf.morph[morphType], kernel, anchor = anchor, iterations = iterations, borderType = conf.borders[borderType])
                else:
                    raise Exception("Invalid border type, should be one of: " + ",".join(conf.borders.keys()) + ".")
            else:
                raise Exception("Invalid kernel type, should be one of: " + ",".join(conf.kernels.keys()) + ".")
        else:
            raise Exception("Invalid morphology type, should be one of: " + ",".join(conf.morph.keys()) + ".")

    def _findSeed(self, seedMask):
        bname, binary = self._loadResource(seedMask)
        if self._isColor(binary):
            binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY)
        contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, 2)
        size = 0
        select = 0
        for cnt in contours:
            if cv2.contourArea(cnt) > size:
                select = cnt
                size = cv2.contourArea(cnt)
        return tuple(select[0][0])

    def floodFill(self, mask, low, high, writeColor = (255, 255, 255), connectivity = 4, fixed = False, seed = (0,0), findSeed = False, seedMask = None, binary = False):
        """
        :param mask: A binary image corresponding to the area you don't want to fill.
        :type mask: str or np.ndarray
        :param seed: The beginning point to use for filling.
        :type seed: Tuple (x, y)
        :param low: Maximal lower brightness/color difference between the currently observed pixel and one of its neighbors belonging to the component, or a seed pixel being added to the component.
        :type low: Tuple (b, g, r) or (i,)
        :param high: Maximal upper brightness/color difference between the currently observed pixel and one of its neighbors belonging to the component, or a seed pixel being added to the component.
        :type high: Tuple (b, g, r) or (i,)
        :param writeColor: The color to write to the filled region.  Default (255, 255, 255).
        :type writeColor: tuple (b, g, r) or (i,)
        :param connectivity: The number of neighboring pixels to consider for the flooding operation.  Should be 4 or 8.
        :type connectivity: int
        :param fixed: If True, calculates color differences relative to the seed.
        :type fixed: boolean
        :param findSeed: If True, picks a seed point based on contours in the seedMask image.
        :type findSeed: boolean
        :param seedMask: Binary image to select seed from.
        :type seedMask: str or np.ndarray
        :param binary: Specify if input image is binary.
        :type binary: boolean

        This function is a wrapper to the OpenCV function `floodFill <http://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#floodfill>`_.
        This function floods the region of an image based on calculated color differences from neighbors or from the seed.  When flooding a binary
        image all input color tuples should have 1 value instead of 3.
        """
        print low,high,writeColor
        mname, mask = self._loadResource(mask)
        if self._isColor(mask):
            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        if binary and self._isColor():
            self.convertColor("bgr", "gray")
        # Mask is required to be 2 pixels wider and taller than the image
        adjmask = np.zeros((self.y + 2, self.x + 2), np.uint8)
        adjmask[1:1+self.y, 1:1+self.x] = mask
        channels = 3 if len(self.image.shape) == 3 else 1
        if len(low) == channels:
            if len(high) == channels:
                if len(writeColor) == channels:
                    if connectivity == 4 or connectivity == 8:
                        if findSeed and seedMask:
                            seed = self._findSeed(seedMask)
                        if fixed:
                            cv2.floodFill(self.image, adjmask, seed, writeColor, low, high, connectivity | cv2.FLOODFILL_FIXED_RANGE)
                        else:
                            cv2.floodFill(self.image, adjmask, seed, writeColor, low, high, connectivity)
                    else:
                        raise Exception("Invalid value for connectivity, should be either 4 or 8.")
                else:
                    raise Exception("Incorrect number of values for write color.  Number of values should match input image channels (%s)" % (channels,))
            else:
                raise Exception("Incorrect number of values for high difference.  Number of values should match input image channels (%s)" % (channels,))
        else:
            raise Exception("Incorrect number of values for low difference.  Number of values should match input image channels (%s)" % (channels,))
        return

    def fill(self, roi, color):
        """
        :param roi: A list corresponding to the area of the image you want.  List should be of the form [ystart, yend, xstart, xend]
        :type roi: list or roi file
        :param color: A list corresponding to BGR values to fill the corresponding area with.
        :type color: list

        Fills the given roi with the given color.
        """
        roi = self._loadROI(roi)
        ystart, yend, xstart, xend = roi
        self.image[ystart:yend, xstart:xend] = color
        return



    def crop(self, roi, resize = False):
        """
        :param roi: A list corresponding to the area of the image you want.  List should be of the form [ystart, yend, xstart, xend]
        :type roi: list or roi file
        :param resize: If True, actually adjusts the size of the image, otherwise just draws over the part of the image not in the roi.
        :type resize: bool

        This function crops the image based on the given roi [ystart, yend, xstart, xend].  There are two crop options,
        by default, the function doesn't actually resize the image.  Instead, it sets each pixel not in the roi to black.
        If resize is set to True, the function will actually crop the image down to the roi.
        """
        roi = self._loadROI(roi)
        ystart, yend, xstart, xend = roi
        if (resize):
            self.image = self.image[ystart: yend, xstart: xend]
            self.x = self.image.shape[1]
            self.y = self.image.shape[0]
        else:
            maxx = self.x
            maxy = self.y
            off = [0, 0, 0] if self._isColor() else [0]
            self.image[0:ystart, 0:maxx] = off
            self.image[yend:maxy, 0:maxx] = off
            self.image[0:maxy, 0:xstart] = off
            self.image[0:maxy, xend:maxx] = off
        return

    def mask(self):
        """
        This function convers the image to a color mask by performing the following operations:

        1. convertColor("bgr", "gray")
        2. threshold(0)
        3. convertColor("gray", "bgr")
        """
        self.convertColor("bgr", "gray")
        self.threshold(0)
        self.convertColor("gray", "bgr")
        return

    def contourChop(self, binary, basemin = 100):
        """
        :param binary: The binary image to find contours of.
        :type binary: str of np.ndarray
        :param basemin: The minimum area a contour must have to be considered part of the foreground.
        :type basemin: int

        This function works very similiarly to the :py:meth:`~ih.imgproc.Image.contourCut`
        function, except that this function does not crop the image, but removes
        all contours that fall below the threshold.
        """

        bname, binary = self._loadResource(binary)
        if self._isColor(binary):
            binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY)
        contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]
        for cnt in contours:
            if (cv2.contourArea(cnt) < basemin):
                cv2.drawContours(self.image, [cnt], -1, (0, 0, 0), -1)
        return

    def getBounds(self):
        """
        :return: The bounding box of the image.
        :rtype: list

        This function finds the bounding box of all contours in the image, and
        returns a list of the form [miny, maxy, minx, maxx]
        """
        binary = self.image.copy()
        if self._isColor(binary):
            binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY)
        contours = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]
        minx = binary.shape[1]
        miny = binary.shape[0]
        maxx = 0
        maxy = 0
        for cnt in contours:
            x,y,w,h = cv2.boundingRect(cnt)
            if (x < minx):
                minx = x
            if (y < miny):
                miny = y
            if (x + w > maxx):
                maxx = x + w
            if (y + h > maxy):
                maxy = y + h
        return [miny, maxy, minx, maxx]

    def contourCut(self, binary, basemin = 100, padding = [0, 0, 0, 0], resize = False, returnBound = False, roiwrite = "roi.json"):
        """
        :param binary: The binary image to find contours of.
        :type binary: str or np.ndarray
        :param basemin: The minimum area a contour must have to be considered part of the foreground.
        :type basemin: int
        :param padding: Padding add to all sides of the final roi.
        :type padding: int
        :param returnBound: If set, instead of cropping the image, simply write the detected roi.
        :type returnBound: bool
        :param resize: Whether or not to resize the image.
        :type resize: bool

        This function crops an image based on the size of detected contours in the image --
        clusters of pixels in the image.  The image is cropped such that all contours
        that are greater than the specified area are included in the final output image.
        image is cropped such that all contours that are greater than the specified area are
        included in the final output image.  If returnBound is set, instead of actually
        cropping the image, the detected roi is written to a file instead.  Otherwise,
        the detected roi is passed into the :py:meth:`~ih.imgproc.Image.crop` function,
        with the given resize value.  This function is useful for getting accurate
        height and width of a specific plant, as well as removing outlying clusters
        of non-important pixels.
        """
        bname, binary = self._loadResource(binary)
        if self._isColor(binary):
            binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY)
        contours = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]
        minx = binary.shape[1]
        miny = binary.shape[0]
        maxx = 0
        maxy = 0
        for cnt in contours:
            x,y,w,h = cv2.boundingRect(cnt)
            if (basemin < cv2.contourArea(cnt)):
                if (x < minx):
                    minx = x
                if (y < miny):
                    miny = y
                if (x + w > maxx):
                    maxx = x + w
                if (y + h > maxy):
                    maxy = y + h
        roi = [0 if miny - padding[0] < 0 else miny - padding[0], binary.shape[0] if maxy + padding[1] > binary.shape[0] else maxy + padding[1], 0 if minx - padding[2] < 0 else minx - padding[2], binary.shape[1] if maxx + padding[3] > binary.shape[1] else maxx + padding[3]]
        if returnBound:
            self._writeROI(roi, roiwrite)
        self.crop(roi, resize)
        return

    def edges(self, threshold1, threshold2, apertureSize = 3, L2gradient = False):
        """
        :param threshold1: First threshold for the hysteresis procedure.
        :type threshold1: int
        :param threshold2: Second threshold for the hysteresis procedure.
        :type threshold2: int
        :param apertureSize: aperture size used for the Sobel operator.  Must be odd, postive, and less than 8.
        :type apertureSize: int
        :L2gradient: Used to calculate Image gradient magnitude, if true then :math:`L = \sqrt{(dI/dx)^2 + (dI/dy)^2}`, if false then :math:`L = dI/dx + dI/dy`.
        :type L2gradient: bool

        This function calculates the edges of an image using the Canny edge detection algorithm using the Sobel operator.  This function is a wrapper to the OpenCV function `Canny <http://docs.opencv.org/modules/imgproc/doc/feature_detection.html#canny>`_.
        """
        self.image = cv2.Canny(self.image, threshold1, threshold2, apertureSize = apertureSize, L2gradient = L2gradient)
        return

    def colorFilter(self, logic, roi = None):
        """
        :param logic: The logic you want to run on the image.
        :type logic: str
        :param roi: The roi you want to apply the filter to
        :type roi: list or roi file

        This function applies a color filter defined by the input logic, to a
        targeted region defined by the input roi. The logic string itself is fairly complicated.
        The string supports the following characters: '+', '-', '*', '/', '>', '>=',
        '==', '<', '<=', 'and', 'or', '(', ')', 'r', 'g', 'b', 'max', and 'min' as well as any numeric
        value.  The logic string itself must be well formed -- each
        operation, arg1 operator arg2, must be surrounded by parenthesis, and the entire statement
        must be surrounded by parenthesis.  For example,
        if you want to check the intensity of the pixel, your logic string would be:
        '(((r + g) + b) < 100)'.  This string in particular will only keep pixels whose
        intensity is less than 100.  Similar rules apply for 'and' and 'or' operators.
        Let's say we only want to keep pixels whose intensity is less than 100, OR both
        the red and blue channels are greater than 150, the logic string would be:
        '((((r + g) + b) < 100) or ((r > 150) and (b > 150)))'.  The more complex
        your logic is the harder it is to read, so you may want to consider breaking
        up complex filtering into multiple steps for readability.  Finally, despite
        the fact this function solves arbitrary logic, it is very fast.
        """
        filter = ColorFilter(logic)
        roi = self._loadROI(roi)
        self.image = filter.apply(self.image, roi)
        return

    def bitwise_not(self):
        """
        Inverts the image.  If the given image has multiple channels (i.e. is a color image) each channel is processed independently.
        """
        self.image = cv2.bitwise_not(self.image)
        return

    def bitwise_and(self, comp):
        """
        :param comp: The comparison image.
        :type comp: str or np.ndarray
        :return: The resulting mask.
        :rtype: numpy.ndarray

        Performs logical AND between the input image and the comp image.
        The comp input is very versatile, and can be one of three input types,
        an image, a path, or a saved state.  An image input is a raw numpy array,
        and this input type will be passed through to the function without modification.
        If a path is specified, ih attempts to load the file as an image, and pass it
        to the function.  Finally, the input is checked against the currently saved
        image states.  If it matches, the corresponding state is passed to the function.
        The function assumes that the two input images are of matching type --
        if they are not an error will be thrown.  By default, images loaded from a
        path are loaded as 'bgr' type images.
        For more information on states, see :py:meth:`~ih.imgproc.Image.save`.
        """
        self.image = cv2.bitwise_and(self.image, self._loadResource(comp)[1])
        return

    def bitwise_or(self, comp):
        """
        :param comp: The comparison image.
        :type comp: str or np.ndarray
        :return: The resulting mask.
        :rtype: numpy.ndarray

        Performs logical OR between the input image and the comp image.
        The comp input is very versatile, and can be one of three input types,
        an image, a path, or a saved state.  An image input is a raw numpy array,
        and this input type will be passed through to the function without modification.
        If a path is specified, ih attempts to load the file as an image, and pass it
        to the function.  Finally, the input is checked against the currently saved
        image states.  If it matches, the corresponding state is passed to the function.
        The function assumes that the two input images are of matching type --
        if they are not an error will be thrown.  By default, images loaded from a
        path are loaded as 'bgr' type images.
        For more information on states, see :py:meth:`~ih.imgproc.Image.save`.
        """
        self.image = cv2.bitwise_or(self.image, self._loadResource(comp)[1])
        return

    def bitwise_xor(self, comp):
        """
        :param comp: The comparison image.
        :type comp: str or np.ndarray
        :return: The resulting mask.
        :rtype: numpy.ndarray

        Performs exclusive logical OR between the input image and the comp image.
        The comp input is very versatile, and can be one of three input types,
        an image, a path, or a saved state.  An image input is a raw numpy array,
        and this input type will be passed through to the function without modification.
        If a path is specified, ih attempts to load the file as an image, and pass it
        to the function.  Finally, the input is checked against the currently saved
        image states.  If it matches, the corresponding state is passed to the function.
        The function assumes that the two input images are of matching type --
        if they are not an error will be thrown.  By default, images loaded from a
        path are loaded as 'bgr' type images.
        For more information on states, see :py:meth:`~ih.imgproc.Image.save`.
        """
        self.image = cv2.bitwise_xor(self.image, self._loadResource(comp)[1])
        return

    def extractLabels(self, fname, meta_labels):
        """
        :param fname: The output file name to write.
        :type fname: str
        :param meta_labels: A dictionary containing required meta info.
        :type meta_labels: dict

        Meta labels should look like:

        .. code-block:: python

            meta_labels {
                "label_name": roi,
                "label_name2": roi
            }
        """
        data = {}
        for labelname in meta_labels:
            roi = self._loadROI(meta_labels[labelname])
            ystart, yend, xstart, xend = roi
            data[labelname] = [list(x.astype(int)) for x in np.reshape(self.image[ystart: yend, xstart: xend], ((yend - ystart) * (xend - xstart), 3))]
        print data
        with open(fname, "w") as wh:
            json.dump(data, wh)
        return

    def extractFinalPath(self):
        """
        This function writes the absolute path of the output file to the database.
        """
        if self.conn:
             finalpath = "/".join(os.path.abspath(self.input).split("/")[-6:])
             self._addColumn("outputPath")
             self.conn.execute("update images set outputPath=? where pegasusid=?", (finalpath, self.dbid))
             self.conn.commit()
        return

    def extractMoments(self):
        """
        :return: A dictionary corresponding to the different moments of the image.
        :rtype: dict

        Calculates the moments of the image, and returns a dicitonary based on them.
        Spatial moments are prefixed with 'm', central moments are prefixed with 'mu',
        and central normalized moments are prefixed with 'nu'.  This function
        is a wrapper to the OpenCV function `moments <http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#moments>`_.
        """
        moments = cv2.moments(cv2.inRange(self.image, np.array([0, 0, 1], np.uint8), np.array([255, 255, 255], np.uint8)))
        if self.conn:
            for id in moments:
                self._addColumn(id)
                self.conn.execute("update images set " + id + "=? where pegasusid=?", (moments[id], self.dbid))
            self.conn.commit()
            return
        else:
            return moments

    def extractDimsFromROI(self, roi):
        """
        :param roi: The roi to calculate height from.
        :type roi: list or roi file

        :return: A list corresponding to the calculated height and width of the image.
        :rtype: list

        Returns a list with the follwoing form: [height, width].  This functions differs
        from the :py:meth:`~ih.imgproc.Image.extractDimensions` in the way that height
        is calculated.  Rather than calculating the total height of the image,
        the height is calculated from the top of the given ROI.
        """

        pot = self._loadROI(roi)
        plant = self.getBounds()
        height = pot[0] - plant[0]
        width = plant[3] - plant[2]
        if self.conn:
            self._addColumn("height")
            self._addColumn("width")
            self.conn.execute("update images set height=?,width=? where pegasusid=?", (height, width, self.dbid))
            self.conn.commit()
            return
        else:
            return [height, self.x]


    def extractDimensions(self):
        """
        :return: A list corresponding to the height and width of the image.
        :rtype: list

        Returns a list with the following form: [height, width]
        """
        bounds = self.getBounds()
        height = bounds[1] - bounds[0]
        width = bounds[3] - bounds[2]
        if self.conn:
            self._addColumn("height")
            self._addColumn("width")
            self.conn.execute("update images set height=?,width=? where pegasusid=?", (height, width, self.dbid))
            self.conn.commit()
            return
        else:
            return [height, width]

    def extractMinEnclosingCircle(self):
        """
        :return: The center, and radius of the minimum enclosing circle.
        :rtype: int

        Returns the center and radius of the minimum enclosing circle of all
        non-black pixels in the image.  The point of this function
        is not to threshold, so the contours are generated from
        all the pixels that fall into the range [0, 0, 1], [255, 255, 255].
        """
        circle = cv2.minEnclosingCircle(self._getMergedContour())
        if self.conn:
            self._addColumn("circle_centerx")
            self._addColumn("circle_centery")
            self._addColumn("circle_radius")
            self.conn.execute("update images set circle_centerx=?, circle_centery=?, circle_radius=? where pegasusid=?", (circle[0][0], circle[0][1], circle[1], self.dbid))
            self.conn.commit()
        else:
            return circle

    def extractConvexHull(self):
        """
        :return: The area of the convex hull.
        :rtype: int

        Returns the area of the convex hull around all non black pixels in the image.
        The point of this function is not to threshold, so the contours are generate from
        all the pixels that fall into the range [0, 0, 1], [255, 255, 255]
        """
        hull = cv2.contourArea(
                 cv2.approxPolyDP(
                    cv2.convexHull(
                        self._getMergedContour()
                    ), 0.001, True
                ))
        if self.conn:
            self._addColumn("convex_hull_area")
            self.conn.execute("update images set convex_hull_area=? where pegasusid=?", (hull, self.dbid))
            self.conn.commit()
        else:
            return hull

    def extractPixels(self):
        """
        :return: The number of non-black pixels in the image.
        :rtype: int

        Returns the number of non-black pixels in the image.  Creates
        a temporary binary image to do this.  The point of this function
        is not to threshold, so the binary image is created by all
        pixels that fall into the range [0, 0, 1], [255, 255, 255].
        """
        pixelCount = cv2.countNonZero(cv2.inRange(self.image, np.array([0, 0, 1], np.uint8), np.array([255, 255, 255], np.uint8)))
        if self.conn:
            self._addColumn("pixels")
            self.conn.execute("update images set pixels=? where pegasusid=?", (pixelCount, self.dbid))
            self.conn.commit()
            return
        else:
            return pixelCount

    def extractColorData(self, nonzero = True, returnhist = False):
        """
        :param nonzero: Whether or not to look at only nonzero pixelsself.  Default true.
        :type nonzero: bool
        :return: Mean & median for each channel.
        :rtype: list

        This function calculates a normalized histogram of each individual color channel of
        the image, and returns the mean & median of the histograms for the channels
        specified.  Because images are imported with the channels ordered as B,G,R,
        the output list is returned the same way.  The returned list always looks like
        this: [ [BlueMean, BlueMedian], [GreenMean, GreenMedian], [RedMean, RedMedian] ].
        Mean values always come before median values.  If nonzero is set to true (default)
        the function will only calculate mediapytn and means based on the non-black pixels.
        If you are connected to a database, the entire histogram is saved to the database,
        not just the mean and median.
        """
        hist = self._colorHistogram()
        if returnhist:
            return hist
        colors = [    [np.mean(hist[0][np.nonzero(hist[0])] if nonzero else hist[0]), np.median(hist[0][np.nonzero(hist[0])] if nonzero else hist[0])],
                    [np.mean(hist[1][np.nonzero(hist[1])] if nonzero else hist[1]), np.median(hist[1][np.nonzero(hist[1])] if nonzero else hist[1])],
                    [np.mean(hist[2][np.nonzero(hist[2])] if nonzero else hist[2]), np.median(hist[2][np.nonzero(hist[2])] if nonzero else hist[2])]
                ]
        if self.conn:
            self._addColumn("rmean")
            self._addColumn("rmed")
            self._addColumn("gmean")
            self._addColumn("gmed")
            self._addColumn("bmean")
            self._addColumn("bmed")
            query = "update images set rmean=?,rmed=?,gmean=?,gmed=?,bmean=?,bmed=?"
            values = [colors[2][0], colors[2][1], colors[1][0], colors[1][1], colors[0][0], colors[0][1]]
            for x,c in enumerate(["bhist", "ghist", "rhist"]):
                for i in range(0, 256):
                    self._addColumn(c + str(i))
                    query += "," + c + str(i) + "=?"
                    values.append(int(hist[x][i]))
            query += " where pegasusid=?"
            values.append(self.dbid)
            self.conn.execute(query, tuple(values))
            self.conn.commit()
            return
        else:
            return colors

    def extractColorChannels(self):
    	"""
    	This function extracts the total number of pixels of each color value
        for each channel.
    	"""
        b, g, r = cv2.split(self.image)
        bdata, gdata, rdata = [], [], []
        for i in range(0, 256):
            bdata.append(np.count_nonzero(np.where(b == i, True, False)))
            gdata.append(np.count_nonzero(np.where(g == i, True, False)))
            rdata.append(np.count_nonzero(np.where(r == i, True, False)))
        data = [bdata, gdata, rdata]
        if self.conn:
            query = "update images set "
            values = []
            for x,c in enumerate(["b", "g", "r"]):
                for i in range(0, 256):
                    self._addColumn(c + str(i))
                    if i == 0 and c == "b":
                        query += c + str(i) + "=?"
                    else:
                        query += "," + c + str(i) + "=?"
                    values.append(data[x][i])
            query += " where pegasusid=?"
            values.append(self.dbid)
            self.conn.execute(query, tuple(values))
            self.conn.commit()
            return
        else:
            return (bdata, gdata, rdata)

    def extractBins(self, binlist):
        """
        :param binlist: The specified bins (color ranges) to count.
        :type binlist: list
        :return: The number of pixels that fall in each bin.
        :rtype: list

        This function counts the number of pixels that fall into the range as
        specified by each bin.  This function expects the input to be a list of
        dictionaries as follows:

        .. code-block:: python

             binlist = [
                {"name": "bin1",
                 "min": [B, G, R],
                 "max": [B, G, R]
                },
                {"name": "bin2",
                ...
            ]

        Each range is defined by 6 values.  A minimum and maximum blue,
        green, and red value. The returned list is very similar to the
        input list, except a 'count' key is added to each dictionary:

        .. code-block:: python

            returnlist = [
                {"name": "bin1",
                "min": [B, G, R],
                "max": [B, G, R],
                "count": VALUE
                },
                ...
            ]

        Where 'VALUE' is the number of pixels that fall into the
        corresponding range.
        A list is used instead of a dictionary as the base structure
        to maintain order for writing to the output database.  When
        using this function within a workflow, the order you specify
        your bins is the order in which they will show up in the
        database, and the name you specify for you bin will be
        the column name in the database.
        """
        binlist = self._loadBins(binlist)
        for i in range(0, len(binlist)):
            binlist[i]["count"] = cv2.countNonZero(cv2.inRange(self.image, np.array(binlist[i]["min"], np.uint8), np.array(binlist[i]["max"], np.uint8)))
        if self.conn:
            for bin in binlist:
                self._addColumn(bin["name"])
                self.conn.execute("update images set " + bin["name"] + "=? where pegasusid=?", (bin["count"], self.dbid))
            self.conn.commit()
            return
        else:
            return binlist
"""
This file is part of Image Harvest.

Image Harvest is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Image Harvest is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Image Harvest.  If not, see <http://www.gnu.org/licenses/>.
"""

import cv2

"""
A dictionary used to map input-output types to a cv2 color code
"""
colors = {
    "bgr": {
        "gray": [cv2.COLOR_BGR2GRAY],
        "hsv": [cv2.COLOR_BGR2HSV],
        "lab": [cv2.COLOR_BGR2LAB],
        "ycrcb": [cv2.COLOR_BGR2YCR_CB]
    },
    "gray": {
        "bgr": [cv2.COLOR_GRAY2BGR],
        "hsv": [cv2.COLOR_GRAY2BGR, cv2.COLOR_BGR2HSV],
        "lab": [cv2.COLOR_GRAY2BGR, cv2.COLOR_BGR2LAB],
        "ycrcb": [cv2.COLOR_GRAY2BGR, cv2.COLOR_BGR2YCR_CB]
    },
    "hsv": {
        "bgr": [cv2.COLOR_HSV2BGR],
        "gray": [cv2.COLOR_HSV2BGR, cv2.COLOR_BGR2GRAY],
        "lab": [cv2.COLOR_YCR_CB2BGR, cv2.COLOR_BGR2LAB],
        "ycrcb": [cv2.COLOR_YCR_CB2BGR, cv2.COLOR_BGR2YCR_CB]
    },
    "lab": {
        "bgr": [cv2.COLOR_LAB2BGR],
        "gray": [cv2.COLOR_LAB2BGR, cv2.COLOR_BGR2GRAY],
        "hsv": [cv2.COLOR_LAB2BGR, cv2.COLOR_BGR2HSV],
        "ycrcb": [cv2.COLOR_LAB2BGR, cv2.COLOR_BGR2YCR_CB]
    },
    "ycrcb": {
        "bgr": [cv2.COLOR_YCR_CB2BGR],
        "gray": [cv2.COLOR_YCR_CB2BGR, cv2.COLOR_BGR2GRAY],
        "hsv": [cv2.COLOR_YCR_CB2BGR, cv2.COLOR_BGR2HSV],
        "lab": [cv2.COLOR_YCR_CB2BGR, cv2.COLOR_BGR2LAB]
    }
}

"""
A dicitonary used to map threshold-types to cv2 threshold codes
"""
thresholds = {
    "binary": cv2.THRESH_BINARY,
    "inverse": cv2.THRESH_BINARY_INV,
    "truncate": cv2.THRESH_TRUNC,
    "tozero": cv2.THRESH_TOZERO,
    "otsu": cv2.THRESH_BINARY + cv2.THRESH_OTSU
}

"""
A dictionary used to map adaptive-types to cv2 adaptive codes
"""
adaptives = {
    "mean": cv2.ADAPTIVE_THRESH_MEAN_C,
    "gauss": cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
    "gaussian": cv2.ADAPTIVE_THRESH_GAUSSIAN_C
}


"""
A dictionary used to map border-types to cv2 border codes
"""
borders = {
    "default": cv2.BORDER_DEFAULT,
    "constant": cv2.BORDER_CONSTANT,
    "reflect": cv2.BORDER_REFLECT,
    "replicate": cv2.BORDER_REPLICATE,
    "transparent": cv2.BORDER_TRANSPARENT,
    "wrap": cv2.BORDER_WRAP
}

"""
A dictionary used to map morphology types to cv2 morphology codes
"""
morph = {
    "dilate": cv2.MORPH_DILATE,
    "erode": cv2.MORPH_ERODE,
    "open": cv2.MORPH_OPEN,
    "close": cv2.MORPH_CLOSE,
    "gradient": cv2.MORPH_GRADIENT,
    "tophat": cv2.MORPH_TOPHAT,
    "blackhat": cv2.MORPH_BLACKHAT
}

"""
A dictionary used to map kernel types to cv2 kernel codes
"""
kernels = {
    "rect": cv2.MORPH_RECT,
    "ellipse": cv2.MORPH_ELLIPSE,
    "cross": cv2.MORPH_CROSS
}

"""
A dictionary used to map center types to cv2 center codes
"""
centers = {
    "random": cv2.KMEANS_RANDOM_CENTERS,
    "pp": cv2.KMEANS_PP_CENTERS
}

"""
A dictionary used to map termination types to cv2 termination codes
"""
ktermination = {
    "accuracy": cv2.TERM_CRITERIA_EPS,
    "iteration": cv2.TERM_CRITERIA_MAX_ITER,
    "either": cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER
}

"""
Defines the maximum size of a displayed image in total pixels.
"""
maxArea = 360000

"""
Defines imtype mapping
"""
typeMap = {
    "rgbsv": "rgb",
    "rgbtv": "rgb",
    "fluosv": "fluo",
    "fluotv": "fluo"
}

"""
Defines all optional / required keys for all templates.
"""

templateKeys = {
    "loading": {
        "required": ["path", "base", "data", "order"],
        "optional": ["translations", "filetype"],
        "data": {
            "required": ["type"],
            "type": ["value", "file", "date"],
            "value": {
                "required": ["value", "type"],
                "optional": ["translate", "case"]
            },
            "file": {
                "required": ["type", "value", "key", "keyColumn", "valueColumn"],
                "optional": ["translate", "separator"]
            },
            "date": {
                "required": ["type", "value", "format"],
                "optional": []
            }
        },
        "translations": {

        }
    },
    "config": {
        "required": ["version", "installdir", "profile"],
        "optional": ["cluster", "notify", "maxwalltime", "osg"],
        "maxwalltime": {
            "optional": ["images", "stats"]
        },
        "notify": {
            "required": ["email", "pegasus_home"]
        },
        "osg": {
            "required": ["tarball", "ssh"]
        }
    },
    "imgproc": {
        "required": ["workflows", "extract"],
        "optional": ["options"],
        "job": {
            "required": ["executable", "inputs", "outputs", "arguments", "name"],
            "optional": ["depends"]
        },
        "options": {
            "required": [],
            "optional": ["save-steps"]
        },
        "extract": {
            "required": ["workflows"],
            "optional": ["histogram-bin"],
            "workflows": {
                "required": [], # imtype are required, handled at run time.
                "optional": []
            },
            "histogram-bin": {
                "required": ["chunks", "group", "channels"],
                "optional": []
            }
        }
    },
    "stats": {
        "required": ["workflows"],
        "optional": ["options"],
        "job": {
            "required": ["executable", "inputs", "outputs", "arguments", "name"],
            "optional": ["depends"]
        }
    }
}

"""
Defines valid date format character
"""
dateFormat = ["Y", "y", "M", "m", "d", "B", "b"]
dateSep = ["/", "-", "_", " "]

"""
Defines required headers for output
"""
outHeaders = ["pegasusid", "experiment", "id", "date", "imtype", "path"]

"""
Defines ALL possible output headers
"""
allHeaders = outHeaders + ["error", "rowNum", "colNum"]

"""
Defines required files for job checking
"""
jobFiles = ["crawl.json", "config.json", "imgproc.json", "stats.json", "images.db"]
imgprocFiles = ["config.json", "imgproc.json", "images.db"]
statisticsFiles = ["config.json", "stats.json"]

workflowFile = "imgproc.json"
configFile = "config.json"
dbFile = "images.db"
statsFile = "stats.json"
outputdb = "output.db"


"""
Defines two things for stats functions.
'Required' columns are the metadata columns a function needs to execute properly.
'Exclude' columns are the metadata columns that become uninteresting after execution.
"""
statsColumns = {
    "histogramBins": {
        "ref": False,
        "add": ["name", "minr", "ming", "minb", "maxr", "maxg", "maxb"],
        "required": ["pegasusid", "experiment", "id", "date", "imtype", "imgname", "outputPath"],
        "exclude": ["all"],
        "overwrite": []
    },
    "ttest": {
        "ref": True,
        "add": [],
        "required": ["pegasusid", "date", "imtype"],
        "exclude": ["pegasusid", "path", "outputPath", "error", "id", "genotype", "treatment"],
        "overwrite": []
    },
    "treatmentComp": {
        "ref": True,
        "add": [],
        "required": ["pegasusid", "id", "genotype", "date", "treatment", "imtype"],
        "exclude": ["pegasusid", "id", "path", "outputPath", "error"],
        "overwrite": ["treatment"]
    },
    "shootArea": {
        "ref": True,
        "add": [],
        "required": ["pegasusid", "id", "genotype", "date", "treatment", "imtype"],
        "exclude": ["pegasusid", "imgname", "path", "outputPath", "error", "rowNum", "colNum"],
        "overwrite": ["imtype"]
    },
    "normalize": {
        "ref": True,
        "add": [],
        "required": ["pegasusid"],
        "exclude": ["pegasusid", "path", "outputPath", "error"],
        "overwrite": []
    },
    "correlation": {
        "ref": True,
        "add": [],
        "required": ["pegasusid", "id", "date"],
        "exclude": ["pegasusid", "path", "outputPath", "error", "rowNum", "colNum", "date", "imgname"],
        "overwrite": []
    },
    "threshold": {
        "ref": True,
        "add": [],
        "required": ["pegasusid"],
        "exclude": ["pegasusid", "path", "outputPath", "error"],
        "overwrite": []
    },
    "anova": {
        "ref": False,
        "add": ["factors"],
        "required": ["pegasusid", "treatment", "date", "imtype"],
        "exclude": ["pegasusid", "experiment", "id", "genotype", "date", "treatment", "imgname", "path", "outputPath", "error"],
        "overwrite": ["imtype"]
    }
}

"""
Defines allowed image extensions, png is preferred.
"""
imageExtensions = [".png", ".jpg", ".jpeg", ".gif"]

"""
Defines allowed file extensions
"""
fileExtensions = {
    "image": ".png",
    "roi": ".json",
    "binfile": ".json",
    "csv": ".csv",
}


"""
Defines valid arguments for stats and image processing
functions.
"""
valid = {
    "ih-resize": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--scale": {
                "type": "numeric"
            },
            "--width": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 5001)
            },
            "--height": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 5001)
            }
        }
    },
    "ih-color-filter": {
        "type": "imgproc",
        "inputs": ["image", "roi"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--roi": {
                "type": "derived",
                "key": "inputs",
                "index": 1
            },
            "--logic": {
                "type": "string",
                "required": "true",
                "complex": "true"
            }
        }
    },
    "ih-edges": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--threshold1": {
                "type": "numeric",
                "required": "true"
            },
            "--threshold2": {
                "type": "numeric",
                "required": "true"
            },
            "--apertureSize": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 50)
            },
            "--L2gradient": {
                "type": "exist"
            }
        }
    },
    "ih-contour-chop": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image", "roi"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--binary": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--basemin": {
                "type": "numeric"
            }
        }
    },
    "ih-contour-cut": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image", "roi"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--binary": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--basemin": {
                "type": "numeric"
            },
            "--padminx": {
                "type": "numeric"
            },
            "--padmaxx": {
                "type": "numeric"
            },
            "--padminy": {
                "type": "numeric"
            },
            "--padmaxy": {
                "type": "numeric"
            },
            "--resize": {
                "type": "exist"
            },
            "--returnBound": {
                "type": "derived",
                "key": "outputs",
                "index": 1,
                "value": ""
            },
            "--roiwrite": {
                "type": "derived",
                "key": "outputs",
                "index": 1
            }
        }
    },
    "ih-mask": {
        "type": "imgproc",
        "inputs": ["image", "roi"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-crop": {
        "type": "imgproc",
        "inputs": ["image", "roi"],
        "outputs": ["image"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--roi": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--resize": {
                "type": "exist"
            }
        }
    },
    "ih-split": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--channel": {
                "type": "numeric",
                "validation": "list",
                "value": [0, 1, 2],
                "required": "true"
            }
        }
    },
    "ih-equalize-hist": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-flood-fill": {
        "type": "imgproc",
        "inputs": ["image", "image", "image"],
        "outputs": ["image"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--mask": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--seedMask": {
                "type": "derived",
                "key": "inputs",
                "index": 2
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--low": {
                "type": "string",
                "required": "true"
            },
            "--high": {
                "type": "string",
                "required": "true"
            },
            "--writeColor": {
                "type": "string",
                "required": "true"
            },
            "--connectivity": {
                "type": "numeric",
                "validation": "list",
                "value": [4, 8]
            },
            "--fixed": {
                "type": "exist"
            },
            "--seedx": {
                "type": "numeric"
            },
            "--seedy": {
                "type": "numeric"
            },
            "--findSeed": {
                "type": "exist"
            },
            "--binary": {
                "type": "exist"
            }
        }
    },
    "ih-fill": {
        "type": "imgproc",
        "inputs": ["image", "roi"],
        "outputs": ["image"],
        "arguments": {
           "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--roi": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--r": {
                "type": "numeric",
                "validation": "list",
                "value": range(0, 256)
            },
            "--g": {
                "type": "numeric",
                "validation": "list",
                "value": range(0, 256)
            },
            "--b": {
                "type": "numeric",
                "validation": "list",
                "value": range(0, 256)
            }
        }
    },
    "ih-morphology": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--morphType": {
                "type": "string",
                "validation": "dictionary",
                "value": morph,
                "required": "true"
            },
            "--ktype": {
                "type": "string",
                "validation": "dictionary",
                "value": kernels,
                "required": "true"
            },
            "--kwidth": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--kheight": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--anchorx": {
                "type": "numeric",
                "validation": "list",
                "value": range(-255, 256),
            },
            "--anchory": {
                "type": "numeric",
                "validation": "list",
                "value": range(-255, 256),
            },
            "--iterations": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 100)
            },
            "--border": {
                "type": "string",
                "validation": "dictionary",
                "value": borders
            }
        }
    },
    "ih-normalize-intensity": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-gaussian-blur": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--kwidth": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--kheight": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--sigmax": {
                "type": "numeric"
            },
            "--sigmay": {
                "type": "numeric"
            },
            "--border": {
                "type": "string",
                "validation": "dictionary",
                "value": borders
            }
        }
    },
    "ih-median-blur": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--ksize": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            }
        }
    },
    "ih-blur": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--kwidth": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--kheight": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--anchorx": {
                "type": "numeric",
                "validation": "list",
                "value": range(-255, 256),
            },
            "--anchory": {
                "type": "numeric",
                "validation": "list",
                "value": range(-255, 256),
            },
            "--border": {
                "type": "string",
                "validation": "dictionary",
                "value": borders
            }
        }
    },
    "ih-adaptive-threshold": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--value": {
                "type": "numeric",
                "validation": "list",
                "value": range(0, 256),
                "required": "true"
            },
            "--adaptiveType": {
                "type": "string",
                "validation": "dictionary",
                "value": adaptives,
                "required": "true"
            },
            "--thresholdType": {
                "type": "string",
                "validation": "dictionary",
                "value": {"binary": "", "inverse": ""},
                "required": "true"
            },
            "--blockSize": {
                "type": "numeric",
                "validation": "list",
                "value": range(1, 256, 2),
                "required": "true"
            },
            "--C": {
                "type": "numeric",
                "required": "true",
            }
        }
    },
    "ih-meanshift": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "weight": .3,
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--spatial_radius": {
                "type": "numeric",
                "required": "true"
            },
            "--range_radius": {
                "type": "numeric",
                "required": "true"
            },
            "--min_density": {
                "type": "numeric",
                "required": "true"
            }
        }
    },
    "ih-convert-color": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--intype": {
                "type": "string",
                "validation": "dictionary",
                "value": colors,
                "required": "true"
            },
            "--outtype": {
                "type": "string",
                "validation": "dictionary",
                "key": "--intype",
                "value": colors,
                "required": "true"
            }
        }
    },
    "ih-threshold": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--thresh": {
                "type": "numeric",
                "required": "true"
            },
            "--max": {
                "type": "numeric",
                "validation": "list",
                "value": range(0, 256)
            },
            "--type": {
                "type": "string",
                "validation": "list",
                "value": ["binary", "inverse", "trunc", "otsu", "tozero"]
            }
        }
    },
    "ih-bitwise-or": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image"],
        "arguments": {
            "--input1": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--input2": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-bitwise-xor": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image"],
        "arguments": {
            "--input1": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--input2": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-bitwise-not": {
        "type": "imgproc",
        "inputs": ["image"],
        "outputs": ["image"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            }
        }
    },
    "ih-add-weighted": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image"],
        "arguments": {
            "--input1": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--input2": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--weight1": {
                "required": "true",
                "type": "numeric"
            },
            "--weight2": {
                "required": "true",
                "type": "numeric"
            }
        }
    },
    "ih-bitwise-and": {
        "type": "imgproc",
        "inputs": ["image", "image"],
        "outputs": ["image"],
        "arguments": {
            "--input1": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--input2": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--output": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--outputdir": {
                "type": "overwrite",
                "value": "."
            },
            "--writeblank": {
                "type": "overwrite",
                "required": "true",
                "value": ""
            },
            "--mask": {
                "type": "exist"
            }
        }
    },
    "ih-extract": {
        "type": "system",
        "inputs": ["image", "binfile"],
        "outputs": ["none"],
        "arguments": {
            "--input": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--dimensions": {
                "type": "exist"
            },
            "--dimfromroi": {
                "type": "string"
            },
            "--pixels": {
                "type": "exist"
            },
            "--moments": {
                "type": "exist"
            },
            "--bins": {
                "type": "string"
            },
            "--channels": {
                "type": "exist"
            },
            "--hull": {
                "type": "exist"
            },
            "--circle": {
                "type": "exist"
            }
        }
    },
    "ih-error-log": {
         "type": "imgproc"
    },



    "ih-extract-all": {
        "type": "system"
    },
    "ih-extract-multi": {
        "type": "system"
    },
    "ih-sql-aggregate": {
        "type": "system"
    },
    "osg-wrapper.sh": {
        "type": "system"
    },



    "ih-stats-export": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["csv"],
        "arguments": {
            "--table": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--fname": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            }
        }
    },
    "ih-stats-histogram-bin": {
        "type": "system",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--channels": {
                "type": "dict",
                "required": "true"
            },
            "--group": {
                "type": "dict",
                "required": "true"
            },
            "--chunks": {
                "type": "dict",
                "required": "true"
            }
        }
    },
    "ih-stats-ttest": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--comp": {
                "type": "string",
                "validation": "list",
                "value": ["imtype", "imgname"],
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-treatment-comp": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--type": {
                "type": "string",
                "validation": "list",
                "value": ["ratio", "difference"],
                "required": "true"
            },
            "--direction": {
                "type": "string",
                "validation": "list",
                "value": ["Control", "Stress"],
                "required": "true"
            },
            "--comp": {
                "type": "string",
                "validation": "list",
                "value": ["imtype", "imgname"],
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-anova": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--group": {
                "type": "list",
                "required": "true",
                "join": " "
            },
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-shoot-area": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--group": {
                "type": "list",
                "required": "true",
                "join": " "
            },
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-threshold": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--thresh": {
                "type": "numeric",
                "required": "true"
            },
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-normalize": {
        "type": "statistics",
        "inputs": ["table"],
        "outputs": ["table"],
        "arguments": {
            "--column": {
                "type": "string"
            },
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    },
    "ih-stats-correlate": {
        "type": "statistics",
        "inputs": ["table", "csv", "csv"],
        "outputs": ["table"],
        "arguments": {
            "--intable": {
                "type": "derived",
                "key": "inputs",
                "index": 0,
                "required": "true"
            },
            "--outtable": {
                "type": "derived",
                "key": "outputs",
                "index": 0,
                "required": "true"
            },
            "--datafile": {
                "type": "derived",
                "key": "inputs",
                "index": 1,
                "required": "true"
            },
            "--dataheaders": {
                "type": "derived",
                "key": "inputs",
                "index": 2,
                "required": "true"
            },
            "--overwrite": {
                "type": "overwrite",
                "value": "",
                "required": "true"
            }
        }
    }
}
from distutils.core import setup

setup(
	name="ih",
	version="1.0",
	packages=["ih"],
	license="GPL",
	scripts=[
		"scripts/ih-bitwise-and",
		"scripts/ih-bitwise-not",
		"scripts/ih-bitwise-or",
		"scripts/ih-bitwise-xor",
		"scripts/ih-convert-color",
		"scripts/ih-meanshift",
		"scripts/ih-threshold",
		"scripts/ih-extract",
		"scripts/ih-adaptive-threshold",
		"scripts/ih-blur",
		"scripts/ih-gaussian-blur",
		"scripts/ih-median-blur",
		"scripts/ih-normalize-intensity",
		"scripts/ih-morphology",
		"scripts/ih-crop",
		"scripts/ih-fill",
		"scripts/ih-contour-cut",
		"scripts/ih-contour-chop",
		"scripts/ih-edges",
		"scripts/ih-color-filter",
		"scripts/ih-resize",
        "scripts/ih-mask",
		"scripts/ih-add-weighted",
		"scripts/ih-split",
		"scripts/ih-equalize-hist",
		"scripts/ih-flood-fill",

		"scripts/ih-seed",
        "scripts/ih-write-sql",

		"scripts/ih-setup",
		"scripts/ih-crawl",
		"scripts/ih-run",
		"scripts/ih-error-log",
		"scripts/ih-extract-all",
		"scripts/ih-extract-multi",
		"scripts/ih-sql-aggregate",
		"scripts/osg-wrapper.sh",

		"scripts/ih-data",


		"scripts/ih-stats-shoot-area",
		"scripts/ih-stats-normalize",
		"scripts/ih-stats-correlate",
		"scripts/ih-stats-threshold",
		"scripts/ih-stats-anova",
		"scripts/ih-stats-treatment-comp",
		"scripts/ih-stats-ttest",
		"scripts/ih-stats-histogram-bin",
		"scripts/ih-stats-export"
	]
)
"""
This file is part of Image Harvest.

Image Harvest is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Image Harvest is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Image Harvest.  If not, see <http://www.gnu.org/licenses/>.
"""
import os
import conf
import sys
import json
import re
import csv
import datetime
import sqlite3
import textwrap
import shutil
import errno
import copy
import traceback
from Pegasus.DAX3 import *

class Validator(object):
    """
    A very, very generic validator.
    """
    def __init__(self, f, type):
        self.err = ""
        if not os.path.isfile(f):
            self.err += "Path Error: Input '%s' doesn't exist.\n" % (f,)
        if not self.err:
            if type == "file":
                with open(f, "r") as rh:
                    try:
                        self.data = json.load(rh)
                    except Exception as e:
                        self.err += "Json Error: File '%s', %s\n" % (f, str(e))
            elif type == "db":
                self.conn = sqlite3.connect(f)
                self.conn.row_factory = sqlite3.Row
                try:
                    result = self.conn.execute("select * from images")
                    if not result.fetchone():
                        self.err += "Database Error: No information in images table\n"
                except Exception as e:
                    self.err += "Database Error: %s\n" % (str(e), )
            else:
                self.err += "Validator Error: Invalid type, must be either 'file' or 'db'\n"
            self.rawFiles = {}
            self.type = type
            self.validate()
        return

    def printErrors(self):
        """
            Prints out errors from validation and then exits.
        """
        if self.err:
            print self.err.strip()
        else:
            print "No Validation Errors."
        return

    def isValid(self):
        """
            A convenience function.  If validation ran successfully,
            return True, else return False.
        """
        if not self.err:
            return True
        else:
            return False

    def validate(self):
        """
            This function should be overloaded
        """
        return

class Workflow(object):

    def __init__(self, template, config, database):
        self.workflow = Validator(template, "file")
        self.config = Validator(config, "file")
        self.db = Validator(database, "db")
        self.err = ""
        if self.workflow.isValid() and self.db.isValid() and self.config.isValid():
            self.validate()
        else:
            self.err += "Workflow components did not validate individually. \n"
            self.err += self.workflow.err
            self.err += self.config.err
            self.err += self.db.err
        return

    def isValid(self):
        """
        A convenience function.
        """
        if not self.err:
            return True
        else:
            return False

    def printErrors(self):
        if self.err:
            print self.err.strip()
        else:
            print "No Validation Errors"
        return

    def _loadOverwriteArgument(self, job, arg):
        job["arguments"][arg] = conf.valid[job["executable"]]["arguments"][arg]["value"]
        return

    def _loadDerivedArgument(self, job, arg, jtype):
        key = conf.valid[job["executable"]]["arguments"][arg]["key"]
        index = conf.valid[job["executable"]]["arguments"][arg]["index"]
        if key in job:
            if isinstance(job[key], list):
                if index <= len(job[key]) - 1:
                    if "value" in conf.valid[job["executable"]]["arguments"][arg]:
                        job["arguments"][arg] = conf.valid[job["executable"]]["arguments"][arg]["value"]
                    else:
                        job["arguments"][arg] = job[key][index]
                else:
                    if "required" in conf.valid[job["executable"]]["arguments"][arg]:
                        self.err += "Workflow, Argument Error: Type '%s' job '%s', derived argument '%s' requires index '%s' for definition '%s', no such index. \n" % (jtype, job["name"], arg, index, key)
            else:
                if "required" in conf.valid[job["executable"]]["arguments"][arg]:
                    self.err += "Workflow, Argument Error: Type '%s' job '%s', derived argument '%s' requires definition for '%s' to be of type list, definition is of type '%s'. \n" % (jtype, job["name"], arg, key, type(job[key]).__name__)
        else:
            if "required" in conf.valid[job["executable"]]["arguments"][arg]:
                self.err += "Workflow, Argument Error: Type '%s' job '%s', derived argument '%s' requires job definition for '%s', no such definition. \n" % (jtype, job["name"], arg, key)
        return

    def _validateArgumentType(self, job, arg, type):
        if arg in job["arguments"]:
            if not isinstance(job["arguments"][arg], {
                "list": list,
                "string": (str, unicode),
                "dict": dict,
                "numeric": (int, long, float),
                "exist": object,
                "derived": object
            }[conf.valid[job["executable"]]["arguments"][arg]["type"]]):
                self.err += "Workflow, Argument Error: Type '%s' job '%s', argument '%s' given value '%s', should be of type '%s'. \n" % (type, job["name"], arg, job["arguments"][arg], conf.valid[job["executable"]]["arguments"][arg]["type"])
            else:
                if isinstance(job["arguments"][arg], list) and "join" in conf.valid[job["executable"]]["arguments"][arg]:
                    job["arguments"][arg] = conf.valid[job["executable"]]["arguments"][arg]["join"].join(job["arguments"][arg])
                elif (isinstance(job["arguments"][arg], dict) or isinstance(job["arguments"][arg], list)):
                    job["arguments"][arg] = str(job["arguments"][arg])
                elif (isinstance(job["arguments"][arg], (str, unicode)) and conf.valid[job["executable"]]["arguments"][arg]["type"] and "complex" in conf.valid[job["executable"]]["arguments"][arg]):
                    job["arguments"][arg] = '"' + job["arguments"][arg] + '"'
        return

    def _validateArgumentRequired(self, job, arg, type):
        if "required" in conf.valid[job["executable"]]["arguments"][arg]:
            if arg in job["arguments"]:
                if job["arguments"][arg] == "":
                    self.err += "Workflow, Argument Error: Type '%s' job '%s', has empty required argument '%s' \n" % (type, job["name"], arg)
            else:
                self.err += "Workflow, Argument Error: Type '%s' job '%s', requires argument '%s', no such argument found. \n" % (type, job["name"], arg)
        return

    def _validateArgumentDictionary(self, job, arg, type):
        arglist = job["arguments"][arg] if isinstance(job["arguments"][arg], list) else [job["arguments"][arg]]
        if conf.valid[job["executable"]]["arguments"][arg]["validation"] == "dictionary":
            d = conf.valid[job["executable"]]["arguments"][arg]["value"] if "key" not in conf.valid[job["executable"]]["arguments"][arg] else conf.valid[job["executable"]]["arguments"][arg]["value"].get(job["arguments"][conf.valid[job["executable"]]["arguments"][arg]["key"]])
            for val in arglist:
                if val not in d:
                    self.err += "Workflow, Argument Error: Type '%s' job '%s', has invalid value '%s' for argument '%s'. \n" % (type, job["name"], val, arg)
        return

    def _validateArgumentList(self, job, arg, type):
        arglist = job["arguments"][arg] if isinstance(job["arguments"][arg], list) else [job["arguments"][arg]]
        l = conf.valid[job["executable"]]["arguments"][arg]["value"]
        for val in arglist:
            if val not in l:
                self.err += "Workflow, Argument Error: Type '%s' job '%s', has invalid value '%s' for argument '%s'. \n" % (type, job["name"], val, arg)
        return

    def _validateArguments(self, job, type):
        if not job["arguments"]:
            job["arguments"] = {}
        for arg in conf.valid[job["executable"]]["arguments"]:
            if conf.valid[job["executable"]]["arguments"][arg]["type"] == "derived":
                self._loadDerivedArgument(job, arg, type)
            if "required" in conf.valid[job["executable"]]["arguments"][arg] or arg in job["arguments"]:
                if conf.valid[job["executable"]]["arguments"][arg]["type"] == "overwrite":
                    self._loadOverwriteArgument(job, arg)
                else:
                    self._validateArgumentRequired(job, arg, type)
                    self._validateArgumentType(job, arg, type)

                if "validation" in conf.valid[job["executable"]]["arguments"][arg]:
                    if arg in job["arguments"]:
                        validate = {
                            "dictionary": self._validateArgumentDictionary,
                            "list": self._validateArgumentList
                        }[conf.valid[job["executable"]]["arguments"][arg]["validation"]](job, arg, type)

        for arg in job["arguments"]:
            if arg not in conf.valid[job["executable"]]["arguments"]:
                self.err += "Workflow, Argument Error: Type '%s' job '%s', has invalid argument '%s'. \n" % (type, job["name"], arg)
        return

    def _validateDependencies(self, job, type):
        try:
            names = [x["name"] for x in self.workflow.data["workflows"][type]]
            unres = copy.deepcopy(job["inputs"])
            for input in job["inputs"]:
                if os.path.isfile(input):
                    unres.remove(input)
                    self.workflow.rawFiles[type].append(input)
                elif input in self.workflow.data["workflows"][type][0]["inputs"]:
                    unres.remove(input)
            if "depends" in job:
                for dependency in job["depends"]:
                    if dependency in names:
                        i = names.index(dependency)
                        for output in self.workflow.data["workflows"][type][i]["outputs"]:
                            if output in unres:
                                unres = [x for x in unres if x != output]
                    else:
                        self.err += "Workflow, Dependency Error: Type '%s' job '%s' depends on job '%s', no such job exists. \n" % (type, job["name"], dependency)
            for val in set(unres):
                self.err += "Workflow, Input Error: Type '%s' job '%s' depends on input '%s'.  Cannot find matching output, or raw file. \n" % (type, job["name"], val)
            for x,input in enumerate(job["inputs"]):
                if "depends" in job:
                    for dependency in job["depends"]:
                        if dependency in names:
                            i = names.index(dependency)
                            if input in self.workflow.data["workflows"][type][i]["outputs"]:
                                j = self.workflow.data["workflows"][type][i]["outputs"].index(input)
                                if conf.valid[job["executable"]]["inputs"][x] != conf.valid[self.workflow.data["workflows"][type][i]["executable"]]["outputs"][j]:
                                    self.err += "Workflow, Dependency Error: Type '%s' job '%s' input '%s' in position '%s' should be of type '%s', however, output '%s' in position '%s' of job '%s' is of type '%s'. \n" %  (type, job["name"], input, x, conf.valid[job["executable"]]["inputs"][x], self.workflow.data["workflows"][type][i]["outputs"][j], j, self.workflow.data["workflows"][type][i]["name"], conf.valid[self.workflow.data["workflows"][type][i]["executable"]]["outputs"][j])
        except:
            self.printErrors()
            print "Validation halted at type '%s' job '%s'.  Fix errors before re-running. \n" % (type, job)
            sys.exit()
        return

    def _validateDB(self):
        """
        Valides the input database file.
        DIFFERENT FOR WORKFLOWS
        OVERLOAD THIS
        """

        return

    def _validateConfig(self):
        """
        Validates the input configuration template.
        SAME FOR BOTH TYPES OF WORKFLOWS
        """
        if self.config.data:
            if set(conf.templateKeys["config"]["required"]) < set(self.config.data.keys()):
                if not os.path.isdir(self.config.data["installdir"]):
                    self.err += "Config, Path Error: Path '%s' specified for 'installdir' does not exist. \n" % (self.config.data["installdir"],)
                for key in self.config.data:
                    if key not in conf.templateKeys["config"]["required"] and key not in conf.templateKeys["config"]["optional"]:
                        self.err += "Config, Key Error: Invalid key '%s' specified.  Allowed keys are '%s'. \n" % (key, conf.templateKeys["config"]["required"] + conf.templateKeys["config"]["optional"])
                if "maxwalltime" in self.config.data:
                    for key in self.config.data["maxwalltime"]:
                        if key not in conf.templateKeys["config"]["maxwalltime"]["optional"]:
                            self.err += "Config, Key Error: Invalid key '%s' specified for 'maxwalltime'.  Allowed keys are '%s'. \n" % (key, conf.templateKeys["config"]["maxwalltime"]["optional"])
                if "notify" in self.config.data:
                    for key in conf.templateKeys["config"]["notify"]["required"]:
                        if key not in self.config.data["notify"]:
                            self.err += "Config, Key Error: Required key '%s' not specified for 'notify'.  \n" % (key,)
                        elif key == "pegasus_home":
                            if not os.path.isfile(self.config.data["notify"]["pegasus_home"] + "/notification/email"):
                                self.err += "Config, Path Error: Required key '%s' has invalid value.  Path '%s' does not exist." % (key, self.config.data["notify"]["pegasus_home"] + "/notification/email")
                for namespace in self.config.data["profile"]:
                    for key in self.config.data["profile"][namespace]:
                        if "path" in key.lower():
                            if not isinstance(self.config.data["profile"][namespace][key], list):
                                self.config.data["profile"][namespace][key] = [self.config.data["profile"][namespace][key]]
                            if "osg" in self.config.data and namespace == "env":
                                # We ignore osg enviornment variables
                                pass
                            else:
                                for path in self.config.data["profile"][namespace][key]:
                                    if not os.path.isdir(path):
                                        self.err += "Config, Path Error: Path '%s' specified for namespace '%s', key '%s' does not exist. \n" % (path, namespace, key)
                if "osg" in self.config.data:
                    if "profile" not in self.config.data:
                        self.config.data["profile"] = {}
                    if "env" not in self.config.data["profile"]:
                        self.config.data["profile"]["env"] = {}
                    for key in conf.templateKeys["config"]["osg"]["required"]:
                        if key not in self.config.data["osg"]:
                            self.err += "Config, OSG Key Error: Specifying 'osg' requires the key '%s' to be defined." % (key,)
                        elif not os.path.isfile(self.config.data["osg"][key]):
                            self.err += "Config, OSG Path Error: Path '%s' specified for key '%s' does not exist. \n" % (self.config.data["osg"][key], key)
            else:
                self.err += "Config, Template Error: Config file does not have all the required keys:" + str(conf.templateKeys["config"]["required"]) + " \n"
        else:
            self.err += "Config, Load Error: Could not load configuration info. \n"
        return

    def _getImageTypes(self):
        return [x["imtype"] for x in self.db.conn.execute("select distinct imtype from images")]

    def validate(self):
        """
        This should be overloaded again.
        """
        return


class ImageProcessor(Workflow):

    def __init__(self, template, config, database):
        super(ImageProcessor, self).__init__(template, config, database)
        return

    def validate(self):
        """
            Validates all inputted files.  Because the workflow is
            dependenet on the config and metadata, workflow validation
            is only done if config and metadata validate successfully.
        """
        if not self.err:
            self._validateConfig()
            self._validateDB()
            if not self.err:
                self._validateWorkflow()
        return

    def _validateWorkflowJobs(self):
        types = self._getImageTypes()
        for type in self.workflow.data["workflows"]:
            if type not in types:
                self.err += "Imgproc, Type Error: Workflow definition exists for type '%s', no images have been loaded for that type. \n" % (type,)
            self.workflow.rawFiles[type] = []
            names = [x["name"] for x in self.workflow.data["workflows"][type]]
            if len(names) == len(set(names)):
                for job in self.workflow.data["workflows"][type]:
                    if set(conf.templateKeys["imgproc"]["job"]["required"]) <= set(job.keys()):
                        for key in job.keys():
                            if key not in conf.templateKeys["imgproc"]["job"]["required"] and key not in conf.templateKeys["imgproc"]["job"]["optional"]:
                                self.err += "Imgproc, Key Error: Job '%s' has invalid key '%s' specified.  Allowed keys are '%s'. \n" % (job["name"], key, conf.templateKeys["imgproc"]["job"]["required"] + conf.templateKeys["imgproc"]["job"]["optional"])
                        if job["executable"] in conf.valid:
                            if not os.path.isfile(self.config.data["installdir"] + "/" + job["executable"]):
                                self.err += "Imgproc, Path Error: Job '%s' executable '%s' does not exist. \n" % (job["name"], self.config.data["installdir"] + "/" + job["executable"])
                            if conf.valid[job["executable"]]["type"] == "imgproc":
                                self._validateArguments(job, type)
                                self._validateDependencies(job, type)
                            else:
                                self.err += "Imgproc, Executable Error: Job '%s' has invalid executable '%s' specified.  Only image processing scripts are allowed. \n" % (job["name"], job["executable"])
                        else:
                            self.err += "Imgproc, Executable Error: Job '%s' has non-existant executable '%s' specified. \n" % (job["name"], job["executable"])
                    else:
                        self.err += "Imgproc, Key Error: Job '%s' doesn't have all required keys: '%s'.  \n" % (job["name"], conf.templateKeys["imgproc"]["job"]["required"])
            else:
                self.err += "Imgproc, Name Error: Cannot parse ambiguous workflow.  Workflow defined for type '%s' contains multiple jobs with the same name. \n" % (type,)
        return

    def _validateWorkflowOptions(self):
        if set(conf.templateKeys["imgproc"]["options"]["required"]) <= set(self.workflow.data["options"].keys()):
            for key in self.workflow.data["options"]:
                if key not in conf.templateKeys["imgproc"]["options"]["required"] and key not in conf.templateKeys["imgproc"]["options"]["optional"]:
                    self.err += "Imgproc, Option Error: Invalid option '%s' specified. \n " % (key,)
        else:
            self.err += "Imgproc, Option Error: Option specification doesn't have all required keys: '%s'. \n " % (conf.templateKeys["imgproc"]["options"]["required"],)
        return

    def _validateWorkflowExtract(self):
        if set(conf.templateKeys["imgproc"]["extract"]["required"]) <= set(self.workflow.data["extract"].keys()):
            for key in self.workflow.data["extract"]:
                if key not in conf.templateKeys["imgproc"]["extract"]["required"] and key not in conf.templateKeys["imgproc"]["extract"]["optional"]:
                    self.err += "Imgproc, Extract Error: Invalid option '%s' specified. \n " % (key,)

            ## Manual hist-bin validation ##
            if "histogram-bin" in self.workflow.data["extract"]:
                hist = {}
                hist["executable"] = "ih-stats-histogram-bin"
                hist["inputs"] = ["images"]
                hist["outputs"] = ["none"]
                hist["name"] = "histogramBin"
                hist["arguments"] = self.workflow.data["extract"]["histogram-bin"].copy()
                self._validateArguments(hist, "extract")
                if not self.err:
                    if set(self.workflow.data["extract"]["histogram-bin"]["--group"].keys()) != set(self.workflow.data["extract"]["histogram-bin"]["--channels"].keys()):
                        self.err += "Imgproc, Extract Error: Histogram bin group names don't match between '--group' and '--channels'. \n"
                else:
                    pass

            if not self.err:
                for type in self.workflow.data["extract"]["workflows"]:
                    if type in self.workflow.data["workflows"]:
                        if "--dimfromroi" in self.workflow.data["extract"]["workflows"][type]["arguments"]:
                            fpath = self.workflow.data["extract"]["workflows"][type]["arguments"]["--dimfromroi"]
                            if os.path.isfile(fpath):
                                self.workflow.rawFiles[type].append(fpath)
                        job = self.workflow.data["extract"]["workflows"][type]
                        job["executable"] = "ih-extract"
                        job["outputs"] = ["none"]
                        job["name"] = type + "_extract"
                        self._validateArguments(job, type)
                        self._validateDependencies(job, type)
                    else:
                        self.err += "Imgproc, Extract Error: Extraction specified for type '%s'.  No processing workflow defined for that type." % (type,)
            else:
                pass
        else:
            self.err += "Imgproc, Extract Error: Extract specification doesn't have all required keys: '%s'. \n" % (conf.templateKeys["imgproc"]["extract"]["required"],)
        return


    def _validateWorkflow(self):
        """
            Validates the input workflow template, making sure all required
            keys are inputted, and all names resolve
        """
        if set(conf.templateKeys["imgproc"]["required"]) <= set(self.workflow.data.keys()):
            for key in self.workflow.data.keys():
                if key not in conf.templateKeys["imgproc"]["required"] and key not in conf.templateKeys["imgproc"]["optional"]:
                    self.err += "Imgproc, Key error: Invalid key '%s' specified.  Allowed keys are '%s'. \n" % (key, conf.templateKeys["imgproc"]["required"] + conf.templateKeys["imgproc"]["optional"])
            try:
                self._validateWorkflowJobs()
                self._validateWorkflowOptions()
                self._validateWorkflowExtract()
            except Exception as e:
                print traceback.format_exc()
                self.err += "Validation halted.  Fix current issues before re-running. \n"
                self.printErrors()
        else:
            self.err += "Imgproc, Template Error: Workflow file does not have all the required keys: '%s'. \n" % (conf.templateKeys["imgproc"]["required"],)
        return

    def _validateDB(self):
        """
            Validates the inputted metadata db, ensuring the appropriate
            column names are in the database.
        """
        if self.db:
            cols = [row[1] for row in self.db.conn.execute("PRAGMA table_info(images)")]
            if set(conf.outHeaders) <= set(cols):
                testpath = self.db.conn.execute("select path from images limit 0,1").next()[0]
                if not os.path.isfile(testpath):
                    self.err += "DB, Path Error: Test file: '%s' could not be found. \n" % (testpath,)
            else:
                 self.err += "DB, Key Error: Meta-data file does not have all the required headers: %s \n" % (str(conf.outHeaders),)
        else:
            self.err += "DB, Load Error: Could not connect to meta-data db. \n"
        return



class Statistics(Workflow):

    def __init__(self, template, config, database):
        super(Statistics, self).__init__(template, config, database)

        return

    def validate(self):
        """
            Validates all inputted files.  Because the workflow is
            dependenet on the config and metadata, workflow validation
            is only done if config and metadata validate successfully.
        """
        if not self.err:
            self._validateConfig()
            self._validateDB()
            if not self.err:
                self._validateWorkflow()
        return

    def _validateDB(self):
        try:
            result = self.db.conn.execute("select * from images")
            cols = [row[1] for row in self.db.conn.execute("PRAGMA table_info(images)")]
            if not set(cols) > set(conf.outHeaders):
                self.err += "Stats, Database Error: Database specified does not contain any numeric columns, process images first. \n"
            if not set(conf.outHeaders) <= set(cols):
                self.err += "Stats, Database Error: Database specified does not contain all the required column names: '%s'. \n " % (conf.outHeaders,)
            if not result.fetchone():
                self.err += "Stats, Database Error: Database specified does not contain an image table with any entries. \n"
        except sqlite3.DatabaseError:
            self.err += "Stats, Database Error: File specified is not a valid database file. \n"
        return

    def _validateWorkflow(self):
        if self.workflow.data:
            for key in self.workflow.data.keys():
                if key not in conf.templateKeys["stats"]["required"] and key not in conf.templateKeys["stats"]["optional"]:
                    self.err += "Stats, Key error: Invalid key '%s' specified.  Allowed keys are '%s'. \n" % (key, conf.templateKeys["stats"]["required"] + conf.templateKeys["stats"]["optional"])
            for type in self.workflow.data["workflows"]:
                self.workflow.rawFiles[type] = []
                names = [x["name"] for x in self.workflow.data["workflows"][type]]
                if len(names) == len(set(names)):
                    for job in self.workflow.data["workflows"][type]:
                        if set(conf.templateKeys["stats"]["job"]["required"]) <= set(job.keys()):
                            for key in job.keys():
                                if key not in conf.templateKeys["stats"]["job"]["required"] and key not in conf.templateKeys["stats"]["job"]["optional"]:
                                    self.err += "Stats, Key Error: Job '%s' has invalid key '%s' specified.  Allowed keys are '%s'. \n" % (job["name"], key, conf.templateKeys["stats"]["job"]["required"] + conf.templateKeys["stats"]["job"]["optional"])
                            if job["executable"] in conf.valid:
                                if conf.valid[job["executable"]]["type"] == "statistics":
                                    if os.path.isfile(self.config.data["installdir"] + "/" + job["executable"]):
                                        self._validateArguments(job, type)
                                        self._validateDependencies(job, type)
                                    else:
                                        self.err += "Stats, Path Error: Job '%s' executable '%s' does not exist. \n" % (job["name"], self.config["installdir"] + "/" + job["executable"])
                                else:
                                    self.err += "Stats, Executable Error: Job '%s' has invalid executable '%s' specified.  Only statistics scripts are allowed. \n" % (job["name"], job["executable"])
                            else:
                                self.err += "Stats, Executable Error: Job '%s' executable '%s' is not a valid executable. \n" % (job["name"], job["executable"])
                        else:
                            self.err += "Stats, Key Error: Job '%s' doesn't have all required keys: '%s'. \n" % (job["name"], conf.templateKeys["stats"]["job"]["required"])
                else:
                    self.err += "Stats, Name Error: Cannot parse ambiguous workflow.  Workflow defined for type '%s' contains multiple jobs with the same name. \n" % (type,)
        else:
            self.err += "Stats, Load Error: Could not load stats file. \n"
        return

class ImageLoader(Validator):

    def __init__(self, f):
        super(ImageLoader, self).__init__(f, "file")
        return

    def validate(self):
        """
            Validates the given template.
        """
        if self.isValid():
            if set(conf.templateKeys["loading"]["required"]) <= set(self.data.keys()):
                for key in self.data:
                    if key not in conf.templateKeys["loading"]["required"] and key not in conf.templateKeys["loading"]["optional"]:
                        self.err += "Loader, Key Error: Invalid key '%s' specified. \\n" % (key,)

                if not self.err:
                    for key in self.data:
                        d = {
                            "path": self._validatePath,
                            "base": self._validateBase,
                            "data": self._validateData,
                            "translations": self._validateTranslations,
                            "order": self._validateOrder,
                            "filetype": self._validateFtype,
                        }[key]()
            else:
                self.err += "Loader, Key Error: Not all required keys specified: '%s' \n" % (conf.templateKeys["loading"]["required"])
        return

    def _validateFtype(self):
        if self.data["filetype"]:
            if self.data["filetype"] not in conf.imageExtensions:
                self.err += "Loader, File Error: Value '%s' specified for 'filetype' is invalid.  Must be on of '%s'. \n" % (self.data["filetype"], conf.imageExtensions)
        else:
            self.err += "Loader, File Error: No value specified for 'filetype'. \n"

    def _validatePath(self):
        """
            Validates the path.  Because the path contains potential references, path existence
            cannot be checked at this stage.
        """
        if not self.data["path"]:
            self.err += "Loader, Path Error: No value specified for 'path'.\n"
        return

    def _validateBase(self):
        """
            Validates the base walk path.  Checks existence of a value, if the value
            is a subset of path, and if the path exists.
        """
        if self.data["base"]:
            if self.data["base"] in self.data["path"]:
                if not os.path.exists(self.data["base"]):
                    self.err += "Loader, Base Error: Value for 'base' is not a valid path. \n"
            else:
                self.err += "Loader, Base Error: Value for 'base' is not a subset of 'path'. \n"
        else:
            self.err += "Loader, Base Error: No value specified for 'base'.\n"
        pass

    def _validateData(self):
        """
            Validates the data!  Uses a seperate function for each data type.
        """
        if self.data["data"]:
            for key in self.data["data"]:
                if "type" in self.data["data"][key]:
                    if self.data["data"][key]["type"] in conf.templateKeys["loading"]["data"]["type"]:
                        if key in self.data["order"]:
                            d = {
                                "value": self._validateDataValue,
                                "file": self._validateDataFile,
                                "date": self._validateDataDate
                            }[self.data["data"][key]["type"]](key)
                        else:
                            self.err += "Loader, Data Error: Definition for data key '%s', no corresponding definition in 'order'. \n" % (key,)
                    else:
                        self.err += "Loader, Data Error: Invalid type '%s' specified for data key '%s', must be one of '%s'. \n" % (self.data["data"][key]["type"], key, conf.templateKeys["loading"]["data"]["type"])
                else:
                    self.err += "Loader, Data Error: No value specified for 'type' of data key '%s'. \n" % (key,)
        else:
            self.err += "Loader, Value Error: No value specified for 'data'. \n"
        pass

    def _validateDataValue(self, key):
        """
            Validates type 'value'.  Values can be combinations of hard-coded text
            as well as references to path identifiers  Checks existence of a value,
            if all the references are valid, and if translation is specified, that
            translations exist.
        """
        if self.data["data"][key]["value"]:
            if set(conf.templateKeys["loading"]["data"]["value"]["required"]) <= set(self.data["data"][key].keys()):

                for subkey in self.data["data"][key]:
                    if subkey not in conf.templateKeys["loading"]["data"]["value"]["required"] and subkey not in conf.templateKeys["loading"]["data"]["value"]["optional"]:
                        self.err += "Loader, Data Value Error: Invalid key '%s' specified for data key '%s'. \n" % (subkey,key)

                m = re.findall(r"%(\w+)%", self.data["data"][key]["value"])
                for val in m:
                    if "%" + val + "%" not in self.data["path"]:
                        self.err += "Loader, Data Value Reference Error: Could not reference identifier '%s' for data key '%s'. \n" % (val, key)
                if "translate" in self.data["data"][key]:
                    if key not in self.data["translations"]:
                        self.err += "Loader, Data Value Translation Error: Can't translate data key '%s', no translation specified. \n" % (key,)
                if "case" in self.data["data"][key]:
                    if self.data["data"][key]["case"] not in ["lower", "upper"]:
                        self.err += "Loader, Data Value Error: Case for key '%s' is defined as '%s', should be either 'lower' or 'upper'. \n" % (key, self.data["data"][key]["case"])
            else:
                print set(conf.templateKeys["loading"]["data"]["value"]["required"])
                print set(self.data["data"][key].keys())
                self.err += "Loader, Data Value Error: Data key '%s' does not have all required keys '%s'. \n" % (key, conf.templateKeys["loading"]["data"]["value"]["required"])
        else:
            self.err += "Loader, Data Value Error: No value specified for data key '%s'. \n " % (key,)
        return

    def _validateDataFile(self, key):
        """
            Validates type 'file'.  Checks to ensure that a file is specified,
            that file exists, and that a value is specified for key, keyColumn,
            and valueColumn.  Assumes the file is of csv format.
        """
        if set(conf.templateKeys["loading"]["data"]["file"]["required"]) <= set(self.data["data"][key].keys()):

            for subkey in self.data["data"][key]:
                if subkey not in conf.templateKeys["loading"]["data"]["file"]["required"] and subkey not in conf.templateKeys["loading"]["data"]["file"]["optional"]:
                    self.err += "Loader, Data File Error: Invalid key '%s' specified for data key '%s'. \n" % (subkey, key)

            if os.path.exists(self.data["data"][key]["value"]):
                with open(self.data["data"][key]["value"], "r") as rh:
                    firstline = rh.readline()
                    sep = self.data["data"][key]["separator"] if "separator" in self.data["data"][key] else ","
                    maxlength = len(firstline.split(sep))
                    if maxlength <= self.data["data"][key]["keyColumn"]:
                        self.err += "Loader, Data File Error: Key column for data key '%s' out of range. \n" % (key,)
                    if maxlength <= self.data["data"][key]["valueColumn"]:
                        self.err += "Loader, Data File Error: Value column for data key '%s' out of range. \n" % (key,)
            else:
                self.err += "Loader, Data Filer Error: File '%s' specified for data key '%s' does not exist. \n" % (self.data["data"][key]["value"], key)

            m = re.findall(r"%(\w+)%", self.data["data"][key]["key"])
            for val in m:
                if "%" + val + "%" not in self.data["path"]:
                    self.err += "Loader, Data Value Reference Error: Could not reference identifier '%s' for data key '%s'. \n" % (val, key)
        else:
            self.err += "Loader, Data File Error: Data key '%s' does not have all required keys '%s'. \n" % (key, conf.templateKeys["loading"]["data"]["file"]["required"])
        return

    def _validateDataDate(self, key):
        """
            Validates type 'date'.  Checks to ensure that a value exists, a format
            exists, and that the format is semi-valid.  True validity of format is
            checked when values are given in the crawling step.
        """
        if set(conf.templateKeys["loading"]["data"]["date"]["required"]) <= set(self.data["data"][key].keys()):

            for subkey in self.data["data"][key]:
                if subkey not in conf.templateKeys["loading"]["data"]["date"]["required"] and subkey not in conf.templateKeys["loading"]["data"]["date"]["optional"]:
                    self.err += "Loader, Data Date Error: Invalid key '%s' specified for data key '%s'. \n" % (subkey, key)

            if not set(self.data["data"][key]["format"]) <= set(conf.dateFormat + conf.dateSep):
                self.err += "Loader, Data Date Error: Invalid values in data key '%s', format only supports '%s'. \n" % (key, conf.dateFormat + conf.dateSep)

            m = re.findall(r"%(\w+)%", self.data["data"][key]["value"])
            for val in m:
                if "%" + val + "%" not in self.data["path"]:
                    self.err += "Loader, Data Value Reference Error: Could not reference identifier '%s' for data key '%s'. \n" % (val, key)
        else:
            self.err += "Loader, Data Date Error: Data key '%s' does not have all required keys '%s'. \n" % (key, conf.templateKeys["loading"]["data"]["date"]["required"])
        return

    def _validateTranslations(self):
        """
            Nothing to see here, move along.
        """
        for key in self.data["translations"]:
            if key not in self.data["data"]:
                self.err += "Loader, Translation Error: Translation defined for '%s', however, no such data value exists. \n" % (key,)
        return

    def _validateOrder(self):
        """
            Validates the order -- the order you want to write values to the csv.
            Checks existence of a value, as well as ensuring all values specified
            have corresponding data values.
        """
        if self.data["order"]:
            if not set(self.data["order"]) == set(self.data["data"].keys() + ["path"]):
                self.err += "Loader, Order Error: Order must contain ALL data keys AND 'path'. \n"
        else:
            self.err += "Loader, Order Error: No value specified for order. \n"
        return
"""
This file is part of Image Harvest.

Image Harvest is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Image Harvest is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Image Harvest.  If not, see <http://www.gnu.org/licenses/>.
"""

import os
import conf
import sys
import json
import re
import csv
import datetime
import sqlite3
import shutil
import errno
import textwrap
import copy
import ih.validator
import getpass
import xml.dom.minidom
from Pegasus.DAX3 import *


class Workflow(object):

    """
        Generic workflow creation class.
    """

    def __init__(self, jobhome, basename = None):
        """
            :param jobhome: The base directory for job submission.
            :type jobhome: str

            Creates a Workflow class based on the input directory.  Only loads and
            validates the config file by default.
        """
        self.err = ""
        self.jobhome = os.path.abspath(jobhome)
        self.jobname = os.path.basename(os.path.dirname(self.jobhome + "/"))
        self.basename = basename
        self.dax = ADAG(self.jobname)
        self.executables = {}
        self.files = {self.dax: {}}
        self.jobs = {self.dax: {}}
        self.deps = {}
        return

    def _loadInputFiles(self):
        """
            Loads the input files into the appropriate variables
            This should be overloaded.
        """
        return

    def _isFile(self, name, dax, imtype, inout):
        """
            Convenience function to check if a given file exists.
        """
        if name in self.files[dax][imtype][inout]:
            return True
        return False

    def _isJob(self, name, dax):
        """
            Convenience function to check if a given job exists.
        """
        if name in self.jobs[dax]:
            return True
        return False

    def _isExecutable(self, name):
        """
            Convenience function to check if a given executable exists.
        """
        if name in self.executables:
            return True
        return False

    def _createSetup(self):
        """
            Creates the base structure for job submission.  Everything is contained
            within a folder based on the current timestamp.
        """
        self.basepath = self.jobhome + "/" + self.basename if self.basename else self.jobhome + "/" + datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
        if not os.path.exists(self.basepath):
            os.makedirs(self.basepath)
        if not os.path.exists(self.basepath + "/input"):
            os.makedirs(self.basepath + "/input")
        if not os.path.exists(self.basepath + "/output"):
            os.makedirs(self.basepath + "/output")
        if not os.path.exists(self.basepath + "input/templates"):
            os.makedirs(self.basepath + "/input/templates")
        if not os.path.exists(self.basepath + "/input/rawfiles"):
            os.makedirs(self.basepath + "/input/rawfiles")
        if "osg" in self.config:
            if not os.path.exists(self.basepath + "/staging"):
                os.makedirs(self.basepath + "/staging")
        return

    def _addFile(self, name, imtype, inout, path = None, dax = None, derivedPath = None):
        """
            Adds the inputted file to the dax, as well as the internal variable self.files
        """
        dax = dax if dax else self.dax
        if not self._isFile(name, dax, imtype, inout):
            self.files[dax][imtype][inout][name] = {"file": File(name), "path": ""}
            if inout == "input":
                self.files[dax][imtype][inout][name]["path"] = path if path else name
                self.files[dax][imtype][inout][name]["file"].addPFN(PFN("file://" + path.replace(" ","%20"), "local"))
                if derivedPath:
                    self.files[dax][imtype][inout][name]["derivedPath"] = derivedPath
                dax.addFile(self.files[dax][imtype][inout][name]["file"])
        return

    def _addJob(self, jobname, executable, inputs, outputs, arguments, dependencies = None, dax = None, label = None, walltime = None):
        dax = dax if dax else self.dax
        if not self._isJob(jobname, dax):
            if "osg" in self.config:
                self.jobs[dax][jobname] = Job("osg-wrapper.sh")
            else:
                self.jobs[dax][jobname] = Job(executable)
            dax.addJob(self.jobs[dax][jobname])
            if "osg" in self.config:
                self.jobs[dax][jobname].uses("ih.tar.gz", link = Link.INPUT)
            for key in inputs:
                self.jobs[dax][jobname].uses(inputs[key]["file"], link = Link.INPUT)
            for key in outputs:
                self.jobs[dax][jobname].uses(outputs[key]["file"], link = Link.OUTPUT, transfer = outputs[key]["transfer"])
            arglist = []
            if "osg" in self.config:
                arglist.append("./ih-" + self.config["version"] + "/scripts/" + executable)
            for arg in arguments:
                arglist.append(arg)
                if str(arguments[arg]) in inputs:
                    arglist.append(inputs[str(arguments[arg])]["file"])
                elif str(arguments[arg]) in outputs:
                    arglist.append(outputs[str(arguments[arg])]["file"])
                else:
                    if "osg" in self.config:
                        arglist.append("'" + str(arguments[arg]) + "'")
                    else:
                        arglist.append(str(arguments[arg]))
            self.jobs[dax][jobname].addArguments(*arglist)
            if dependencies:
                for depend in dependencies:
                    dax.depends(child = self.jobs[dax][jobname], parent = self.jobs[dax][depend])
            if label:
                self.jobs[dax][jobname].profile(Namespace.PEGASUS, "label", label)
            if walltime:
                self.jobs[dax][jobname].profile(Namespace.GLOBUS, "maxwalltime", walltime)
        return

    def create(self):
        """
            Creates a new pegasus submission directory based on the current timestamp,
            and populates it with all required information to submit a pegasus job.
            This function should be overloaded.
        """
        return


class Statistics(Workflow):

    """
        A class specifically for statistics workflow
        validation and submission.
    """

    def __init__(self, jobhome, basename, validOnly = False):
        super(Statistics, self).__init__(jobhome, basename)
        self.configfile = jobhome + "/input/" + conf.configFile
        self.statsfile = jobhome + "/input/" + conf.statsFile
        self.dbfile = jobhome + "/" + basename + "/output/" + conf.outputdb
        self.validOnly = validOnly
        self.rawFiles = {}
        self._loadInputFiles()
        return

    def _getImageTypes(self):
        return [x[0] for x in self.db.execute("select distinct imtype from images")]


    def _loadInputFiles(self):
        self.validator = ih.validator.Statistics(self.statsfile, self.configfile, self.dbfile)
        if not self.validator.isValid() or self.validOnly:
            self.validator.printErrors()
            sys.exit()
        else:
            self.stats = self.validator.workflow.data
            self.db = self.validator.db.conn
            self.config = self.validator.config.data
            self.rawFiles = self.validator.workflow.rawFiles
        return

    def _copyFiles(self):
        """
            Copies all input files to the correct location.
        """
        if not os.path.exists(self.basepath + "/input/stats"):
            os.makedirs(self.basepath + "/input/stats")
        shutil.copyfile(self.statsfile, self.basepath + "/input/templates/" + os.path.basename(self.statsfile))
        return

    def _loadFiles(self, loc):
        """
            Loads all files (input and output) into the dax and the self.files variable
        """
        self.files[self.dax] = {}
        self.files[self.dax]["raw"] = {"input": {}, "output": {}}
        self._addFile("output.db", "raw", "input", self.basepath + "/output/output.db")
        for type in self.stats["workflows"]:
            self.files[self.dax][type] = {"input": {}, "output": {}}
            for f in self.rawFiles[type]:
                shutil.copyfile(f, self.basepath + "/input/rawfiles/" + os.path.basename(f))
                self._addFile(os.path.basename(f), type, "input", self.basepath + "/input/rawfiles/" + os.path.basename(f))
        return

    def _loadExecutables(self, os="linux", arch="x86_64"):
        """
            Loads all executables (as specified in the conf) into
            the dax, as well as the internal self.executables variable
        """
        for ex in conf.valid:
            if "osg" in self.config:
                self.executables[ex] = Executable(name=ex, os=os, arch=arch, installed=False)
            else:
                self.executables[ex] = Executable(name=ex, os=os, arch=arch)
            self.executables[ex].addPFN(PFN("file://" + self.config["installdir"] + "/" + ex, "local"))
            #if "cluster" in self.config:
            #     self.executables[ex].addProfile(Profile(Namespace.PEGASUS, "clusters.size", self.config["cluster"]))
            self.dax.addExecutable(self.executables[ex])
        return

    def _loadNotify(self):
        if "notify" in self.config:
            self.dax.invoke(When.AT_END, self.basepath + "/input/stats/notify.sh")
            with open(self.basepath + "/input/stats/notify.sh", "w") as wh:
                notify = textwrap.dedent("""\
                        #!/bin/bash
                        %s/notification/email -t %s --report=pegasus-analyzer
                """ % (self.config["notify"]["pegasus_home"], self.config["notify"]["email"]))
                wh.write(notify)
            os.chmod(self.basepath + "/input/stats/notify.sh", 0755)
        return

    def _loadJobInputs(self, job, type, basename, extension, save = False):
        inputs = {}
        for i,input in enumerate(job["inputs"]):
            ftype = conf.valid[job["executable"]]["inputs"][i]
            if input in self.rawFiles[type]:
                inputs[input] = {"file": os.path.basename(input), "transfer": save}
            else:
                ex = extension if job["name"] == self.workflow["workflows"][type][0]["name"] else conf.fileExtensions[ftype]
                inputs[input] = {"file": basename + "_" + input + ex, "transfer": save}
        return inputs

    def _loadJobInputs(self, job, type, save = False):
        inputs = {"output.db": {"file": "output.db", "transfer": True}}
        for i,input in enumerate(job["inputs"]):
            ftype = conf.valid[job["executable"]]["inputs"][i]
            if input in self.rawFiles[type]:
                inputs[input] = {"file": os.path.basename(input), "transfer": save}
        return inputs

    def _createDax(self, loc):
        """
            Loads all jobs into the dax, and then writes the dax
            to input/loc/stats.dax
        """
        serial = []
        if "maxwalltime" in self.config:
            if "stats" in self.config["maxwalltime"]:
                maxwalltime = self.config["maxwalltime"]["stats"]
            else:
                maxwalltime = None
        else:
            maxwalltime = None
        for type in self.stats["workflows"]:
            for job in self.stats["workflows"][type]:
                jobname = type + "_" + job["name"]
                job["arguments"]["--intable"] = type + "_" + job["arguments"]["--intable"] if job["arguments"]["--intable"] != "images" else job["arguments"]["--intable"]
                job["arguments"]["--outtable"] = type + "_" + job["arguments"]["--outtable"]
                job["arguments"]["--db"] = "output.db"
                inputs = self._loadJobInputs(job, type)
                outputs = {}
                depends = [type + "_" + depend for depend in job["depends"]] if "depends" in job else serial
                if self.stats["workflows"][type].index(job) == len(self.stats["workflows"][type]) - 1:
                    serial = [jobname]
                else:
                    serial = []
                self._addJob(jobname, job["executable"], inputs, outputs, job["arguments"], depends, walltime = maxwalltime)
        with open(self.basepath + "/" + loc + "/stats.dax", "w") as wh:
            self.dax.writeXML(wh)
        return

    def _createReplica(self, loc):
        """
            Creates the pegasus configuration replica catalog.  input/conf.rc
        """
        with open(self.basepath + "/" + loc + "/conf.rc", "w") as wh:
            pegasusrc = textwrap.dedent("""\
                        pegasus.catalog.site = XML
                        pegasus.catalog.site.file = %s/sites.xml

                        pegasus.condor.logs.symlink = false

                        pegasus.transfer.links = true

                        pegasus.data.configuration = %s

                        """ % (self.basepath + "/" + loc))
            wh.write(pegasusrc)
        return

    def _createSites(self, loc):
        """
            Creates the pegasus site catalog.  input/sites.xml
        """
        with open(self.basepath + "/" + loc + "/sites.xml", "w") as wh:
            if "osg" in self.config:
                sites = """\
                <sitecatalog version="3.0" xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd">
                    <site handle="local" arch="x86_64" os="LINUX">
                            <head-fs>
                                <scratch>
                                        <shared>
                                            <file-server protocol="file" url="file://" mount-point="%s"/>
                                            <internal-mount-point mount-point="%s"/>
                                        </shared>
                                    </scratch>
                                <storage>
                                        <shared>
                                            <file-server protocol="file" url="file://" mount-point="%s"/>
                                            <internal-mount-point mount-point="%s"/>
                                        </shared>
                                    </storage>
                            </head-fs>
                    """ % (self.basepath + "/work/stats/", self.basepath + "/work/stats/", self.basepath + "/output/", self.basepath + "/output/", self.basepath)
                sites += """\
                    </site>
                    <site handle="condorpool" arch="x86_64" os="LINUX">
                            <head-fs>
                                    <scratch />
                                    <storage />
                            </head-fs>
                """
            else:
                sites = """\
                 <sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-4.0.xsd" version="4.0">

                            <site  handle="local" arch="x86_64" os="LINUX">
                                <directory type="shared-scratch" path="%s">
                                    <file-server operation="all" url="file://%s" />
                                </directory>
                                <directory type="local-storage" path="%s">
                                    <file-server operation="all" url="file://%s" />
                                </directory>
                            """ % (self.basepath + "/work/stats/", self.basepath + "/work/stats/", self.basepath + "/output/", self.basepath + "/output/")
            for namespace in self.config["profile"]:
                for key in self.config["profile"][namespace]:
                    val = ":".join(self.config["profile"][namespace][key]) if "path" in key.lower() else self.config["profile"][namespace][key]
                    sites += """\n\t<profile namespace="%s" key="%s">%s</profile> """ % (namespace, key, val)
            sites += "</site></sitecatalog>"
            sites = sites.replace("\n","")
            wh.write("\n".join([line for line in xml.dom.minidom.parseString(sites).toprettyxml().split('\n') if line.strip()]))
        return

    def _createSubmit(self, loc):
        """
            Creates the pegasus submit script.  submit.sh
        """
        with open(self.basepath + "/" + loc + "/submit.sh", "w") as wh:

            submit = textwrap.dedent("""\
                    #!/bin/bash
                    %s
                    plan=`pegasus-plan \\
                    --conf "%s" \\
                    --sites "%s" \\
                    --dir "%s" \\
                    --output-site local \\
                    --dax "%s" \\
                    --randomdir \\""" % ("module unload python/2.7" if "osg" in self.config else "", self.basepath + "/" + loc + "/conf.rc", "condorpool" if "osg" in self.config else "local", self.basepath + "/work/stats", self.basepath + "/" + loc + "/stats.dax"))
            if "cluster" in self.config:
                submit += """--cluster horizontal \\\n"""
            submit += textwrap.dedent("""\
                    --submit`

                    status=`echo "$plan" | grep pegasus-status | tr -s ' '| cut -d ' ' -f 6`
                    echo -e "#!/bin/bash
                    pegasus-status -l $status" > status.sh
                    chmod 744 status.sh

                    remove=`echo "$plan" | grep pegasus-remove | tr -s ' '| cut -d ' ' -f 5`
                    echo -e "#!/bin/bash
                    pegasus-remove $remove" > remove.sh
                    chmod 744 remove.sh

                    echo "$plan"
                    echo "Alternatively, you can use the status & remove scripts in the current directory!"

                    """)
            wh.write(submit)
            os.chmod(self.basepath + "/" + loc + "/submit.sh", 0755)
        return

    def create(self):
        """
            Creates a new pegasus submission directory based on the current timestamp,
            and populates it with all required information to submit a pegasus job.
        """
        loc = "/input/stats/"
        self._createSetup()
        self._copyFiles()
        self._loadFiles(loc)
        self._loadExecutables()
        self._loadNotify()
        self._createDax(loc)
        self._createReplica(loc)
        self._createSites(loc)
        self._createSubmit(loc)
        return





class ImageProcessor(Workflow):

    """
        A class specifically for image processing workflow
        validation and submission.
    """

    def __init__(self, jobhome, basename = None, validOnly = False):
        """
            :param jobhome: The base directory for job submission.
            :type jobhome: str

            Creates a Workflow class based on the input directory.  Loads and validates all input files,
            and quits out if something doesn't exist or is defined innapropriately.
        """
        super(ImageProcessor, self).__init__(jobhome, basename)
        self.validOnly = validOnly
        self.workfile = jobhome + "/input/" + conf.workflowFile
        self.metafile = jobhome + "/input/" + conf.dbFile
        self.configfile = jobhome + "/input/" + conf.configFile
        self._loadInputFiles()
        self.exdax = ADAG("extract-" + self.jobname)
        self.jobs[self.exdax] = {}
        self.files[self.exdax] = {}
        return

    def _loadInputFiles(self):
        """
            Loads the input files into the appropriate variables
        """
        self.validator = ih.validator.ImageProcessor(self.workfile, self.configfile, self.metafile)
        if not self.validator.isValid() or self.validOnly:
            self.validator.printErrors()
            sys.exit()
        else:
            self.workflow = self.validator.workflow.data
            self.metadata = self.validator.db.conn
            self.config = self.validator.config.data
            self.rawFiles = self.validator.workflow.rawFiles
        return

    def _getImageTypes(self):
        """
            Convenience function to get all image types from the database.
        """
        return [x[0] for x in self.metadata.execute("select distinct imtype from images")]

    def _copyFiles(self):
        """
            Copies all input files to the correct location.
        """
        if not os.path.exists(self.basepath + "/input/imgproc"):
            os.makedirs(self.basepath + "/input/imgproc")
        shutil.copyfile(self.workfile, self.basepath + "/input/templates/" + os.path.basename(self.workfile))
        shutil.copyfile(self.configfile, self.basepath + "/input/templates/" + os.path.basename(self.configfile))
        shutil.copyfile(self.metafile, self.basepath + "/output/output.db")
        for type in self.rawFiles:
            for input in self.rawFiles[type]:
                shutil.copyfile(input, self.basepath + "/input/rawfiles/" + os.path.basename(input))
        if "osg" in self.config:
            shutil.copyfile(self.config["osg"]["tarball"], self.basepath + "/input/rawfiles/" + os.path.basename(self.config["osg"]["tarball"]))
        return

    def _loadFiles(self, loc):
        """
            Loads all files (input and output) into the dax and the self.files variable
        """
        with open(self.basepath + "/input/rawfiles/extract.json", "w") as wh:
            json.dump(self.workflow["extract"], wh)
        with open(self.basepath + "/" + loc + "/map.rc", "w") as wh:
            self.exInput = {"db": "img.db", "extract.json": "extract.json"}
            self.files[self.dax] = {}
            self.files[self.exdax] = {}
            self.files[self.exdax]["all"] = {"input": {}, "output": {}}
            self.files[self.dax]["raw"] = {"input": {}, "output": {}}
            self.files[self.exdax]["raw"] = {"input": {}, "output": {}}
            if "osg" in self.config:
                self._addFile("ih.tar.gz", "raw", "input", self.basepath + "/input/rawfiles/ih-" + self.config["version"] + ".tar.gz")
                self._addFile("ih.tar.gz", "raw", "input", self.basepath + "/input/rawfiles/ih-" + self.config["version"] + ".tar.gz", dax = self.exdax)
            self._addFile("img.db", "raw", "input", self.basepath + "/output/output.db")
            self._addFile("img.db", "all", "input", self.basepath + "/output/output.db", dax = self.exdax)
            self._addFile("img2.db", "raw", "output", self.basepath + "/output/img2.db")
            self._addFile("img2.db", "raw", "output", self.basepath + "/output/img2.db", dax = self.exdax)
            self._addFile("img3.db", "raw", "output", self.basepath + "/output/img3.db")
            self._addFile("img3.db", "raw", "output", self.basepath + "/output/img3.db", dax = self.exdax)
            self._addFile("img.log", "raw", "output", self.basepath + "/output/imgproc.log")
            self._addFile("extract.json", "raw", "input", self.basepath + "/input/rawfiles/extract.json")
            self._addFile("extract.json", "all", "input", self.basepath + "/input/rawfiles/extract.json", dax = self.exdax)
            for type in self.workflow["workflows"]:
                self.files[self.dax][type] = {"input": {}, "output": {}}
                for input in self.rawFiles[type]:
                    self._addFile(os.path.basename(input), type, "input", self.basepath + "/input/rawfiles/" + os.path.basename(input))
                wh.write("img.log file://" + self.basepath + "/output/imgproc.log pool=\"local\"\n")
                wh.write("img2.db file://" + self.basepath + "/output/img2.db pool=\"local\"\n")
                wh.write("img3.db file://" + self.basepath + "/output/img3.db pool=\"local\"\n")
                inputImages = [self.workflow["workflows"][type][0]["inputs"][i] for i,x in enumerate(conf.valid[self.workflow["workflows"][type][0]["executable"]]["inputs"]) if x == "image"]
                outputImages = [self.workflow["workflows"][type][z]["outputs"][i] for z in range(0, len(self.workflow["workflows"][type])) for i,x in enumerate(conf.valid[self.workflow["workflows"][type][z]["executable"]]["outputs"]) if x == "image"]
                outputFiles = dict((self.workflow["workflows"][type][z]["outputs"][i], conf.fileExtensions[conf.valid[self.workflow["workflows"][type][z]["executable"]]["outputs"][i]]) for z in range(0, len(self.workflow["workflows"][type])) for i,x in enumerate(conf.valid[self.workflow["workflows"][type][z]["executable"]]["outputs"]) if x != "image" and x != "none" and len(self.workflow["workflows"][type][z]["outputs"]) > i)
                for row in self.metadata.execute("select pegasusid, experiment, id, date, imgname, path from images where imtype=?", (type,)):
                    realname = row["path"].split("/")[-1].split(".")[0]
                    #derivedPath = row["experiment"].replace(" ","%20") + "/" + row["id"].replace(" ","%20") + "/" + row["date"].replace(" ","%20") + "/" + type + "/" + row["imgname"].replace(" ","%20") + "/"
                    derivedPath = row["experiment"].replace(" ","") + "/" + row["id"].replace(" ","") + "/" + row["date"].replace(" ","") + "/" + type + "/" + row["imgname"].replace(" ","") + "/"
                    for input in inputImages:
                        if "osg" in self.config:
                            self._addFile(row["pegasusid"] + "_" + input + "." + row["path"].split(".")[1], type, "input", row["path"], derivedPath = derivedPath + row["pegasusid"])
                        else:
                            self._addFile(derivedPath + row["pegasusid"] + "_" + input + "." + row["path"].split(".")[1], type, "input", row["path"], derivedPath = derivedPath + row["pegasusid"])
                    for output in outputImages:
                        if output in self.workflow["extract"]["workflows"][type]["inputs"]:
                            self.exInput[derivedPath + row["pegasusid"] + "_" + output + ".png"] = derivedPath + row["pegasusid"] + "_" + output + ".png"
                            self._addFile(derivedPath + row["pegasusid"] + "_" + output + ".png", "all", "input", self.basepath + "/output/" + derivedPath + row["pegasusid"] + "_" + output + ".png", dax = self.exdax)
                        self._addFile(derivedPath + row["pegasusid"] + "_" + output + ".png", type, "output")
                        wh.write(derivedPath + row["pegasusid"] + "_" + output + ".png file://" + self.basepath + "/output/" + derivedPath + row["pegasusid"].replace(" ","%20") + "_" + output + ".png pool=\"local\"\n")
                    for output in outputFiles:
                        outname = output + outputFiles[output]
                        self._addFile(derivedPath + row["pegasusid"] + "_" + outname, type, "output")
                        wh.write(derivedPath + row["pegasusid"] + "_" + outname + " file://" + self.basepath + "/output/" + derivedPath.replace("_","%20") + outname + " pool=\"local\"\n")
        return

    def _loadNotify(self):
        if "notify" in self.config:
            self.dax.invoke(When.AT_END, self.basepath + "/input/imgproc/notify.sh")
            self.exdax.invoke(When.AT_END, self.basepath + "/input/imgproc/notify.sh")
            with open(self.basepath + "/input/imgproc/notify.sh", "w") as wh:
                notify = textwrap.dedent("""\
                        #!/bin/bash
                        %s/notification/email -t %s --report=pegasus-analyzer
                """ % (self.config["notify"]["pegasus_home"], self.config["notify"]["email"]))
                wh.write(notify)
            os.chmod(self.basepath + "/input/imgproc/notify.sh", 0755)
        return

    def _loadExecutables(self, os="linux", arch="x86_64"):
        """
            Loads all executables (as specified in the conf) into
            the dax, as well as the internal self.executables variable
        """
        for ex in conf.valid:
            if "osg" in self.config:
                self.executables[ex] = Executable(name=ex, os=os, arch=arch, installed = False)
            else:
                self.executables[ex] = Executable(name=ex, os=os, arch=arch)
            self.executables[ex].addPFN(PFN("file://" + self.config["installdir"] + "/" + ex, "local"))
            #if "cluster" in self.config:
            #    val = int(self.config["cluster"] * conf.valid[ex]["weight"]) if "weight" in conf.valid[ex] else self.config["cluster"]
            #    self.executables[ex].addProfile(Profile(Namespace.PEGASUS, "clusters.size", val))
            self.dax.addExecutable(self.executables[ex])
            self.exdax.addExecutable(self.executables[ex])
        return

    def _loadJobInputs(self, job, type, basename, extension, save = False):
        inputs = {}
        for i,input in enumerate(job["inputs"]):
            ftype = conf.valid[job["executable"]]["inputs"][i]
            if input in self.rawFiles[type]:
                inputs[input] = {"file": os.path.basename(input), "transfer": save}
            else:
                ex = extension if job["name"] == self.workflow["workflows"][type][0]["name"] else conf.fileExtensions[ftype]
                if "osg" in self.config:
                    if input == "base":
                        inputs[input] = {"file": os.path.basename(basename) + "_" + input + ex, "transfer": save}
                    else:
                        inputs[input] = {"file": basename + "_" + input + ex, "transfer": save}
                else:
                    inputs[input] = {"file": basename + "_" + input + ex, "transfer": save}
        return inputs

    def _loadJobOutputs(self, job, type, basename, save):
        outputs = {}
        for i,output in enumerate(job["outputs"]):
            if output != "none":
                outputs[output] = {"file": basename + "_" + output + conf.fileExtensions[conf.valid[job["executable"]]["outputs"][i]], "transfer": save}
        return outputs

    #def _loadJobOutputs(self, job, type, basename):
    #    return dict((output, basename + "_" + output + conf.fileExtensions[conf.valid[job["executable"]]["outputs"][i]]) for i,output in enumerate(job["outputs"]) if output != "none")

    def _createDax(self, loc):
        """
            Loads all jobs into the dax, and then writes the dax
            to input/workflow.dax
        """
        exDep = {}
        exInput = {}
        clusternum = {}
        meancluster = {}
        excluster = {}

        if "maxwalltime" in self.config:
            if "images" in self.config["maxwalltime"]:
                maxwalltime = self.config["maxwalltime"]["images"]
            else:
                maxwalltime = None
        else:
            maxwalltime = None

        save = True if "save-steps" in self.workflow["options"] else False
        for type in self.workflow["workflows"]:
            exDep[type] = [[]]
            exInput[type] = [{}]
            jobnum = -1
            skipFirst = True
            clusternum[type] = 0
            meancluster[type] = 0
            excluster[type] = 0
            exNames = self.workflow["extract"]["workflows"][type]["depends"]
            for infile in [x for x in self.files[self.dax][type]["input"] if "." + x.split(".")[1] in conf.imageExtensions]:
                if "cluster" in self.config:
                    jobnum += 1
                    if jobnum == self.config["cluster"]:
                        jobnum = 0
                        clusternum[type] += 1
                    if ((clusternum[type] * 100 + jobnum) % int(self.config["cluster"] * 0.3)) == 0:
                        meancluster[type] += 1
                    if (jobnum % 50) == 0 and not skipFirst:
                        exDep[type].append([])
                        exInput[type].append({})
                        excluster[type] += 1
                    skipFirst = False
                extension = "." + infile.split(".")[1]
                realname = self.files[self.dax][type]["input"][infile]["path"].split("/")[-1].split(".")[0]
                derivedPath = self.files[self.dax][type]["input"][infile]["derivedPath"]
                for stepnum,job in enumerate(self.workflow["workflows"][type]):
                    jobname = derivedPath + "_" + job["name"]
                    inputs = self._loadJobInputs(job, type, derivedPath, extension)
                    if job["name"] in exNames:
                        outputs = self._loadJobOutputs(job, type, derivedPath, True)
                        exDep[type][excluster[type]].append(jobname)
                        reqFile = derivedPath + "_" + self.workflow["extract"]["workflows"][type]["inputs"][0] + ".png"
                        exInput[type][excluster[type]][reqFile] = {"file": reqFile, "transfer": save}
                        if "--dimfromroi" in self.workflow["extract"]["workflows"][type]["arguments"]:
                            if os.path.isfile(self.workflow["extract"]["workflows"][type]["arguments"]["--dimfromroi"]):
                                roiFile = os.path.basename(self.workflow["extract"]["workflows"][type]["arguments"]["--dimfromroi"])
                            else:
                                roiFile = derivedPath + "_" + self.workflow["extract"]["workflows"][type]["arguments"]["--dimfromroi"] + ".json"
                            exInput[type][excluster[type]][roiFile] = {"file": roiFile, "transfer": save}
                    else:
                        outputs = self._loadJobOutputs(job, type, derivedPath, save)
                    depends = [derivedPath + "_" + depend for depend in job["depends"]] if "depends" in job else []
                    if job["executable"] == "ih-meanshift":
                        self._addJob(jobname, job["executable"], inputs, outputs, job["arguments"], depends, label = type + "_step" + str(stepnum) + "_cluster" + str(meancluster[type]) if "cluster" in self.config else None, walltime = maxwalltime)
                    else:
                        self._addJob(jobname, job["executable"], inputs, outputs, job["arguments"], depends, label = type + "_step" + str(stepnum) + "_cluster" + str(clusternum[type]) if "cluster" in self.config else None, walltime = maxwalltime)


        maprc = open(self.basepath + "/" + loc + "/map.rc", "a")
        binDep = []
        aggIn = {}
        aggIn2 = {}
        for type in self.workflow["workflows"]:
            for q in range(0, excluster[type] + 1):
                arguments = self.workflow["extract"]["workflows"][type]["arguments"]
                if "--input" in arguments:
                    del arguments["--input"]
                if "--dimfromroi" in arguments:
                    del arguments["--dimfromroi"]
                    arguments["--dimfromroi"] = " ".join([x for x in exInput[type][q].keys() if ".json" in x])
                if "histogram-bin" in self.workflow["extract"]:
                    if type in [imtype for key in self.workflow["extract"]["histogram-bin"]["--group"] for imtype in self.workflow["extract"]["histogram-bin"]["--group"][key]]:
                        arguments["--colors"] = ""
                arguments["--db"] = "db"
                arguments["--createdb"] = ""
                arguments["--inputs"] = " ".join([x for x in exInput[type][q].keys() if ".png" in x])
                self._addFile(type + str(q) + ".db", type, "output")
                self._addFile(type + str(q) + "_2.db", type, "output")
                self._addJob(type + "_extract" + str(q), "ih-extract-multi", exInput[type][q], {"db": {"file": type + str(q) + ".db", "transfer": False}}, arguments, exDep[type][q], walltime = 180)
                self._addJob(type + "_extract" + str(q), "ih-extract-multi", exInput[type][q], {"db": {"file": type + str(q) + ".db", "transfer": False}}, arguments, [], dax = self.exdax, walltime = 180)
                maprc.write(type + str(q) + ".db" + " file://" + self.basepath + "/output/" + type + str(q) + ".db" + " pool=\"local\"\n")
                binDep.append(type + "_extract" + str(q))
                aggIn[type + str(q) + ".db"] = {"file": type + str(q) + ".db", "transfer": False}
                aggIn2[type + str(q) + "_2.db"] = {"file": type + str(q) + "_2.db", "transfer": False}
        aggIn["db"] = {"file": "img.db", "transfer": False}
        self._addJob("sql_aggregate1", "ih-sql-aggregate", aggIn, {"img2.db": {"file": "img2.db", "transfer": False if "histogram-bin" in self.workflow["extract"] else True}}, {"--db": "db", "--output": "img2.db", "--inputs": " ".join([aggIn[x]["file"] for x in aggIn if x != "db"])}, binDep, walltime = 180)
        self._addJob("sql_aggregate1", "ih-sql-aggregate", aggIn, {"img2.db": {"file": "img2.db", "transfer": False if "histogram-bin" in self.workflow["extract"] else True}}, {"--db": "db", "--output": "img2.db", "--inputs": " ".join([aggIn[x]["file"] for x in aggIn if x != "db"])}, binDep, dax = self.exdax, walltime = 180)

        if "histogram-bin" in self.workflow["extract"]:
            outputs = {}
            for name in self.workflow["extract"]["histogram-bin"]["--group"]:
                self._addFile(name + "_hist_bins.json", "raw", "output")
                maprc.write(name + "_hist_bins.json" + " file://" + self.basepath + "/output/" + name + "_hist_bins.json" + " pool=\"local\"\n")
                outputs[name + "_hist_bins.json"] = {"file": name + "_hist_bins.json", "transfer": True}
            self._addJob("bin_creation", "ih-stats-histogram-bin", {"db": {"file": "img2.db", "transfer": True}, "extract.json": {"file": "extract.json", "transfer": False}}, outputs, {"--db": "db", "--options": "extract.json", "--intable": "images", "--outtable": "histogramBins", "--jsonwrite": "", "--overwrite": ""}, ["sql_aggregate1"])
            self._addJob("bin_creation", "ih-stats-histogram-bin", {"db": {"file": "img2.db", "transfer": True}, "extract.json": {"file": "extract.json", "transfer": False}}, outputs, {"--db": "db", "--options": "extract.json", "--intable": "images", "--outtable": "histogramBins", "--jsonwrite": "", "--overwrite": ""}, ["sql_aggregate1"], dax = self.exdax)

            binDep = []
            map = {}

            for group in self.workflow["extract"]["histogram-bin"]["--group"]:
                for type in self.workflow["extract"]["histogram-bin"]["--group"][group]:
                    map[type] = group

            for type in self.workflow["workflows"]:
                for q in range(0, excluster[type] + 1):
                    arguments = {}
                    arguments["--db"] = "db"
                    arguments["--copydb"] = "copydb"
                    arguments["--inputs"] = " ".join(exInput[type][q].keys())
                    exInput[type][q]["db"] = {"file": type + str(q) + ".db", "transfer": False}
                    exInput[type][q]["binfile"] = {"file": map[type] + "_hist_bins.json", "transfer": False}
                    arguments["--bins"] = "binfile"
                    self._addJob(type + "_extractBins" + str(q), "ih-extract-multi", exInput[type][q], {"copydb": {"file": type + str(q) + "_2.db", "transfer": False}}, arguments, ["bin_creation"], walltime = 300)
                    self._addJob(type + "_extractBins" + str(q), "ih-extract-multi", exInput[type][q], {"copydb": {"file": type + str(q) + "_2.db", "transfer": False}}, arguments, ["bin_creation"], dax = self.exdax, walltime = 300)
                    binDep.append(type + "_extractBins" + str(q))
            aggIn2["db"] = {"file": "img2.db", "transfer": False}
            self._addJob("sql_aggregate2", "ih-sql-aggregate", aggIn2, {"img3.db": {"file": "img3.db", "transfer": True}}, {"--db": "db", "--output": "img3.db", "--inputs": " ".join([aggIn2[x]["file"] for x in aggIn2 if x != "db"])}, binDep, walltime = 180)
            self._addJob("sql_aggregate2", "ih-sql-aggregate", aggIn2, {"img3.db": {"file": "img3.db", "transfer": True}}, {"--db": "db", "--output": "img3.db", "--inputs": " ".join([aggIn2[x]["file"] for x in aggIn2 if x != "db"])}, binDep, dax = self.exdax, walltime = 180)

        z = 2 if "histogram-bin" in self.workflow["extract"] else 1
        indb = "img3.db" if "histogram-bin" in self.workflow["extract"] else "img.db"
        self._addJob("error-log", "ih-error-log", {"db": {"file": indb, "transfer": True}}, {"output": {"file": "img.log", "transfer": True}}, {"--db": "db", "--output": "output"}, ["sql_aggregate" + str(z)])
        with open(self.basepath + "/" + loc + "/workflow.dax", "w") as wh:
            self.dax.writeXML(wh)
        return

    def _createExtract(self, loc):
        """
            Creates the extraction step only dax!
        """
        #self._addJob("extractOnly", "ih-extract-all", self.exInput, {}, {"--db": "db", "--options": "extract.json"}, dax = self.exdax)
        with open(self.basepath + "/" + loc + "/extract.dax", "w") as wh:
            self.exdax.writeXML(wh)
        return

    def _createReplica(self, loc):
        """
            Creates the pegasus configuration replica catalog.  input/conf.rc
        """
        with open(self.basepath + "/" + loc + "/conf.rc", "w") as wh:
            pegasusrc = textwrap.dedent("""\
                        pegasus.catalog.site = XML
                        pegasus.catalog.site.file = %s/sites.xml

                        pegasus.condor.logs.symlink = false

                        pegasus.transfer.links = true

                        pegasus.data.configuration = %s

                        pegasus.dir.storage.mapper = Replica
                        pegasus.dir.storage.mapper.replica = File
                        pegasus.dir.storage.mapper.replica.file = %s/map.rc
                        """ % (self.basepath + "/" + loc, "nonsharedfs" if "osg" in self.config else "sharedfs", self.basepath + "/" + loc))
            if "osg" in self.config:
                pegasusrc += textwrap.dedent("""\
                    pegasus.stagein.clusters = 4
                    pegasus.stageout.clusters = 4
                    pegasus.transfer.threads = 4
                    pegasus.transfer.lite.threads = 4

                    dagman.maxjobs = 200
                """)
            wh.write(pegasusrc)
        return

    def _createSites(self, loc):
        """
            Creates the pegasus site catalog.  input/sites.xml
        """
        with open(self.basepath + "/" + loc + "/sites.xml", "w") as wh:
            if "osg" in self.config:
                userName = getpass.getuser()
                sites = """\
                <sitecatalog version="4.0" xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-4.0.xsd">
                    <site handle="local" arch="x86_64" os="LINUX">
                        <directory type="shared-scratch" path="%s">
                            <file-server operation="all" url="file://%s" />
                        </directory>
                        <directory type="local-storage" path="%s">
                            <file-server operation="all" url="file://%s" />
                        </directory>
                        <profile namespace="pegasus" key="SSH_PRIVATE_KEY">%s</profile>
                    </site>
                    <site handle="stash" arch="x86_64" os="LINUX">
                        <directory type="shared-scratch" path="%s">
                            <!-- <file-server operation="get" url="stash://%s"/> -->
                            <file-server operation="all" url="scp://%s@login02.osgconnect.net/%s"/>
                        </directory>
                    </site>
                    <site handle="isi_workflow" arch="x86_64" os="LINUX">
                        <directory type="shared-scratch" path="/nfs/ccg4/scratch-purge-no-backups/workflow.isi.edu/scratch2/%s/scratch">
                            <file-server operation="get" url="http://workflow.isi.edu/scratch2/%s/scratch"/>
                            <file-server operation="put" url="scp://%s@workflow.isi.edu/nfs/ccg4/scratch-purge-no-backups/workflow.isi.edu/scratch2/%s/scratch"/>
                        </directory>
                    </site>
                    <site handle="condorpool" arch="x86_64" os="LINUX">



                    """ % (self.basepath + "/work/imgproc/", self.basepath + "/work/imgproc/", self.basepath + "/output/", self.basepath + "/output/", self.config["osg"]["ssh"], self.basepath + "/staging/", "/".join((self.basepath + "/staging/").split("/")[2:]), userName, self.basepath + "/staging/", userName, userName, userName, userName)
            else:
                sites = """\
                 <sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-4.0.xsd" version="4.0">

                            <site  handle="local" arch="x86_64" os="LINUX">
                                <directory type="shared-scratch" path="%s">
                                    <file-server operation="all" url="file://%s" />
                                </directory>
                                <directory type="local-storage" path="%s">
                                    <file-server operation="all" url="file://%s" />
                                </directory>
                            """ % (self.basepath + "/work/imgproc/", self.basepath + "/work/imgproc/", self.basepath + "/output/", self.basepath + "/output/")
            for namespace in self.config["profile"]:
                for key in self.config["profile"][namespace]:
                    val = ":".join(self.config["profile"][namespace][key]) if "path" in key.lower() else self.config["profile"][namespace][key]
                    sites += """\n\t<profile namespace="%s" key="%s">%s</profile> """ % (namespace, key, val)
            sites += "</site></sitecatalog>"
            sites = sites.replace("\n","")
            wh.write("\n".join([line for line in xml.dom.minidom.parseString(sites).toprettyxml().split('\n') if line.strip()]))
        return

    def _createSubmit(self, loc):
        """
            Creates the pegasus submit script.  submit.sh
        """
        with open(self.basepath + "/" + loc + "/submit.sh", "w") as wh:
            submit = textwrap.dedent("""\
                    #!/bin/bash
                    if [ "$1" = "--extract-only" ] || [ "$1" = "--extract" ] || [ "$1" = "-e" ]; then
                        DAX="%s"
                    else
                        DAX="%s"
                    fi
                    plan=`pegasus-plan \\
                    --conf "%s" \\
                    --sites "%s" \\
                    --dir "%s" \\
                    --output-site local \\
                    --dax "$DAX" \\
                    --randomdir \\
                    """ % (self.basepath + "/" + loc + "/extract.dax", self.basepath + "/" + loc + "/workflow.dax", self.basepath + "/" + loc + "/conf.rc", "condorpool" if "osg" in self.config else "local", self.basepath + "/work/imgproc"))
            if "cluster" in self.config:
                submit += """--cluster label \\\n"""
            if "osg" in self.config:
                submit += """--staging-site stash \\\n"""
                submit += """--staging isi_workflow \\\n"""
                submit += """--cleanup leaf \\\n"""
            submit += textwrap.dedent("""\
                    --submit`

                    status=`echo "$plan" | grep pegasus-status | tr -s ' '| cut -d ' ' -f 6`
                    echo -e "#!/bin/bash
                    pegasus-status -l $status" > status.sh
                    chmod 744 status.sh

                    remove=`echo "$plan" | grep pegasus-remove | tr -s ' '| cut -d ' ' -f 5`
                    echo -e "#!/bin/bash
                    pegasus-remove $remove" > remove.sh
                    chmod 744 remove.sh

                    echo "$plan"
                    echo "Alternatively, you can use the status & remove scripts in the current directory!"

                    """)
            wh.write(submit)
            os.chmod(self.basepath + "/" + loc + "/submit.sh", 0755)
        return

    def create(self):
        """
            Creates a new pegasus submission directory based on the current timestamp,
            and populates it with all required information to submit a pegasus job.
        """
        print "Generating workflow.  Please wait."
        loc = "/input/imgproc/"
        self._createSetup()
        self._copyFiles()
        self._loadFiles(loc)
        self._loadExecutables()
        self._loadNotify()
        self._createDax(loc)
        self._createExtract(loc)
        self._createReplica(loc)
        self._createSites(loc)
        self._createSubmit(loc)
        return


class ImageLoader:
    """
        This should handle all meta-data definitions
    """

    def __init__(self, template, output, validOnly = False, overwrite = False):
        """
            :param template: Path to input template file, should be valid json.
            :type template: str
        """
        self.templatePath = os.path.abspath(template)
        self.err = ""
        self.output = output + "/input/"
        try:
            open(self.output + "/images.db")
        except IOError:
            self.err += "Path Error: Cannot open output db file. \n"
        try:
            self.log = open(self.output + "/crawl.log", "w")
        except IOError:
            self.err += "Path Error: Cannot open log file. \n"
        self.validator = ih.validator.ImageLoader(self.templatePath)
        if not self.validator.isValid() or validOnly:
            self.validator.printErrors()
            sys.exit()
        self.overwrite = overwrite
        self.template = self.validator.data
        self.template["order"].insert(0, "pegasusid")

        self.count = {}
        self.depth = len(self.template["path"].split("/")) - 1
        self.structure = self._dictTemplate()
        self.data = []
        return

    def _success(self):
        if self.overwrite:
            print "Data crawled!"
            print "Use ih-run to submit your jobs."
        else:
            print "Directory setup successful!"
            print "Define the required templates in " + self.output + "."
            print "Then use ih-run to submit your jobs."
        return

    def _dictTemplate(self):
        """
            Creates the first of two intermediary dictionaries for crawling.  This relates
            identifiers in the path, to a specified depth, as well as operators to split
            the value at that depth.
        """
        m = [x for x in re.findall(r"%(\w+)%", self.template["path"]) if x != "null"]
        sp = self.template["path"].split("/")
        d = {}
        for key in m:
            d[key] = {}
            d[key]["depth"] = [n for n,x in enumerate(sp) if key in x][0]
            d[key]["split"] = []
            val = sp[d[key]["depth"]]
            for operator in [" ", "_", "-", "."]:
                if operator in val:
                    b = val.split(operator)
                    x = [n for n,x in enumerate(b) if key in x][0]
                    d[key]["split"].append({"operator": operator, "index": x})
                    val = b[x]
        return d

    def _loadStructure(self, splitPath):
        """
            Creates the second of two intermediary dictionaries for crawling.
            This utilizes the template created in _dictTemplate, and creates
            a dictionary with actual values instead of structure.
        """
        d ={}
        for key in self.structure:
            val = splitPath[self.structure[key]["depth"]]
            for operator in self.structure[key]["split"]:
                val = val.split(operator["operator"])[operator["index"]]
            d[key] = val
        return d

    def _loadDataFile(self, dict, structure, key):
        """
            Loads data from a data file.  If the specified key isn't in the file,
            write 'UNKNOWN' instead.
        """
        keyVal = self._convertReferences(structure, self.template["data"][key]["key"])
        if keyVal in self.filedata[self.template["data"][key]["value"]]:
            dict[key] = self.filedata[self.template["data"][key]["value"]][keyVal]
        else:
            dict[key] = "UNKNOWN"
        return

    def _loadDataValue(self, dict, structure, key):
        """
            Loads a data value.
        """
        val = self._convertReferences(structure, self.template["data"][key]["value"])
        if "case" in self.template["data"][key]:
            if self.template["data"][key]["case"] == "lower":
                val = val.lower()
            elif self.template["data"][key]["case"] == "upper":
                val = val.upper()

        if "translate" in self.template["data"][key]:
            if val in self.template["translations"][key]:
                val = self.template["translations"][key][val]
        dict[key] = val

        return

    def _loadDataDate(self, dict, structure, key):
        """
            Loads a date.
        """
        val = self._convertReferences(structure, self.template["data"][key]["value"])
        format = "".join([x.replace(x, "%" + x) if x in conf.dateFormat else x for x in self.template["data"][key]["format"]])
        val = datetime.datetime.strptime(val, format).strftime("%Y-%m-%d")
        dict[key] = val
        return

    def _convertReferences(self, structure, val):
        """
            Replaces all references ('%%') to actual values.
        """
        idList = [x for x in re.findall(r"%(\w+)%", val) if x != "null"]
        for identifier in idList:
            if identifier in structure:
                val = val.replace("%" + identifier + "%", structure[identifier])
        return val

    def _loadData(self, path):
        """
            Loads all data for a particular path.
        """
        temp = {}
        final = {}
        splitPath = path.split("/")
        structure = self._loadStructure(splitPath)
        for key in self.template["data"]:
            d = {
                "value": self._loadDataValue,
                "file": self._loadDataFile,
                "date": self._loadDataDate
            }[self.template["data"][key]["type"]](final, structure, key)
        final["path"] = path
        if final["imtype"] not in self.count:
            self.count[final["imtype"]] = 1
        else:
            self.count[final["imtype"]] += 1
        final["pegasusid"] = final["imtype"] + str(self.count[final["imtype"]])
        return final

    def _loadFiles(self):
        """
            Reads all data from all specified files.
        """
        self.filedata = {}
        for key in self.template["data"]:
            if self.template["data"][key]["type"] == "file":
                if self.template["data"][key]["value"] not in self.filedata:
                    self.filedata[self.template["data"][key]["value"]] = {}
                    with open(self.template["data"][key]["value"], "r") as rh:
                        for line in rh.readlines():
                            info = line.strip().split(",")
                            self.filedata[self.template["data"][key]["value"]][info[self.template["data"][key]["keyColumn"]]] = info[self.template["data"][key]["valueColumn"]]
        return

    def crawl(self):
        """
            Recursively walks through directories in base, and loads
            the appropriate data.
        """
        self._loadFiles()
        for root, dirs, files in os.walk(self.template["base"]):
            arr = root.split("/")
            depth = len(arr)
            if (depth == self.depth):
                for f in files:
                    if f[-len(self.template["filetype"]):] == self.template["filetype"] and f[0] != ".":
                        try:
                            d = self._loadData(root + "/" + f)
                            self.data.append(d)
                        except Exception as e:
                            self.log.write("Could not load file from path: '%s/%s'\n" % (root, f))
        return

    def write(self):
        """
            Writes the data loaded from crawl into csv format based on 'order'
        """
        if self.data:
            conn = sqlite3.connect(self.output + "/images.db")
            tablestr = "(pegasusid PRIMARY KEY"
            for x in self.template["order"]:
                if x != "pegasusid":
                    tablestr += "," + str(x)
            tablestr += ")"
            if self.overwrite:
                conn.execute("DROP TABLE IF EXISTS images")
                conn.commit()
            conn.execute("CREATE TABLE images " + tablestr)
            writedata = []
            for row in self.data:
                writedata.append(tuple([str(row[x]) for x in self.template["order"]]))
            query = "insert into images " + str(tuple([str(x) for x in self.template["order"]])) + " values (" + ("?," * len(self.template["order"]))[:-1] + ")"
            conn.executemany(query, writedata)
            conn.commit()
            if not self.overwrite:
                shutil.copyfile(self.templatePath, self.output + "/crawl.json")
        else:
            print "Call crawl first!"
        return

class DirectorySetup:

    def __init__(self, jobhome):
        """
            :param jobhome: The base directory to setup a job.
            :type jobhome: str
        """
        if not os.path.isdir(os.path.dirname(os.path.dirname(os.path.abspath(jobhome) + "/"))):
            print "Can't create job folder, path doesn't exist!"
            sys.exit()
        self.jobhome = jobhome
        return

    def _makeDir(self, dirpath):
        """
            Makes a directory if it doesn't exist.  Catches and
            prints out common errors.
        """
        try:
            os.makedirs(dirpath)
        except OSError as e:
            raise Exception({
                errno.EEXIST: "Directory " + dirpath + " already exists!",
                errno.EACCES: "You don't have permission to create directory "  + dirpath + ".",
            }.get(e.errno, "Error for directory " + dirpath + " error:" + str(e)))
        return

    def _makeFile(self, filepath):
        """
            Opens a file to create it, uses append mode
            so it doesn't overwrite any existing files.
        """
        try:
            open(filepath, "a")
        except OSError as e:
            raise Exception({

            }.get(e, "Unsepcified error for file " + filepath))
        return

    def setup(self):
        """
            Creates the directory structure needed to submit jobs,
            and creates empty template files needed for job submission.
        """
        self._makeDir(self.jobhome)
        self._makeDir(self.jobhome + "/input/")
        for fname in conf.jobFiles:
            self._makeFile(self.jobhome + "/input/" + fname)
        return
